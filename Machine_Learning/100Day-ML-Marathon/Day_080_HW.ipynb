{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 請結合前面的知識與程式碼，比較不同的 optimizer 與 learning rate 組合對訓練的結果與影響\n",
    "常見的 optimizer 包含\n",
    "- SGD\n",
    "- RMSprop\n",
    "- AdaGrad\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:33:16.825006Z",
     "start_time": "2019-07-24T14:33:16.820543Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "\n",
    "# Disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:33:17.927459Z",
     "start_time": "2019-07-24T14:33:17.564423Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:33:18.532084Z",
     "start_time": "2019-07-24T14:33:18.525140Z"
    }
   },
   "outputs": [],
   "source": [
    "## 資料前處理\n",
    "def preproc_x(x, flatten=True):\n",
    "    x = x / 255.\n",
    "    if flatten:\n",
    "        x = x.reshape((len(x), -1))\n",
    "    return x\n",
    "\n",
    "def preproc_y(y, num_classes=10):\n",
    "    if y.shape[-1] == 1:\n",
    "        y = keras.utils.to_categorical(y, num_classes)\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:33:19.324323Z",
     "start_time": "2019-07-24T14:33:19.276707Z"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2120767505fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Preproc the inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreproc_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreproc_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-ab7f40603e2c>\u001b[0m in \u001b[0;36mpreproc_x\u001b[1;34m(x, flatten)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## 資料前處理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreproc_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "\n",
    "# Preproc the inputs\n",
    "x_train = preproc_x(x_train)\n",
    "x_test = preproc_x(x_test)\n",
    "\n",
    "# Preprc the outputs\n",
    "y_train = preproc_y(y_train)\n",
    "y_test = preproc_y(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 選擇optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T12:34:40.433529Z",
     "start_time": "2019-07-24T12:34:40.425096Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_shape, output_units=10, num_neurons=[512, 256, 128, 64]):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "    \n",
    "    for i, n_units in enumerate(num_neurons):\n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(units=n_units, activation='relu', name='hidden_layer'+ str(i+1))(input_layer)\n",
    "        else:\n",
    "            x = keras.layers.Dense(units=n_units, activation='relu', name='hidden_layer'+ str(i+1))(x)\n",
    "            \n",
    "    out = keras.layers.Dense(units=output_units, activation='softmax', name='output')(x)\n",
    "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T12:34:40.721163Z",
     "start_time": "2019-07-24T12:34:40.717098Z"
    }
   },
   "outputs": [],
   "source": [
    "## 超參數設定\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "# MOMENTUM = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T13:42:49.231665Z",
     "start_time": "2019-07-24T12:34:41.097556Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 20:34:41.106978  6308 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0724 20:34:41.108467  6308 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0724 20:34:41.139714  6308 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0724 20:34:41.141699  6308 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0724 20:34:41.145667  6308 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0724 20:34:41.235443  6308 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expertiment with： SGD Optimizer\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "hidden_layer4 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,746,506\n",
      "Trainable params: 1,746,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 20:34:41.375266  6308 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 7s 130us/step - loss: 2.1095 - acc: 0.2307 - val_loss: 1.9776 - val_acc: 0.2925\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.9183 - acc: 0.3143 - val_loss: 1.8915 - val_acc: 0.3172\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 1.8381 - acc: 0.3485 - val_loss: 1.8493 - val_acc: 0.3304\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 4s 78us/step - loss: 1.7885 - acc: 0.3695 - val_loss: 1.8525 - val_acc: 0.3350\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 4s 77us/step - loss: 1.7470 - acc: 0.3836 - val_loss: 1.7147 - val_acc: 0.3924\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.7161 - acc: 0.3928 - val_loss: 1.8032 - val_acc: 0.3600\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 1.6819 - acc: 0.4063 - val_loss: 1.6783 - val_acc: 0.4071\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 1.6579 - acc: 0.4134 - val_loss: 1.7808 - val_acc: 0.3777\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.6369 - acc: 0.4212 - val_loss: 1.6493 - val_acc: 0.4083\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.6219 - acc: 0.4281 - val_loss: 1.6612 - val_acc: 0.4115\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 1.6016 - acc: 0.4364 - val_loss: 1.6954 - val_acc: 0.4006\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 4s 83us/step - loss: 1.5830 - acc: 0.4416 - val_loss: 1.6127 - val_acc: 0.4295\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 4s 83us/step - loss: 1.5629 - acc: 0.4481 - val_loss: 1.5754 - val_acc: 0.4338\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.5422 - acc: 0.4545 - val_loss: 1.5500 - val_acc: 0.4550\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.5331 - acc: 0.4597 - val_loss: 1.6009 - val_acc: 0.4358\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 4s 89us/step - loss: 1.5119 - acc: 0.4654 - val_loss: 1.5716 - val_acc: 0.4336\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.5041 - acc: 0.4675 - val_loss: 1.5653 - val_acc: 0.4378\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.4899 - acc: 0.4730 - val_loss: 1.5131 - val_acc: 0.4638\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 4s 82us/step - loss: 1.4751 - acc: 0.4803 - val_loss: 1.5477 - val_acc: 0.4546\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 1.4591 - acc: 0.4843 - val_loss: 1.6002 - val_acc: 0.4232\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.4533 - acc: 0.4877 - val_loss: 1.5193 - val_acc: 0.4492\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 1.4386 - acc: 0.4922 - val_loss: 1.5615 - val_acc: 0.4487\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.4293 - acc: 0.4958 - val_loss: 1.5028 - val_acc: 0.4633\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 1.4120 - acc: 0.5031 - val_loss: 1.4976 - val_acc: 0.4622\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 1.4007 - acc: 0.5069 - val_loss: 1.5297 - val_acc: 0.4515\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 1.3931 - acc: 0.5067 - val_loss: 1.4880 - val_acc: 0.4779\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 4s 82us/step - loss: 1.3782 - acc: 0.5136 - val_loss: 1.5655 - val_acc: 0.4394\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 4s 86us/step - loss: 1.3760 - acc: 0.5147 - val_loss: 1.5142 - val_acc: 0.4621\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 4s 83us/step - loss: 1.3630 - acc: 0.5179 - val_loss: 1.4552 - val_acc: 0.4786\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.3513 - acc: 0.5228 - val_loss: 1.4288 - val_acc: 0.4878\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 1.3414 - acc: 0.5284 - val_loss: 1.4260 - val_acc: 0.4939\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 1.3346 - acc: 0.5296 - val_loss: 1.4021 - val_acc: 0.5005\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 1.3254 - acc: 0.5312 - val_loss: 1.5320 - val_acc: 0.4660\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 1.3200 - acc: 0.5332 - val_loss: 1.4008 - val_acc: 0.4971\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 1.3075 - acc: 0.5389 - val_loss: 1.5161 - val_acc: 0.4601\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 1.2982 - acc: 0.5437 - val_loss: 1.3680 - val_acc: 0.5119\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 4s 86us/step - loss: 1.2916 - acc: 0.5454 - val_loss: 1.5568 - val_acc: 0.4690\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 4s 86us/step - loss: 1.2826 - acc: 0.5475 - val_loss: 1.4150 - val_acc: 0.4910\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 4s 79us/step - loss: 1.2753 - acc: 0.5496 - val_loss: 1.3881 - val_acc: 0.5005\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 1.2643 - acc: 0.5537 - val_loss: 1.4713 - val_acc: 0.4846\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 1.2533 - acc: 0.5587 - val_loss: 1.4668 - val_acc: 0.4860\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 5s 90us/step - loss: 1.2481 - acc: 0.5607 - val_loss: 1.5760 - val_acc: 0.4492\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 1.2472 - acc: 0.5601 - val_loss: 1.4427 - val_acc: 0.4821\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 1.2319 - acc: 0.5671 - val_loss: 1.3753 - val_acc: 0.5129\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 1.2246 - acc: 0.5666 - val_loss: 1.3841 - val_acc: 0.5068\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 1.2161 - acc: 0.5711 - val_loss: 1.3807 - val_acc: 0.5138\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 4s 83us/step - loss: 1.2137 - acc: 0.5732 - val_loss: 1.4634 - val_acc: 0.4912\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.1998 - acc: 0.5764 - val_loss: 1.5035 - val_acc: 0.4755\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 1.1944 - acc: 0.5780 - val_loss: 1.4597 - val_acc: 0.4922\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.1872 - acc: 0.5813 - val_loss: 1.4692 - val_acc: 0.4894\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 1.1808 - acc: 0.5852 - val_loss: 1.4737 - val_acc: 0.4873\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.1767 - acc: 0.5872 - val_loss: 1.4508 - val_acc: 0.4847\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.1649 - acc: 0.5884 - val_loss: 1.4872 - val_acc: 0.4876\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 1.1572 - acc: 0.5914 - val_loss: 1.5149 - val_acc: 0.4726\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 1.1489 - acc: 0.5956 - val_loss: 1.5196 - val_acc: 0.4622\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 1.1473 - acc: 0.5946 - val_loss: 1.3719 - val_acc: 0.5135\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 1.1361 - acc: 0.5987 - val_loss: 1.3856 - val_acc: 0.5175\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 1.1262 - acc: 0.6039 - val_loss: 1.4264 - val_acc: 0.4991\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 4s 80us/step - loss: 1.1212 - acc: 0.6063 - val_loss: 1.3454 - val_acc: 0.5273\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 4s 76us/step - loss: 1.1109 - acc: 0.6082 - val_loss: 1.6860 - val_acc: 0.4262\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.1134 - acc: 0.6089 - val_loss: 1.5001 - val_acc: 0.4802\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 4s 83us/step - loss: 1.1026 - acc: 0.6119 - val_loss: 1.4152 - val_acc: 0.5075\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 1.0970 - acc: 0.6140 - val_loss: 1.5933 - val_acc: 0.4503\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 1.0865 - acc: 0.6184 - val_loss: 1.4005 - val_acc: 0.5091\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.0769 - acc: 0.6205 - val_loss: 1.4915 - val_acc: 0.4897\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 1.0684 - acc: 0.6242 - val_loss: 1.4140 - val_acc: 0.5068\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 1.0618 - acc: 0.6241 - val_loss: 1.3619 - val_acc: 0.5224\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 1.0560 - acc: 0.6263 - val_loss: 1.4717 - val_acc: 0.4938\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 1.0539 - acc: 0.6266 - val_loss: 1.3528 - val_acc: 0.5264\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.0453 - acc: 0.6324 - val_loss: 1.6502 - val_acc: 0.4524\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.0368 - acc: 0.6362 - val_loss: 1.5756 - val_acc: 0.4650\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.0339 - acc: 0.6363 - val_loss: 1.3899 - val_acc: 0.5157\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 1.0212 - acc: 0.6394 - val_loss: 1.3645 - val_acc: 0.5285\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 1.0132 - acc: 0.6440 - val_loss: 1.4715 - val_acc: 0.5073\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 1.0063 - acc: 0.6470 - val_loss: 1.3785 - val_acc: 0.5227\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.9978 - acc: 0.6510 - val_loss: 1.4256 - val_acc: 0.5151\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.9917 - acc: 0.6506 - val_loss: 1.4112 - val_acc: 0.5153\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.9819 - acc: 0.6566 - val_loss: 1.4093 - val_acc: 0.5218\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.9820 - acc: 0.6559 - val_loss: 1.4756 - val_acc: 0.4959\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.9693 - acc: 0.6594 - val_loss: 1.7393 - val_acc: 0.4557\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.9721 - acc: 0.6587 - val_loss: 1.4871 - val_acc: 0.5112\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.9585 - acc: 0.6615 - val_loss: 1.3629 - val_acc: 0.5332\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.9475 - acc: 0.6669 - val_loss: 1.5009 - val_acc: 0.4904\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.9448 - acc: 0.6687 - val_loss: 1.5356 - val_acc: 0.4995\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 4s 89us/step - loss: 0.9342 - acc: 0.6704 - val_loss: 1.3659 - val_acc: 0.5320\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 0.9300 - acc: 0.6743 - val_loss: 1.5750 - val_acc: 0.4942\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 4s 87us/step - loss: 0.9219 - acc: 0.6768 - val_loss: 1.6212 - val_acc: 0.4817\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 5s 90us/step - loss: 0.9109 - acc: 0.6777 - val_loss: 1.5668 - val_acc: 0.4889\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 0.9122 - acc: 0.6772 - val_loss: 1.4588 - val_acc: 0.5162\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 4s 88us/step - loss: 0.8965 - acc: 0.6831 - val_loss: 1.4706 - val_acc: 0.5065\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 0.8990 - acc: 0.6855 - val_loss: 1.4449 - val_acc: 0.5056\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 4s 81us/step - loss: 0.8868 - acc: 0.6860 - val_loss: 1.5656 - val_acc: 0.4899\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.8823 - acc: 0.6880 - val_loss: 1.4109 - val_acc: 0.5243\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.8748 - acc: 0.6914 - val_loss: 1.5149 - val_acc: 0.5050\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.8649 - acc: 0.6972 - val_loss: 1.5403 - val_acc: 0.5024\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.8645 - acc: 0.6985 - val_loss: 1.4113 - val_acc: 0.5284\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 6s 126us/step - loss: 0.8588 - acc: 0.6982 - val_loss: 1.4185 - val_acc: 0.5261\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.8479 - acc: 0.7013 - val_loss: 1.6547 - val_acc: 0.4718\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.8299 - acc: 0.7091 - val_loss: 1.5036 - val_acc: 0.5108\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.8424 - acc: 0.7021 - val_loss: 1.4929 - val_acc: 0.5109\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.8206 - acc: 0.7112 - val_loss: 1.5182 - val_acc: 0.5131\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.8150 - acc: 0.7115 - val_loss: 1.5765 - val_acc: 0.5028\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.8167 - acc: 0.7131 - val_loss: 1.5397 - val_acc: 0.5088\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.8034 - acc: 0.7178 - val_loss: 1.5313 - val_acc: 0.5154\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.7972 - acc: 0.7195 - val_loss: 1.5179 - val_acc: 0.5122\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.7988 - acc: 0.7181 - val_loss: 1.5559 - val_acc: 0.5077\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.7839 - acc: 0.7244 - val_loss: 1.5301 - val_acc: 0.5155\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.7762 - acc: 0.7285 - val_loss: 1.7112 - val_acc: 0.4853\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.7634 - acc: 0.7330 - val_loss: 1.7748 - val_acc: 0.4696\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.7763 - acc: 0.7293 - val_loss: 1.4542 - val_acc: 0.5335 1s - loss:\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.7600 - acc: 0.7329 - val_loss: 1.5585 - val_acc: 0.5185\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 0.7531 - acc: 0.7339 - val_loss: 1.7261 - val_acc: 0.4839\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 4s 84us/step - loss: 0.7358 - acc: 0.7418 - val_loss: 1.6207 - val_acc: 0.5110\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.7361 - acc: 0.7403 - val_loss: 1.5158 - val_acc: 0.5235\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.7254 - acc: 0.7459 - val_loss: 1.5137 - val_acc: 0.5296\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.7181 - acc: 0.7478 - val_loss: 1.5934 - val_acc: 0.5096\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 4s 82us/step - loss: 0.7125 - acc: 0.7505 - val_loss: 1.5590 - val_acc: 0.5169\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 4s 86us/step - loss: 0.7133 - acc: 0.7476 - val_loss: 1.8961 - val_acc: 0.4628\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 4s 80us/step - loss: 0.7147 - acc: 0.7487 - val_loss: 1.7093 - val_acc: 0.5000\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 0.7030 - acc: 0.7562 - val_loss: 1.5823 - val_acc: 0.5155\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.6851 - acc: 0.7592 - val_loss: 2.3465 - val_acc: 0.4113\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 0.7069 - acc: 0.7546 - val_loss: 1.7257 - val_acc: 0.4933\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 4s 89us/step - loss: 0.6786 - acc: 0.7626 - val_loss: 1.7417 - val_acc: 0.4729\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 0.6796 - acc: 0.7607 - val_loss: 1.5734 - val_acc: 0.5122\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 4s 88us/step - loss: 0.6706 - acc: 0.7658 - val_loss: 1.8296 - val_acc: 0.4798\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.6556 - acc: 0.7710 - val_loss: 1.5477 - val_acc: 0.5255\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 4s 88us/step - loss: 0.6475 - acc: 0.7738 - val_loss: 1.5666 - val_acc: 0.5266\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.6516 - acc: 0.7725 - val_loss: 1.8382 - val_acc: 0.4701\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 4s 82us/step - loss: 0.6423 - acc: 0.7742 - val_loss: 2.4225 - val_acc: 0.3886\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 4s 90us/step - loss: 0.6825 - acc: 0.7668 - val_loss: 1.6661 - val_acc: 0.5099\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 4s 85us/step - loss: 0.6193 - acc: 0.7838 - val_loss: 1.5600 - val_acc: 0.5353\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.6197 - acc: 0.7832 - val_loss: 1.6191 - val_acc: 0.5208\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 4s 88us/step - loss: 0.6073 - acc: 0.7885 - val_loss: 1.7478 - val_acc: 0.4953\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.6105 - acc: 0.7881 - val_loss: 1.7483 - val_acc: 0.4945\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 4s 86us/step - loss: 0.6088 - acc: 0.7886 - val_loss: 1.7597 - val_acc: 0.4870\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 4s 88us/step - loss: 0.5944 - acc: 0.7945 - val_loss: 1.8385 - val_acc: 0.4943\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.5863 - acc: 0.7963 - val_loss: 1.6856 - val_acc: 0.5191- loss: 0.5814 - acc\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.5826 - acc: 0.7978 - val_loss: 1.9170 - val_acc: 0.4885\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.5788 - acc: 0.7987 - val_loss: 1.6272 - val_acc: 0.5194\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 4s 83us/step - loss: 0.5572 - acc: 0.8084 - val_loss: 1.6692 - val_acc: 0.5256\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.5573 - acc: 0.8078 - val_loss: 1.7729 - val_acc: 0.5015\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.5731 - acc: 0.7995 - val_loss: 1.6089 - val_acc: 0.5316\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.5647 - acc: 0.8053 - val_loss: 1.7618 - val_acc: 0.5051\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.5359 - acc: 0.8156 - val_loss: 1.7207 - val_acc: 0.5141\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.5451 - acc: 0.8114 - val_loss: 1.7162 - val_acc: 0.5160\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.5490 - acc: 0.8134 - val_loss: 1.6821 - val_acc: 0.5325\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.5215 - acc: 0.8211 - val_loss: 1.9951 - val_acc: 0.4846\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 0.5190 - acc: 0.8221 - val_loss: 1.7154 - val_acc: 0.5317\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.5172 - acc: 0.8216 - val_loss: 1.7991 - val_acc: 0.5099\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.5772 - acc: 0.8077 - val_loss: 1.7952 - val_acc: 0.5157\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 0.5073 - acc: 0.8272 - val_loss: 1.8703 - val_acc: 0.4974\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.4806 - acc: 0.8364 - val_loss: 1.9044 - val_acc: 0.4966\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.5008 - acc: 0.8266 - val_loss: 2.4355 - val_acc: 0.4359\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.5226 - acc: 0.8241 - val_loss: 1.7497 - val_acc: 0.5236\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.4600 - acc: 0.8421 - val_loss: 1.8377 - val_acc: 0.5182\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.4504 - acc: 0.8459 - val_loss: 2.1875 - val_acc: 0.4735\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 5s 110us/step - loss: 0.4832 - acc: 0.8378 - val_loss: 2.0669 - val_acc: 0.4780\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.4758 - acc: 0.8361 - val_loss: 2.0912 - val_acc: 0.4744\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.4461 - acc: 0.8486 - val_loss: 1.8731 - val_acc: 0.5236\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.4862 - acc: 0.8371 - val_loss: 1.8209 - val_acc: 0.5160\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.4170 - acc: 0.8571 - val_loss: 1.9877 - val_acc: 0.5028\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.4549 - acc: 0.8447 - val_loss: 1.9107 - val_acc: 0.5187\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 0.4248 - acc: 0.8555 - val_loss: 2.0154 - val_acc: 0.5078\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 4s 88us/step - loss: 0.4159 - acc: 0.8603 - val_loss: 1.9106 - val_acc: 0.5202\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 4s 89us/step - loss: 0.4174 - acc: 0.8596 - val_loss: 2.0571 - val_acc: 0.4942\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.4089 - acc: 0.8600 - val_loss: 1.9748 - val_acc: 0.5162\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.4517 - acc: 0.8550 - val_loss: 1.9357 - val_acc: 0.5055\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.4113 - acc: 0.8596 - val_loss: 2.0146 - val_acc: 0.5126\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.3813 - acc: 0.8704 - val_loss: 2.1140 - val_acc: 0.5087\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.3912 - acc: 0.8689 - val_loss: 1.9996 - val_acc: 0.4941\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.3931 - acc: 0.8680 - val_loss: 1.8166 - val_acc: 0.5344\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.3716 - acc: 0.8752 - val_loss: 2.2660 - val_acc: 0.4568\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.3963 - acc: 0.8707 - val_loss: 1.9195 - val_acc: 0.5226\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3866 - acc: 0.8730 - val_loss: 3.6276 - val_acc: 0.3461\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.4062 - acc: 0.8672 - val_loss: 3.0399 - val_acc: 0.4355\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.4671 - acc: 0.8503 - val_loss: 2.0092 - val_acc: 0.5016\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 6s 117us/step - loss: 0.3734 - acc: 0.8763 - val_loss: 2.2029 - val_acc: 0.4873\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.3228 - acc: 0.8923 - val_loss: 1.9390 - val_acc: 0.5370\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.3558 - acc: 0.8823 - val_loss: 2.1241 - val_acc: 0.5045\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.3595 - acc: 0.8816 - val_loss: 2.4139 - val_acc: 0.4719\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.3497 - acc: 0.8866 - val_loss: 2.0564 - val_acc: 0.5123\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.3352 - acc: 0.8877 - val_loss: 1.9711 - val_acc: 0.5243\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.2850 - acc: 0.9084 - val_loss: 2.2108 - val_acc: 0.4989\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.3429 - acc: 0.8866 - val_loss: 2.0382 - val_acc: 0.5266\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.3182 - acc: 0.8937 - val_loss: 2.0546 - val_acc: 0.5220\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3000 - acc: 0.9009 - val_loss: 2.1064 - val_acc: 0.5143\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.3532 - acc: 0.8891 - val_loss: 2.0598 - val_acc: 0.5224\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.3235 - acc: 0.8967 - val_loss: 2.2992 - val_acc: 0.5008\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.3432 - acc: 0.8900 - val_loss: 2.0582 - val_acc: 0.5282\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.2746 - acc: 0.9097 - val_loss: 2.2127 - val_acc: 0.5155\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2711 - acc: 0.9140 - val_loss: 2.3829 - val_acc: 0.4984\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3870 - acc: 0.8844 - val_loss: 2.1264 - val_acc: 0.5170\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.3456 - acc: 0.8926 - val_loss: 2.0638 - val_acc: 0.5297\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.2215 - acc: 0.9320 - val_loss: 2.3706 - val_acc: 0.4969\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.2923 - acc: 0.9111 - val_loss: 2.7522 - val_acc: 0.4403\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.3196 - acc: 0.8992 - val_loss: 2.1363 - val_acc: 0.5295\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.3077 - acc: 0.9017 - val_loss: 2.1228 - val_acc: 0.5308\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.2256 - acc: 0.9277 - val_loss: 2.1893 - val_acc: 0.5311\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.2213 - acc: 0.9313 - val_loss: 2.2517 - val_acc: 0.5224\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.3999 - acc: 0.8787 - val_loss: 2.2499 - val_acc: 0.5135\n",
      "Expertiment with： RMSprop Optimizer\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "hidden_layer4 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,746,506\n",
      "Trainable params: 1,746,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 2.2120 - acc: 0.2032 - val_loss: 2.0753 - val_acc: 0.2296\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 1.9021 - acc: 0.3127 - val_loss: 1.9056 - val_acc: 0.3259\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 1.8051 - acc: 0.3489 - val_loss: 1.7780 - val_acc: 0.3595\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 6s 123us/step - loss: 1.7318 - acc: 0.3813 - val_loss: 1.6878 - val_acc: 0.3892\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 1.6782 - acc: 0.4000 - val_loss: 1.6681 - val_acc: 0.4048\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 1.6356 - acc: 0.4145 - val_loss: 1.6783 - val_acc: 0.3955\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.5928 - acc: 0.4329 - val_loss: 1.6858 - val_acc: 0.3954\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 1.5563 - acc: 0.4430 - val_loss: 1.5693 - val_acc: 0.4389\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 1.5308 - acc: 0.4534 - val_loss: 1.7843 - val_acc: 0.3776\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 1.4990 - acc: 0.4645 - val_loss: 1.5792 - val_acc: 0.4473\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 1.4731 - acc: 0.4733 - val_loss: 1.6126 - val_acc: 0.4303\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 1.4434 - acc: 0.4838 - val_loss: 1.5775 - val_acc: 0.4289\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.4209 - acc: 0.4926 - val_loss: 1.5167 - val_acc: 0.4615\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 1.3950 - acc: 0.5006 - val_loss: 1.5436 - val_acc: 0.4590\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.3675 - acc: 0.5126 - val_loss: 1.5126 - val_acc: 0.4578\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 1.3490 - acc: 0.5173 - val_loss: 1.6178 - val_acc: 0.4325\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.3262 - acc: 0.5276 - val_loss: 1.4975 - val_acc: 0.4792\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 1.3029 - acc: 0.5349 - val_loss: 1.5237 - val_acc: 0.4638\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 1.2800 - acc: 0.5425 - val_loss: 1.5407 - val_acc: 0.4689\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 1.2652 - acc: 0.5491 - val_loss: 1.5273 - val_acc: 0.4703\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 1.2410 - acc: 0.5543 - val_loss: 1.5458 - val_acc: 0.4597\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.2197 - acc: 0.5633 - val_loss: 1.4868 - val_acc: 0.4869\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 1.2011 - acc: 0.5706 - val_loss: 1.7419 - val_acc: 0.4171\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 109us/step - loss: 1.1817 - acc: 0.5775 - val_loss: 1.7059 - val_acc: 0.4479\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 1.1627 - acc: 0.5859 - val_loss: 1.4927 - val_acc: 0.4921\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 1.1397 - acc: 0.5932 - val_loss: 1.5036 - val_acc: 0.4935\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 1.1221 - acc: 0.5983 - val_loss: 1.5125 - val_acc: 0.4838\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 1.0983 - acc: 0.6067 - val_loss: 1.5460 - val_acc: 0.4934\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 1.0809 - acc: 0.6160 - val_loss: 1.5848 - val_acc: 0.4777\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 1.0688 - acc: 0.6157 - val_loss: 1.6139 - val_acc: 0.4770\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 1.0530 - acc: 0.6225 - val_loss: 1.5572 - val_acc: 0.4877\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 5s 110us/step - loss: 1.0288 - acc: 0.6307 - val_loss: 1.5953 - val_acc: 0.4873\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 1.0114 - acc: 0.6368 - val_loss: 1.5552 - val_acc: 0.4931\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.9981 - acc: 0.6408 - val_loss: 1.6366 - val_acc: 0.4895\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.9801 - acc: 0.6502 - val_loss: 1.6938 - val_acc: 0.4822\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.9648 - acc: 0.6565 - val_loss: 1.6059 - val_acc: 0.4972\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.9478 - acc: 0.6598 - val_loss: 1.6345 - val_acc: 0.4869\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.9311 - acc: 0.6644 - val_loss: 1.7384 - val_acc: 0.4687\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.9147 - acc: 0.6719 - val_loss: 1.6801 - val_acc: 0.4932\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.9062 - acc: 0.6749 - val_loss: 1.8405 - val_acc: 0.4772\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 6s 116us/step - loss: 0.8895 - acc: 0.6821 - val_loss: 1.7140 - val_acc: 0.4894\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 0.8825 - acc: 0.6834 - val_loss: 1.7277 - val_acc: 0.4891\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 6s 120us/step - loss: 0.8619 - acc: 0.6929 - val_loss: 1.8056 - val_acc: 0.4847\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 0.8495 - acc: 0.6924 - val_loss: 1.7559 - val_acc: 0.4868\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.8366 - acc: 0.6991 - val_loss: 1.8609 - val_acc: 0.4708\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 0.8250 - acc: 0.7027 - val_loss: 1.8801 - val_acc: 0.4846\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.8111 - acc: 0.7091 - val_loss: 1.8408 - val_acc: 0.4857\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 0.7958 - acc: 0.7133 - val_loss: 1.8896 - val_acc: 0.4797\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 0.7823 - acc: 0.7207 - val_loss: 1.8553 - val_acc: 0.4838\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.7680 - acc: 0.7223 - val_loss: 1.9417 - val_acc: 0.4903\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.7609 - acc: 0.7272 - val_loss: 1.8729 - val_acc: 0.4887\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.7520 - acc: 0.7299 - val_loss: 2.0419 - val_acc: 0.4791\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.7368 - acc: 0.7331 - val_loss: 2.0967 - val_acc: 0.4773\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.7269 - acc: 0.7380 - val_loss: 2.0940 - val_acc: 0.4713\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 0.7164 - acc: 0.7428 - val_loss: 2.1477 - val_acc: 0.4805\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.7057 - acc: 0.7447 - val_loss: 2.0991 - val_acc: 0.4713\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.7000 - acc: 0.7508 - val_loss: 2.2228 - val_acc: 0.4725\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.6867 - acc: 0.7523 - val_loss: 2.0401 - val_acc: 0.4898\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.6755 - acc: 0.7578 - val_loss: 2.1453 - val_acc: 0.4860\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.6686 - acc: 0.7596 - val_loss: 2.2244 - val_acc: 0.4702\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.6537 - acc: 0.7646 - val_loss: 1.9368 - val_acc: 0.4949\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 5s 110us/step - loss: 0.6538 - acc: 0.7626 - val_loss: 2.2217 - val_acc: 0.4692\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 0.6443 - acc: 0.7677 - val_loss: 2.1891 - val_acc: 0.4803\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.6363 - acc: 0.7710 - val_loss: 2.3286 - val_acc: 0.4817\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.6224 - acc: 0.7769 - val_loss: 2.3429 - val_acc: 0.4807\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.6154 - acc: 0.7773 - val_loss: 2.3247 - val_acc: 0.4661\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.6153 - acc: 0.7799 - val_loss: 2.2334 - val_acc: 0.4815\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.6061 - acc: 0.7808 - val_loss: 2.1674 - val_acc: 0.4806\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.5931 - acc: 0.7868 - val_loss: 2.3634 - val_acc: 0.4923\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.5878 - acc: 0.7895 - val_loss: 2.3579 - val_acc: 0.4982\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.5779 - acc: 0.7902 - val_loss: 2.5311 - val_acc: 0.4839-\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.5679 - acc: 0.7965 - val_loss: 2.5127 - val_acc: 0.4719\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.5712 - acc: 0.7957 - val_loss: 2.4815 - val_acc: 0.4758\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.5529 - acc: 0.8029 - val_loss: 2.7144 - val_acc: 0.4764\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.5542 - acc: 0.8012 - val_loss: 2.5584 - val_acc: 0.4821\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.5443 - acc: 0.8027 - val_loss: 2.6120 - val_acc: 0.4690\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.5379 - acc: 0.8055 - val_loss: 2.5827 - val_acc: 0.4669\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.5378 - acc: 0.8047 - val_loss: 2.4725 - val_acc: 0.4956\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.5239 - acc: 0.8116 - val_loss: 2.4935 - val_acc: 0.4849\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.5162 - acc: 0.8128 - val_loss: 2.6942 - val_acc: 0.4850\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.5136 - acc: 0.8182 - val_loss: 2.9102 - val_acc: 0.4629\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.5101 - acc: 0.8170 - val_loss: 2.6577 - val_acc: 0.4834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.5050 - acc: 0.8187 - val_loss: 2.6469 - val_acc: 0.4861\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.4953 - acc: 0.8206 - val_loss: 2.6499 - val_acc: 0.4694\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.4971 - acc: 0.8198 - val_loss: 2.7377 - val_acc: 0.4856\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.4870 - acc: 0.8255 - val_loss: 2.8509 - val_acc: 0.4632\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.4843 - acc: 0.8259 - val_loss: 2.7236 - val_acc: 0.4666\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.4808 - acc: 0.8280 - val_loss: 2.6386 - val_acc: 0.4867\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.4792 - acc: 0.8280 - val_loss: 2.8158 - val_acc: 0.4849\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.4709 - acc: 0.8313 - val_loss: 2.9010 - val_acc: 0.4784\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.4654 - acc: 0.8336 - val_loss: 2.9704 - val_acc: 0.4678\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.4623 - acc: 0.8337 - val_loss: 2.9045 - val_acc: 0.4856\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.4558 - acc: 0.8389 - val_loss: 2.9068 - val_acc: 0.4764\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.4489 - acc: 0.8409 - val_loss: 2.9819 - val_acc: 0.4805\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.4488 - acc: 0.8390 - val_loss: 3.0128 - val_acc: 0.4830\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.4412 - acc: 0.8419 - val_loss: 3.1211 - val_acc: 0.4612\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.4397 - acc: 0.8435 - val_loss: 3.0543 - val_acc: 0.4857\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.4359 - acc: 0.8457 - val_loss: 3.1941 - val_acc: 0.4527\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.4367 - acc: 0.8448 - val_loss: 3.0416 - val_acc: 0.4561\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.4323 - acc: 0.8455 - val_loss: 2.9991 - val_acc: 0.4819\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.4215 - acc: 0.8478 - val_loss: 3.2138 - val_acc: 0.4684\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.4198 - acc: 0.8500 - val_loss: 3.3255 - val_acc: 0.4775\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.4173 - acc: 0.8533 - val_loss: 2.9976 - val_acc: 0.4747\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.4117 - acc: 0.8533 - val_loss: 3.2116 - val_acc: 0.4668\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.4072 - acc: 0.8556 - val_loss: 3.1819 - val_acc: 0.4795\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.4021 - acc: 0.8589 - val_loss: 3.1596 - val_acc: 0.4831\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.3976 - acc: 0.8589 - val_loss: 3.3656 - val_acc: 0.4607\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.4050 - acc: 0.8566 - val_loss: 3.4056 - val_acc: 0.4727\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.3967 - acc: 0.8586 - val_loss: 3.3768 - val_acc: 0.4554\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.3941 - acc: 0.8604 - val_loss: 3.2797 - val_acc: 0.4786\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3874 - acc: 0.8636 - val_loss: 3.3740 - val_acc: 0.4767\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.3877 - acc: 0.8622 - val_loss: 3.1374 - val_acc: 0.4766\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.3886 - acc: 0.8618 - val_loss: 3.2300 - val_acc: 0.4801\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.3843 - acc: 0.8633 - val_loss: 3.3389 - val_acc: 0.4699\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3776 - acc: 0.8654 - val_loss: 3.3861 - val_acc: 0.4831\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.3717 - acc: 0.8695 - val_loss: 3.5178 - val_acc: 0.4645\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.3681 - acc: 0.8697 - val_loss: 3.4306 - val_acc: 0.4677\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3730 - acc: 0.8676 - val_loss: 3.2714 - val_acc: 0.4797\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3634 - acc: 0.8712 - val_loss: 3.3600 - val_acc: 0.4837\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.3666 - acc: 0.8707 - val_loss: 3.3487 - val_acc: 0.4759\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.3612 - acc: 0.8732 - val_loss: 3.5513 - val_acc: 0.4673\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3561 - acc: 0.8749 - val_loss: 3.5832 - val_acc: 0.4790\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.3562 - acc: 0.8744 - val_loss: 3.4794 - val_acc: 0.4833\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3549 - acc: 0.8746 - val_loss: 3.3870 - val_acc: 0.4711\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3528 - acc: 0.8761 - val_loss: 3.5596 - val_acc: 0.4899\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3458 - acc: 0.8771 - val_loss: 3.4944 - val_acc: 0.4606\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3470 - acc: 0.8791 - val_loss: 3.5123 - val_acc: 0.4766\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3431 - acc: 0.8803 - val_loss: 3.6542 - val_acc: 0.4808\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.3408 - acc: 0.8803 - val_loss: 3.6448 - val_acc: 0.4830\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.3298 - acc: 0.8851 - val_loss: 3.4411 - val_acc: 0.4820\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.3353 - acc: 0.8817 - val_loss: 3.7795 - val_acc: 0.4668\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.3302 - acc: 0.8838 - val_loss: 3.7467 - val_acc: 0.4684\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.3333 - acc: 0.8826 - val_loss: 3.8856 - val_acc: 0.4601\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.3341 - acc: 0.8845 - val_loss: 3.6751 - val_acc: 0.4819\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.3226 - acc: 0.8848 - val_loss: 3.7238 - val_acc: 0.4799\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.3215 - acc: 0.8880 - val_loss: 3.6003 - val_acc: 0.4705\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3279 - acc: 0.8866 - val_loss: 3.6699 - val_acc: 0.4700\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3171 - acc: 0.8885 - val_loss: 3.8491 - val_acc: 0.4803\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.3245 - acc: 0.8862 - val_loss: 3.6143 - val_acc: 0.4803\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.3136 - acc: 0.8908 - val_loss: 3.7330 - val_acc: 0.4735\n",
      "Epoch 141/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.3160 - acc: 0.8899 - val_loss: 3.5932 - val_acc: 0.4736\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.3162 - acc: 0.8921 - val_loss: 3.8236 - val_acc: 0.4753\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 0.3159 - acc: 0.8902 - val_loss: 3.6250 - val_acc: 0.4869\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.3061 - acc: 0.8940 - val_loss: 3.7646 - val_acc: 0.4771\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 0.3077 - acc: 0.8926 - val_loss: 3.7597 - val_acc: 0.4792\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.3022 - acc: 0.8949 - val_loss: 3.8389 - val_acc: 0.4740\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2966 - acc: 0.8961 - val_loss: 3.8338 - val_acc: 0.4774\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 0.3032 - acc: 0.8945 - val_loss: 3.9378 - val_acc: 0.4796\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.3057 - acc: 0.8933 - val_loss: 4.0098 - val_acc: 0.4726\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 0.2970 - acc: 0.8978 - val_loss: 3.9468 - val_acc: 0.4796\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2963 - acc: 0.8972 - val_loss: 3.9426 - val_acc: 0.4817\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2958 - acc: 0.8963 - val_loss: 4.1286 - val_acc: 0.4595\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.2881 - acc: 0.9000 - val_loss: 3.7997 - val_acc: 0.4674\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2959 - acc: 0.8987 - val_loss: 4.0288 - val_acc: 0.4673\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2914 - acc: 0.9005 - val_loss: 3.8345 - val_acc: 0.4788\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2810 - acc: 0.9017 - val_loss: 3.9845 - val_acc: 0.4706\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2783 - acc: 0.9045 - val_loss: 4.2030 - val_acc: 0.4770\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2876 - acc: 0.8999 - val_loss: 4.0670 - val_acc: 0.4764\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2788 - acc: 0.9036 - val_loss: 3.9196 - val_acc: 0.4753\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2882 - acc: 0.9008 - val_loss: 3.9631 - val_acc: 0.4668\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.2770 - acc: 0.9041 - val_loss: 4.0070 - val_acc: 0.4772\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2881 - acc: 0.9010 - val_loss: 4.1008 - val_acc: 0.4694\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2765 - acc: 0.9040 - val_loss: 4.0413 - val_acc: 0.4740\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.2779 - acc: 0.9040 - val_loss: 3.9049 - val_acc: 0.4785\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.2762 - acc: 0.9041 - val_loss: 3.9370 - val_acc: 0.4756\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.2681 - acc: 0.9074 - val_loss: 3.9920 - val_acc: 0.4654\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2660 - acc: 0.9094 - val_loss: 3.9548 - val_acc: 0.4777\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.2714 - acc: 0.9077 - val_loss: 4.1512 - val_acc: 0.4743\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.2758 - acc: 0.9058 - val_loss: 3.9585 - val_acc: 0.4810\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.2635 - acc: 0.9111 - val_loss: 4.2146 - val_acc: 0.4681\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.2685 - acc: 0.9092 - val_loss: 4.0708 - val_acc: 0.4740\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.2640 - acc: 0.9084 - val_loss: 4.1675 - val_acc: 0.4566\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.2637 - acc: 0.9092 - val_loss: 4.1578 - val_acc: 0.4669\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2582 - acc: 0.9129 - val_loss: 4.1709 - val_acc: 0.4802\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2600 - acc: 0.9116 - val_loss: 4.4163 - val_acc: 0.4648\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2584 - acc: 0.9129 - val_loss: 4.2153 - val_acc: 0.4784\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.2618 - acc: 0.9114 - val_loss: 4.0396 - val_acc: 0.4645\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.2580 - acc: 0.9111 - val_loss: 4.3016 - val_acc: 0.4516\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2600 - acc: 0.9120 - val_loss: 4.3065 - val_acc: 0.4728\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.2495 - acc: 0.9147 - val_loss: 4.3668 - val_acc: 0.4696\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2589 - acc: 0.9122 - val_loss: 4.2532 - val_acc: 0.4659\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.2507 - acc: 0.9120 - val_loss: 4.2948 - val_acc: 0.4727\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.2525 - acc: 0.9143 - val_loss: 4.1847 - val_acc: 0.4754\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.2598 - acc: 0.9145 - val_loss: 4.3338 - val_acc: 0.4766\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.2491 - acc: 0.9155 - val_loss: 4.1558 - val_acc: 0.4728\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2448 - acc: 0.9174 - val_loss: 4.2789 - val_acc: 0.4799\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.2478 - acc: 0.9164 - val_loss: 4.1515 - val_acc: 0.4712\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.2437 - acc: 0.9177 - val_loss: 4.2589 - val_acc: 0.4636\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2460 - acc: 0.9170 - val_loss: 4.3951 - val_acc: 0.4671\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.2438 - acc: 0.9177 - val_loss: 4.1250 - val_acc: 0.4694\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2429 - acc: 0.9176 - val_loss: 4.3957 - val_acc: 0.4639\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.2410 - acc: 0.9171 - val_loss: 4.3990 - val_acc: 0.4665\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.2413 - acc: 0.9181 - val_loss: 4.3869 - val_acc: 0.4821\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.2441 - acc: 0.9170 - val_loss: 4.3423 - val_acc: 0.4704\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2356 - acc: 0.9205 - val_loss: 4.6109 - val_acc: 0.4717\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.2435 - acc: 0.9177 - val_loss: 4.5146 - val_acc: 0.4724\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2308 - acc: 0.9213 - val_loss: 4.4333 - val_acc: 0.4681\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.2377 - acc: 0.9206 - val_loss: 4.4337 - val_acc: 0.4603\n",
      "Epoch 199/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.2384 - acc: 0.9210 - val_loss: 4.4234 - val_acc: 0.4646\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.2297 - acc: 0.9229 - val_loss: 4.3268 - val_acc: 0.4734\n",
      "Expertiment with： AdaGrad Optimizer\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "hidden_layer4 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,746,506\n",
      "Trainable params: 1,746,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 14.4133 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 5s 90us/step - loss: 14.5061 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 14.5060 - acc: 0.1000 - val_loss: 14.5055 - val_acc: 0.1000\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 11.5278 - acc: 0.1042 - val_loss: 2.3516 - val_acc: 0.1221\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 2.0760 - acc: 0.2335 - val_loss: 1.9042 - val_acc: 0.3115\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 1.8678 - acc: 0.3255 - val_loss: 1.8047 - val_acc: 0.3551\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.7758 - acc: 0.3596 - val_loss: 1.7815 - val_acc: 0.3503\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.7113 - acc: 0.3846 - val_loss: 1.6867 - val_acc: 0.3986\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 1.6651 - acc: 0.4022 - val_loss: 1.7873 - val_acc: 0.3553\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.6264 - acc: 0.4199 - val_loss: 1.7131 - val_acc: 0.3847\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 1.5959 - acc: 0.4318 - val_loss: 1.5936 - val_acc: 0.4310\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 1.5656 - acc: 0.4438 - val_loss: 1.5995 - val_acc: 0.4216\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 1.5394 - acc: 0.4520 - val_loss: 1.6844 - val_acc: 0.4015\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.5176 - acc: 0.4595 - val_loss: 1.5897 - val_acc: 0.4372\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.4960 - acc: 0.4673 - val_loss: 1.5257 - val_acc: 0.4553\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.4740 - acc: 0.4782 - val_loss: 1.5152 - val_acc: 0.4600ss: 1.476\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 1.4536 - acc: 0.4823 - val_loss: 1.4749 - val_acc: 0.4767\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.4408 - acc: 0.4869 - val_loss: 1.5064 - val_acc: 0.4622\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 1.4194 - acc: 0.4956 - val_loss: 1.6253 - val_acc: 0.4306\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 1.4101 - acc: 0.4991 - val_loss: 1.5028 - val_acc: 0.4640\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 1.3935 - acc: 0.5030 - val_loss: 1.4446 - val_acc: 0.4896\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.3778 - acc: 0.5102 - val_loss: 1.4390 - val_acc: 0.4918\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.3625 - acc: 0.5159 - val_loss: 1.4775 - val_acc: 0.4807\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 1.3504 - acc: 0.5188 - val_loss: 1.4741 - val_acc: 0.4753\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 1.3368 - acc: 0.5266 - val_loss: 1.4856 - val_acc: 0.4798\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.3267 - acc: 0.5301 - val_loss: 1.4261 - val_acc: 0.4963\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 1.3143 - acc: 0.5325 - val_loss: 1.4477 - val_acc: 0.4924\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 1.3011 - acc: 0.5392 - val_loss: 1.4459 - val_acc: 0.4918\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.2940 - acc: 0.5394 - val_loss: 1.6108 - val_acc: 0.4423\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.2813 - acc: 0.5453 - val_loss: 1.4727 - val_acc: 0.4769\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.2711 - acc: 0.5482 - val_loss: 1.4913 - val_acc: 0.4823\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.2622 - acc: 0.5522 - val_loss: 1.3869 - val_acc: 0.5147\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 1.2489 - acc: 0.5594 - val_loss: 1.3958 - val_acc: 0.5098\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.2414 - acc: 0.5604 - val_loss: 1.4365 - val_acc: 0.5001\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.2301 - acc: 0.5630 - val_loss: 1.3951 - val_acc: 0.5069\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 1.2216 - acc: 0.5680 - val_loss: 1.4598 - val_acc: 0.4938\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 1.2109 - acc: 0.5719 - val_loss: 1.3793 - val_acc: 0.5168\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.2034 - acc: 0.5742 - val_loss: 1.4247 - val_acc: 0.5067\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 1.1940 - acc: 0.5783 - val_loss: 1.4156 - val_acc: 0.5074\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 1.1863 - acc: 0.5805 - val_loss: 1.4406 - val_acc: 0.5021\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 1.1731 - acc: 0.5853 - val_loss: 1.4103 - val_acc: 0.5115\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 1.1737 - acc: 0.5853 - val_loss: 1.3816 - val_acc: 0.5154\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 1.1592 - acc: 0.5907 - val_loss: 1.3943 - val_acc: 0.5216\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.1505 - acc: 0.5938 - val_loss: 1.3904 - val_acc: 0.5190\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 1.1451 - acc: 0.5963 - val_loss: 1.3926 - val_acc: 0.5232\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 1.1362 - acc: 0.5990 - val_loss: 1.4041 - val_acc: 0.5162\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.1278 - acc: 0.6000 - val_loss: 1.4277 - val_acc: 0.4941\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 1.1206 - acc: 0.6033 - val_loss: 1.3775 - val_acc: 0.5159\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.1133 - acc: 0.6066 - val_loss: 1.3925 - val_acc: 0.5242\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 1.1060 - acc: 0.6097 - val_loss: 1.4280 - val_acc: 0.5106\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 1.0961 - acc: 0.6126 - val_loss: 1.3945 - val_acc: 0.5164\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 1.0935 - acc: 0.6147 - val_loss: 1.4574 - val_acc: 0.5027\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 1.0827 - acc: 0.6187 - val_loss: 1.3795 - val_acc: 0.5249\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 1.0764 - acc: 0.6209 - val_loss: 1.3871 - val_acc: 0.5223\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 1.0698 - acc: 0.6249 - val_loss: 1.4099 - val_acc: 0.5167\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 1.0602 - acc: 0.6248 - val_loss: 1.3847 - val_acc: 0.5249\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 1.0580 - acc: 0.6272 - val_loss: 1.4035 - val_acc: 0.5159\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 1.0498 - acc: 0.6319 - val_loss: 1.4034 - val_acc: 0.5184\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 1.0411 - acc: 0.6330 - val_loss: 1.4269 - val_acc: 0.5166\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 1.0343 - acc: 0.6355 - val_loss: 1.4180 - val_acc: 0.5170\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 1.0298 - acc: 0.6388 - val_loss: 1.4244 - val_acc: 0.5163\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 1.0212 - acc: 0.6411 - val_loss: 1.4291 - val_acc: 0.5186\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 1.0192 - acc: 0.6425 - val_loss: 1.4701 - val_acc: 0.5033\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 1.0101 - acc: 0.6447 - val_loss: 1.3832 - val_acc: 0.5325\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 1.0019 - acc: 0.6495 - val_loss: 1.4004 - val_acc: 0.5289\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.9993 - acc: 0.6510 - val_loss: 1.4175 - val_acc: 0.5227\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.9897 - acc: 0.6541 - val_loss: 1.4192 - val_acc: 0.5220\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.9860 - acc: 0.6559 - val_loss: 1.4254 - val_acc: 0.5233\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.9799 - acc: 0.6566 - val_loss: 1.4777 - val_acc: 0.5129\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.9715 - acc: 0.6603 - val_loss: 1.4512 - val_acc: 0.5195\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.9693 - acc: 0.6616 - val_loss: 1.4172 - val_acc: 0.5194\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.9634 - acc: 0.6634 - val_loss: 1.4317 - val_acc: 0.5188\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.9592 - acc: 0.6649 - val_loss: 1.5007 - val_acc: 0.5064\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 5s 91us/step - loss: 0.9490 - acc: 0.6693 - val_loss: 1.4596 - val_acc: 0.5197\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.9410 - acc: 0.6724 - val_loss: 1.4706 - val_acc: 0.5189\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.9376 - acc: 0.6744 - val_loss: 1.4615 - val_acc: 0.5228\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 5s 110us/step - loss: 0.9316 - acc: 0.6751 - val_loss: 1.5046 - val_acc: 0.5055\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.9295 - acc: 0.6746 - val_loss: 1.4629 - val_acc: 0.5185\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.9214 - acc: 0.6789 - val_loss: 1.4608 - val_acc: 0.5156\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.9177 - acc: 0.6804 - val_loss: 1.4636 - val_acc: 0.5212\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.9123 - acc: 0.6824 - val_loss: 1.5071 - val_acc: 0.5162\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.9096 - acc: 0.6818 - val_loss: 1.4548 - val_acc: 0.5183\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.8995 - acc: 0.6866 - val_loss: 1.5451 - val_acc: 0.5034\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.8950 - acc: 0.6893 - val_loss: 1.4984 - val_acc: 0.5140\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.8903 - acc: 0.6911 - val_loss: 1.4631 - val_acc: 0.5230\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.8859 - acc: 0.6915 - val_loss: 1.4437 - val_acc: 0.5278\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.8813 - acc: 0.6934 - val_loss: 1.4699 - val_acc: 0.5236\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.8737 - acc: 0.6959 - val_loss: 1.4759 - val_acc: 0.5199\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.8694 - acc: 0.6987 - val_loss: 1.4863 - val_acc: 0.5221\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 4s 90us/step - loss: 0.8653 - acc: 0.6994 - val_loss: 1.4802 - val_acc: 0.5238\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 0.8606 - acc: 0.6998 - val_loss: 1.5379 - val_acc: 0.5146\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.8581 - acc: 0.7026 - val_loss: 1.4668 - val_acc: 0.5264\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.8515 - acc: 0.7045 - val_loss: 1.5119 - val_acc: 0.5191\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.8448 - acc: 0.7072 - val_loss: 1.4985 - val_acc: 0.5225\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.8423 - acc: 0.7090 - val_loss: 1.5060 - val_acc: 0.5262\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.8391 - acc: 0.7100 - val_loss: 1.5035 - val_acc: 0.5227\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.8304 - acc: 0.7114 - val_loss: 1.5023 - val_acc: 0.5214\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.8272 - acc: 0.7116 - val_loss: 1.5173 - val_acc: 0.5213\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.8248 - acc: 0.7149 - val_loss: 1.5654 - val_acc: 0.5140\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.8191 - acc: 0.7176 - val_loss: 1.5239 - val_acc: 0.5258\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.8147 - acc: 0.7196 - val_loss: 1.5401 - val_acc: 0.5163\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.8126 - acc: 0.7185 - val_loss: 1.5231 - val_acc: 0.5258\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.8073 - acc: 0.7201 - val_loss: 1.6101 - val_acc: 0.5027\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.8037 - acc: 0.7226 - val_loss: 1.5262 - val_acc: 0.5273\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.7988 - acc: 0.7244 - val_loss: 1.5343 - val_acc: 0.5221\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.7949 - acc: 0.7254 - val_loss: 1.5299 - val_acc: 0.5265\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.7891 - acc: 0.7270 - val_loss: 1.5571 - val_acc: 0.5190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.7880 - acc: 0.7284 - val_loss: 1.5394 - val_acc: 0.5303\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.7794 - acc: 0.7321 - val_loss: 1.5945 - val_acc: 0.5180\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.7769 - acc: 0.7340 - val_loss: 1.5484 - val_acc: 0.5271\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.7728 - acc: 0.7351 - val_loss: 1.6139 - val_acc: 0.5171\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.7688 - acc: 0.7349 - val_loss: 1.5647 - val_acc: 0.5236\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.7657 - acc: 0.7359 - val_loss: 1.5762 - val_acc: 0.5235\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.7613 - acc: 0.7377 - val_loss: 1.6042 - val_acc: 0.5113\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.7561 - acc: 0.7407 - val_loss: 1.5908 - val_acc: 0.5251\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 5s 95us/step - loss: 0.7543 - acc: 0.7405 - val_loss: 1.5959 - val_acc: 0.5209\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.7496 - acc: 0.7416 - val_loss: 1.6107 - val_acc: 0.5187\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.7452 - acc: 0.7442 - val_loss: 1.6125 - val_acc: 0.5197\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.7398 - acc: 0.7451 - val_loss: 1.5996 - val_acc: 0.5191\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 5s 93us/step - loss: 0.7404 - acc: 0.7439 - val_loss: 1.6516 - val_acc: 0.5180\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.7326 - acc: 0.7486 - val_loss: 1.6342 - val_acc: 0.5146\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.7296 - acc: 0.7502 - val_loss: 1.6186 - val_acc: 0.5226\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.7275 - acc: 0.7506 - val_loss: 1.6081 - val_acc: 0.5252\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.7217 - acc: 0.7527 - val_loss: 1.6461 - val_acc: 0.5148\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 5s 94us/step - loss: 0.7183 - acc: 0.7557 - val_loss: 1.6797 - val_acc: 0.5103\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.7160 - acc: 0.7552 - val_loss: 1.6517 - val_acc: 0.5144\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.7103 - acc: 0.7566 - val_loss: 1.6612 - val_acc: 0.5133\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.7092 - acc: 0.7574 - val_loss: 1.6794 - val_acc: 0.5167\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.7049 - acc: 0.7587 - val_loss: 1.6630 - val_acc: 0.5135\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 6s 116us/step - loss: 0.7011 - acc: 0.7595 - val_loss: 1.6711 - val_acc: 0.5171\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.6966 - acc: 0.7623 - val_loss: 1.6428 - val_acc: 0.5234\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.6914 - acc: 0.7642 - val_loss: 1.6843 - val_acc: 0.5170\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 0.6876 - acc: 0.7666 - val_loss: 1.6527 - val_acc: 0.5212\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.6845 - acc: 0.7662 - val_loss: 1.6914 - val_acc: 0.5164\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.6820 - acc: 0.7675 - val_loss: 1.7399 - val_acc: 0.5098\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.6792 - acc: 0.7689 - val_loss: 1.7256 - val_acc: 0.5140\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.6764 - acc: 0.7697 - val_loss: 1.6966 - val_acc: 0.5202\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.6698 - acc: 0.7733 - val_loss: 1.6996 - val_acc: 0.5185\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.6665 - acc: 0.7733 - val_loss: 1.7084 - val_acc: 0.5168\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.6632 - acc: 0.7752 - val_loss: 1.7325 - val_acc: 0.5098\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.6606 - acc: 0.7746 - val_loss: 1.7233 - val_acc: 0.5156\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.6562 - acc: 0.7769 - val_loss: 1.7581 - val_acc: 0.5096\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.6558 - acc: 0.7772 - val_loss: 1.7578 - val_acc: 0.5127\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.6512 - acc: 0.7790 - val_loss: 1.7232 - val_acc: 0.5196\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.6471 - acc: 0.7811 - val_loss: 1.7701 - val_acc: 0.5075\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 5s 110us/step - loss: 0.6441 - acc: 0.7824 - val_loss: 1.7400 - val_acc: 0.5170\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.6407 - acc: 0.7828 - val_loss: 1.7542 - val_acc: 0.5189\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.6346 - acc: 0.7852 - val_loss: 1.7134 - val_acc: 0.5269\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.6344 - acc: 0.7864 - val_loss: 1.7785 - val_acc: 0.5117\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.6302 - acc: 0.7883 - val_loss: 1.7622 - val_acc: 0.5171\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.6281 - acc: 0.7869 - val_loss: 1.7323 - val_acc: 0.5180\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.6232 - acc: 0.7896 - val_loss: 1.7967 - val_acc: 0.5130\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 0.6213 - acc: 0.7916 - val_loss: 1.7777 - val_acc: 0.5189\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.6181 - acc: 0.7923 - val_loss: 1.7618 - val_acc: 0.5169\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 0.6173 - acc: 0.7934 - val_loss: 1.8153 - val_acc: 0.5062\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 6s 117us/step - loss: 0.6124 - acc: 0.7936 - val_loss: 1.7632 - val_acc: 0.5218\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 6s 117us/step - loss: 0.6108 - acc: 0.7945 - val_loss: 1.9587 - val_acc: 0.4909\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.6077 - acc: 0.7961 - val_loss: 1.8137 - val_acc: 0.5171\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.6038 - acc: 0.7957 - val_loss: 1.8475 - val_acc: 0.5138\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.6003 - acc: 0.7986 - val_loss: 1.8277 - val_acc: 0.5208\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.5963 - acc: 0.8005 - val_loss: 1.8622 - val_acc: 0.5078\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.5931 - acc: 0.8016 - val_loss: 1.8218 - val_acc: 0.5154\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.5899 - acc: 0.8022 - val_loss: 1.8132 - val_acc: 0.5157\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.5867 - acc: 0.8036 - val_loss: 1.8176 - val_acc: 0.5183\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 6s 116us/step - loss: 0.5826 - acc: 0.8059 - val_loss: 1.8421 - val_acc: 0.5165\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.5825 - acc: 0.8051 - val_loss: 1.8928 - val_acc: 0.5078\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.5804 - acc: 0.8059 - val_loss: 1.9458 - val_acc: 0.4990\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.5768 - acc: 0.8075 - val_loss: 1.8396 - val_acc: 0.5186\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.5740 - acc: 0.8091 - val_loss: 1.8630 - val_acc: 0.5124\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.5715 - acc: 0.8100 - val_loss: 1.8570 - val_acc: 0.5221\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 0.5653 - acc: 0.8118 - val_loss: 1.9678 - val_acc: 0.4983\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 0.5652 - acc: 0.8122 - val_loss: 1.9396 - val_acc: 0.5076\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 0.5617 - acc: 0.8123 - val_loss: 1.8783 - val_acc: 0.5180\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.5579 - acc: 0.8146 - val_loss: 1.9131 - val_acc: 0.5131\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.5560 - acc: 0.8155 - val_loss: 1.8852 - val_acc: 0.5162\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.5526 - acc: 0.8158 - val_loss: 1.8996 - val_acc: 0.5158\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.5513 - acc: 0.8169 - val_loss: 1.8867 - val_acc: 0.5214\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.5457 - acc: 0.8192 - val_loss: 1.9922 - val_acc: 0.4995\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.5440 - acc: 0.8201 - val_loss: 1.9059 - val_acc: 0.5156\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.5413 - acc: 0.8199 - val_loss: 1.9289 - val_acc: 0.5139\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.5392 - acc: 0.8212 - val_loss: 1.9802 - val_acc: 0.5071\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.5363 - acc: 0.8226 - val_loss: 1.9383 - val_acc: 0.5111\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.5347 - acc: 0.8242 - val_loss: 1.9200 - val_acc: 0.5177\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.5312 - acc: 0.8254 - val_loss: 1.9333 - val_acc: 0.5137\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 5s 97us/step - loss: 0.5275 - acc: 0.8259 - val_loss: 1.9814 - val_acc: 0.5107\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.5249 - acc: 0.8273 - val_loss: 1.9598 - val_acc: 0.5128\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.5219 - acc: 0.8283 - val_loss: 2.0004 - val_acc: 0.5079\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.5203 - acc: 0.8284 - val_loss: 2.0284 - val_acc: 0.5042\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 5s 92us/step - loss: 0.5186 - acc: 0.8299 - val_loss: 1.9564 - val_acc: 0.5146\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 5s 96us/step - loss: 0.5136 - acc: 0.8312 - val_loss: 1.9637 - val_acc: 0.5122\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.5125 - acc: 0.8331 - val_loss: 1.9977 - val_acc: 0.5105\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 4s 88us/step - loss: 0.5115 - acc: 0.8314 - val_loss: 1.9884 - val_acc: 0.5125\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 4s 90us/step - loss: 0.5082 - acc: 0.8339 - val_loss: 1.9917 - val_acc: 0.5158\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 5s 99us/step - loss: 0.5057 - acc: 0.8347 - val_loss: 1.9900 - val_acc: 0.5130\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.5010 - acc: 0.8370 - val_loss: 2.0119 - val_acc: 0.5130\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.5006 - acc: 0.8369 - val_loss: 2.0144 - val_acc: 0.5151\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.4981 - acc: 0.8387 - val_loss: 2.0413 - val_acc: 0.5082\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 5s 98us/step - loss: 0.4954 - acc: 0.8372 - val_loss: 2.0392 - val_acc: 0.5104\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.4929 - acc: 0.8395 - val_loss: 2.0383 - val_acc: 0.5069\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 0.4905 - acc: 0.8387 - val_loss: 2.0693 - val_acc: 0.5080\n",
      "Expertiment with： Adam Optimizer\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "hidden_layer4 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 1,746,506\n",
      "Trainable params: 1,746,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 8s 164us/step - loss: 1.9364 - acc: 0.2956 - val_loss: 1.8048 - val_acc: 0.3552\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 1.7215 - acc: 0.3819 - val_loss: 1.6645 - val_acc: 0.4065\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 1.6282 - acc: 0.4184 - val_loss: 1.6007 - val_acc: 0.4287\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 1.5668 - acc: 0.4401 - val_loss: 1.5431 - val_acc: 0.4515\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 1.5226 - acc: 0.4541 - val_loss: 1.5345 - val_acc: 0.4594\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 6s 114us/step - loss: 1.4876 - acc: 0.4701 - val_loss: 1.4782 - val_acc: 0.4715\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 1.4502 - acc: 0.4810 - val_loss: 1.4669 - val_acc: 0.4756\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 1.4167 - acc: 0.4939 - val_loss: 1.4495 - val_acc: 0.4864\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 1.3919 - acc: 0.4993 - val_loss: 1.4624 - val_acc: 0.4783\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 1.3625 - acc: 0.5135 - val_loss: 1.4286 - val_acc: 0.4946\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 6s 112us/step - loss: 1.3384 - acc: 0.5193 - val_loss: 1.4024 - val_acc: 0.5006\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 1.3283 - acc: 0.5253 - val_loss: 1.4207 - val_acc: 0.4949\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 6s 123us/step - loss: 1.2939 - acc: 0.5376 - val_loss: 1.4081 - val_acc: 0.4971\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 7s 135us/step - loss: 1.2662 - acc: 0.5500 - val_loss: 1.4220 - val_acc: 0.4931\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 7s 131us/step - loss: 1.2418 - acc: 0.5566 - val_loss: 1.3876 - val_acc: 0.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 1.2239 - acc: 0.5628 - val_loss: 1.4140 - val_acc: 0.5024\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 1.2013 - acc: 0.5701 - val_loss: 1.4097 - val_acc: 0.5089\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 6s 127us/step - loss: 1.1866 - acc: 0.5765 - val_loss: 1.4161 - val_acc: 0.5054\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 6s 120us/step - loss: 1.1590 - acc: 0.5850 - val_loss: 1.3849 - val_acc: 0.5093\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 1.1367 - acc: 0.5951 - val_loss: 1.4142 - val_acc: 0.5094\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 1.1244 - acc: 0.5994 - val_loss: 1.3726 - val_acc: 0.5262\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 1.1055 - acc: 0.6050 - val_loss: 1.3712 - val_acc: 0.5234\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 1.0859 - acc: 0.6132 - val_loss: 1.3913 - val_acc: 0.5126\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 1.0587 - acc: 0.6225 - val_loss: 1.4205 - val_acc: 0.5159\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 1.0467 - acc: 0.6297 - val_loss: 1.4297 - val_acc: 0.5136\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 1.0298 - acc: 0.6337 - val_loss: 1.4076 - val_acc: 0.5203\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 1.0086 - acc: 0.6383 - val_loss: 1.4012 - val_acc: 0.5225\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 1.0028 - acc: 0.6426 - val_loss: 1.4301 - val_acc: 0.5098\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.9799 - acc: 0.6474 - val_loss: 1.4434 - val_acc: 0.5183\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 7s 137us/step - loss: 0.9590 - acc: 0.6564 - val_loss: 1.4636 - val_acc: 0.5187\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 6s 129us/step - loss: 0.9317 - acc: 0.6681 - val_loss: 1.4202 - val_acc: 0.5259\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 0.9146 - acc: 0.6732 - val_loss: 1.4707 - val_acc: 0.5235\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.9053 - acc: 0.6759 - val_loss: 1.4494 - val_acc: 0.5254\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.8895 - acc: 0.6832 - val_loss: 1.4580 - val_acc: 0.5219\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 6s 122us/step - loss: 0.8655 - acc: 0.6902 - val_loss: 1.4914 - val_acc: 0.5152\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.8550 - acc: 0.6931 - val_loss: 1.5078 - val_acc: 0.5152\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 6s 126us/step - loss: 0.8374 - acc: 0.6983 - val_loss: 1.5223 - val_acc: 0.5251\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.8223 - acc: 0.7059 - val_loss: 1.5245 - val_acc: 0.5155\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 6s 124us/step - loss: 0.8148 - acc: 0.7087 - val_loss: 1.5443 - val_acc: 0.5246\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 6s 125us/step - loss: 0.7917 - acc: 0.7162 - val_loss: 1.5434 - val_acc: 0.5251\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.7625 - acc: 0.7271 - val_loss: 1.6166 - val_acc: 0.5073\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 7s 134us/step - loss: 0.7702 - acc: 0.7229 - val_loss: 1.6027 - val_acc: 0.5141\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.7487 - acc: 0.7324 - val_loss: 1.6606 - val_acc: 0.5117\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.7295 - acc: 0.7383 - val_loss: 1.6922 - val_acc: 0.5152\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.7133 - acc: 0.7436 - val_loss: 1.6549 - val_acc: 0.5179\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.7106 - acc: 0.7459 - val_loss: 1.7269 - val_acc: 0.5044\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.6861 - acc: 0.7540 - val_loss: 1.7255 - val_acc: 0.5082\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 6s 121us/step - loss: 0.6830 - acc: 0.7541 - val_loss: 1.7212 - val_acc: 0.5208\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.6681 - acc: 0.7605 - val_loss: 1.7942 - val_acc: 0.5128\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 6s 115us/step - loss: 0.6576 - acc: 0.7621 - val_loss: 1.8202 - val_acc: 0.5077\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.6421 - acc: 0.7705 - val_loss: 1.7383 - val_acc: 0.5181\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 6s 126us/step - loss: 0.6144 - acc: 0.7802 - val_loss: 1.8671 - val_acc: 0.5063\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.6302 - acc: 0.7724 - val_loss: 1.8307 - val_acc: 0.5096\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.5922 - acc: 0.7876 - val_loss: 1.8937 - val_acc: 0.5080\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 7s 132us/step - loss: 0.5958 - acc: 0.7882 - val_loss: 1.8682 - val_acc: 0.5134\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 6s 128us/step - loss: 0.5732 - acc: 0.7953 - val_loss: 1.9456 - val_acc: 0.5080\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 6s 116us/step - loss: 0.5654 - acc: 0.7990 - val_loss: 2.0126 - val_acc: 0.5126\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 6s 116us/step - loss: 0.5687 - acc: 0.7959 - val_loss: 1.9860 - val_acc: 0.5101\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 0.5472 - acc: 0.8036 - val_loss: 2.0313 - val_acc: 0.5007\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 7s 138us/step - loss: 0.5442 - acc: 0.8053 - val_loss: 1.9676 - val_acc: 0.5108\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.5183 - acc: 0.8146 - val_loss: 2.0483 - val_acc: 0.5101\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 5s 108us/step - loss: 0.5258 - acc: 0.8113 - val_loss: 2.0652 - val_acc: 0.5114\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.5099 - acc: 0.8164 - val_loss: 2.1458 - val_acc: 0.5114\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.5006 - acc: 0.8189 - val_loss: 2.0882 - val_acc: 0.5152\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 5s 110us/step - loss: 0.5068 - acc: 0.8180 - val_loss: 2.1388 - val_acc: 0.5050\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.4830 - acc: 0.8266 - val_loss: 2.1897 - val_acc: 0.5096\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.4684 - acc: 0.8316 - val_loss: 2.3053 - val_acc: 0.4974\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.4586 - acc: 0.8342 - val_loss: 2.2572 - val_acc: 0.5092\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.4667 - acc: 0.8322 - val_loss: 2.3222 - val_acc: 0.5026\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.4422 - acc: 0.8413 - val_loss: 2.3547 - val_acc: 0.5090\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.4749 - acc: 0.8297 - val_loss: 2.3847 - val_acc: 0.4908\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 6s 113us/step - loss: 0.4483 - acc: 0.8391 - val_loss: 2.3201 - val_acc: 0.5035\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.4307 - acc: 0.8452 - val_loss: 2.3914 - val_acc: 0.4948\n",
      "Epoch 74/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.4136 - acc: 0.8503 - val_loss: 2.4250 - val_acc: 0.5038\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.4112 - acc: 0.8507 - val_loss: 2.4566 - val_acc: 0.5072\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.4110 - acc: 0.8507 - val_loss: 2.5357 - val_acc: 0.5060\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.4212 - acc: 0.8469 - val_loss: 2.4695 - val_acc: 0.5027\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.4151 - acc: 0.8502 - val_loss: 2.5497 - val_acc: 0.5034\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.3794 - acc: 0.8634 - val_loss: 2.5575 - val_acc: 0.5074\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.3730 - acc: 0.8668 - val_loss: 2.5461 - val_acc: 0.5062\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.3796 - acc: 0.8627 - val_loss: 2.5547 - val_acc: 0.5020\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.3870 - acc: 0.8607 - val_loss: 2.6237 - val_acc: 0.5036\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.3659 - acc: 0.8683 - val_loss: 2.6604 - val_acc: 0.4980\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3715 - acc: 0.8654 - val_loss: 2.6648 - val_acc: 0.5070\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3605 - acc: 0.8699 - val_loss: 2.6969 - val_acc: 0.5001\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3831 - acc: 0.8628 - val_loss: 2.7052 - val_acc: 0.5125\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.3547 - acc: 0.8725 - val_loss: 2.7781 - val_acc: 0.4996\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.3381 - acc: 0.8800 - val_loss: 2.7392 - val_acc: 0.5080\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.3460 - acc: 0.8770 - val_loss: 2.7966 - val_acc: 0.4955\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.3340 - acc: 0.8811 - val_loss: 2.8547 - val_acc: 0.5104\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3370 - acc: 0.8787 - val_loss: 2.8161 - val_acc: 0.5030\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.3484 - acc: 0.8750 - val_loss: 2.8227 - val_acc: 0.5069\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3217 - acc: 0.8860 - val_loss: 2.7874 - val_acc: 0.4961\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3250 - acc: 0.8840 - val_loss: 2.7948 - val_acc: 0.5027\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.3154 - acc: 0.8870 - val_loss: 2.9096 - val_acc: 0.5049\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3129 - acc: 0.8872 - val_loss: 2.9080 - val_acc: 0.5017\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.3040 - acc: 0.8906 - val_loss: 3.0335 - val_acc: 0.5037\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.3190 - acc: 0.8841 - val_loss: 3.0710 - val_acc: 0.5015\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.3204 - acc: 0.8852 - val_loss: 2.9966 - val_acc: 0.5014\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3291 - acc: 0.8822 - val_loss: 2.9197 - val_acc: 0.5036\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.2901 - acc: 0.8952 - val_loss: 2.9739 - val_acc: 0.4932\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2947 - acc: 0.8949 - val_loss: 3.0976 - val_acc: 0.4974\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.2740 - acc: 0.9013 - val_loss: 3.0667 - val_acc: 0.4996\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.2999 - acc: 0.8939 - val_loss: 3.1316 - val_acc: 0.4962\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.2883 - acc: 0.8967 - val_loss: 3.0798 - val_acc: 0.5057\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2798 - acc: 0.9008 - val_loss: 3.2347 - val_acc: 0.4938\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2811 - acc: 0.9010 - val_loss: 3.1429 - val_acc: 0.4994\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2876 - acc: 0.8961 - val_loss: 3.2247 - val_acc: 0.4927\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.3067 - acc: 0.8904 - val_loss: 3.1149 - val_acc: 0.4910\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.2975 - acc: 0.8933 - val_loss: 3.1382 - val_acc: 0.4928\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2507 - acc: 0.9106 - val_loss: 3.1915 - val_acc: 0.5006\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2841 - acc: 0.8974 - val_loss: 3.2629 - val_acc: 0.4965\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.2616 - acc: 0.9056 - val_loss: 3.1764 - val_acc: 0.5027\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 7s 139us/step - loss: 0.2642 - acc: 0.9054 - val_loss: 3.2070 - val_acc: 0.5044\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.2664 - acc: 0.9040 - val_loss: 3.3435 - val_acc: 0.5020\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2506 - acc: 0.9091 - val_loss: 3.3448 - val_acc: 0.4928\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.2487 - acc: 0.9106 - val_loss: 3.3238 - val_acc: 0.4937\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.2861 - acc: 0.8982 - val_loss: 3.3275 - val_acc: 0.4924\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.2410 - acc: 0.9139 - val_loss: 3.3211 - val_acc: 0.4962\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2674 - acc: 0.9055 - val_loss: 3.3363 - val_acc: 0.4962\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2430 - acc: 0.9129 - val_loss: 3.3089 - val_acc: 0.5053\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2326 - acc: 0.9163 - val_loss: 3.4392 - val_acc: 0.4974\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2267 - acc: 0.9186 - val_loss: 3.5412 - val_acc: 0.4998\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2569 - acc: 0.9093 - val_loss: 3.4159 - val_acc: 0.4971\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.2601 - acc: 0.9082 - val_loss: 3.4515 - val_acc: 0.4998\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2251 - acc: 0.9185 - val_loss: 3.4834 - val_acc: 0.4959\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.2394 - acc: 0.9135 - val_loss: 3.5104 - val_acc: 0.4955\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2314 - acc: 0.9183 - val_loss: 3.4907 - val_acc: 0.4989\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2526 - acc: 0.9100 - val_loss: 3.5258 - val_acc: 0.5013\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 6s 118us/step - loss: 0.2158 - acc: 0.9237 - val_loss: 3.5010 - val_acc: 0.5019\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 6s 119us/step - loss: 0.2574 - acc: 0.9081 - val_loss: 3.5031 - val_acc: 0.4889\n",
      "Epoch 132/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.2213 - acc: 0.9212 - val_loss: 3.6067 - val_acc: 0.4988\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2281 - acc: 0.9181 - val_loss: 3.5427 - val_acc: 0.4995\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.2290 - acc: 0.9197 - val_loss: 3.6160 - val_acc: 0.5019\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2124 - acc: 0.9248 - val_loss: 3.5797 - val_acc: 0.4947\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.2688 - acc: 0.9051 - val_loss: 3.6701 - val_acc: 0.4897\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 6s 110us/step - loss: 0.2351 - acc: 0.9164 - val_loss: 3.5684 - val_acc: 0.4957\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.2069 - acc: 0.9263 - val_loss: 3.5617 - val_acc: 0.5022\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.1957 - acc: 0.9292 - val_loss: 3.6187 - val_acc: 0.5002\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.1906 - acc: 0.9328 - val_loss: 3.6842 - val_acc: 0.4985\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2276 - acc: 0.9196 - val_loss: 3.7104 - val_acc: 0.4915\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 6s 111us/step - loss: 0.2558 - acc: 0.9099 - val_loss: 3.6169 - val_acc: 0.4956\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.2107 - acc: 0.9240 - val_loss: 3.6509 - val_acc: 0.4948\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.1975 - acc: 0.9299 - val_loss: 3.6313 - val_acc: 0.5033 0s - loss: 0.1950 - acc: 0.9\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.1992 - acc: 0.9282 - val_loss: 3.5976 - val_acc: 0.5060\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2229 - acc: 0.9211 - val_loss: 3.6109 - val_acc: 0.4998\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.1943 - acc: 0.9303 - val_loss: 3.7770 - val_acc: 0.4990\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 5s 109us/step - loss: 0.2224 - acc: 0.9215 - val_loss: 3.7098 - val_acc: 0.5055\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2013 - acc: 0.9291 - val_loss: 3.7745 - val_acc: 0.4961\n",
      "Epoch 150/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2280 - acc: 0.9188 - val_loss: 3.6368 - val_acc: 0.4966\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2208 - acc: 0.9230 - val_loss: 3.6911 - val_acc: 0.4952\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.1954 - acc: 0.9305 - val_loss: 3.7534 - val_acc: 0.5005\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.1639 - acc: 0.9415 - val_loss: 3.7652 - val_acc: 0.5039\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.1630 - acc: 0.9432 - val_loss: 3.7975 - val_acc: 0.5024\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2162 - acc: 0.9239 - val_loss: 3.7790 - val_acc: 0.4951\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2067 - acc: 0.9265 - val_loss: 3.8509 - val_acc: 0.4929\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2045 - acc: 0.9272 - val_loss: 3.8756 - val_acc: 0.4917\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.1808 - acc: 0.9357 - val_loss: 3.8766 - val_acc: 0.4982\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.2025 - acc: 0.9279 - val_loss: 3.8425 - val_acc: 0.4925\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.1511 - acc: 0.9479 - val_loss: 3.8713 - val_acc: 0.4982\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.1432 - acc: 0.9499 - val_loss: 4.0762 - val_acc: 0.4920\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2305 - acc: 0.9189 - val_loss: 3.9268 - val_acc: 0.4940\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.2157 - acc: 0.9245 - val_loss: 3.9639 - val_acc: 0.4907\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.1788 - acc: 0.9374 - val_loss: 3.8548 - val_acc: 0.5003\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.1759 - acc: 0.9377 - val_loss: 3.8873 - val_acc: 0.4994\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.1513 - acc: 0.9457 - val_loss: 4.0564 - val_acc: 0.4928\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 5s 100us/step - loss: 0.1907 - acc: 0.9334 - val_loss: 3.9918 - val_acc: 0.4932\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2426 - acc: 0.9160 - val_loss: 3.9404 - val_acc: 0.4984\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.1778 - acc: 0.9380 - val_loss: 3.9844 - val_acc: 0.4974\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 5s 107us/step - loss: 0.1871 - acc: 0.9336 - val_loss: 4.0029 - val_acc: 0.4959\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.1494 - acc: 0.9482 - val_loss: 3.8668 - val_acc: 0.5013\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.1801 - acc: 0.9369 - val_loss: 3.9428 - val_acc: 0.4965\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.2058 - acc: 0.9277 - val_loss: 3.9422 - val_acc: 0.5061\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.1672 - acc: 0.9419 - val_loss: 4.0357 - val_acc: 0.5009\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.1696 - acc: 0.9394 - val_loss: 4.0485 - val_acc: 0.4939\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.1764 - acc: 0.9367 - val_loss: 4.0256 - val_acc: 0.4969\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.1572 - acc: 0.9443 - val_loss: 3.9641 - val_acc: 0.4994\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.1646 - acc: 0.9416 - val_loss: 3.9737 - val_acc: 0.4946\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.1867 - acc: 0.9351 - val_loss: 4.0185 - val_acc: 0.4927\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.2123 - acc: 0.9282 - val_loss: 3.9834 - val_acc: 0.4978\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.1688 - acc: 0.9417 - val_loss: 4.0474 - val_acc: 0.4925\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.1483 - acc: 0.9473 - val_loss: 4.0971 - val_acc: 0.4931\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.1868 - acc: 0.9348 - val_loss: 4.1439 - val_acc: 0.4931\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.1397 - acc: 0.9510 - val_loss: 4.0446 - val_acc: 0.4949\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.1786 - acc: 0.9373 - val_loss: 4.0907 - val_acc: 0.4941\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.1961 - acc: 0.9321 - val_loss: 4.0425 - val_acc: 0.5020\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 5s 104us/step - loss: 0.1538 - acc: 0.9467 - val_loss: 4.1260 - val_acc: 0.4975\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 5s 106us/step - loss: 0.1551 - acc: 0.9459 - val_loss: 4.0692 - val_acc: 0.4953\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 5s 105us/step - loss: 0.1588 - acc: 0.9443 - val_loss: 4.0989 - val_acc: 0.4978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.1736 - acc: 0.9396 - val_loss: 4.1220 - val_acc: 0.4892\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.1460 - acc: 0.9488 - val_loss: 4.1207 - val_acc: 0.4983\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.1442 - acc: 0.9493 - val_loss: 4.2043 - val_acc: 0.4973\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.1202 - acc: 0.9577 - val_loss: 4.3861 - val_acc: 0.4953\n",
      "Epoch 194/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.2384 - acc: 0.9204 - val_loss: 4.0851 - val_acc: 0.4920\n",
      "Epoch 195/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.1798 - acc: 0.9372 - val_loss: 4.0681 - val_acc: 0.4965\n",
      "Epoch 196/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.1693 - acc: 0.9416 - val_loss: 4.0951 - val_acc: 0.5029\n",
      "Epoch 197/200\n",
      "50000/50000 [==============================] - 5s 102us/step - loss: 0.1199 - acc: 0.9573 - val_loss: 4.2032 - val_acc: 0.4959\n",
      "Epoch 198/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.1357 - acc: 0.9525 - val_loss: 4.0995 - val_acc: 0.4888\n",
      "Epoch 199/200\n",
      "50000/50000 [==============================] - 5s 101us/step - loss: 0.1595 - acc: 0.9436 - val_loss: 4.1935 - val_acc: 0.4935\n",
      "Epoch 200/200\n",
      "50000/50000 [==============================] - 5s 103us/step - loss: 0.1865 - acc: 0.9346 - val_loss: 4.2517 - val_acc: 0.4922\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for opt in ['SGD', 'RMSprop', 'AdaGrad', 'Adam']:\n",
    "    keras.backend.clear_session() # 清除舊的Graph\n",
    "    print('Expertiment with： ' + opt + ' Optimizer')\n",
    "    model = build_mlp(input_shape=x_train.shape[1:])\n",
    "    model.summary()\n",
    "    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "    \n",
    "    # collect results\n",
    "    train_loss = model.history.history[\"loss\"]\n",
    "    valid_loss = model.history.history[\"val_loss\"]\n",
    "    train_acc = model.history.history[\"acc\"]\n",
    "    valid_acc = model.history.history[\"val_acc\"]\n",
    "    \n",
    "    exp_name_tag = opt\n",
    "    results[exp_name_tag] = {'train-loss':train_loss,\n",
    "                             'valid_loss':valid_loss,\n",
    "                             'train_acc':train_acc,\n",
    "                             'valid_acc':valid_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T13:43:28.251955Z",
     "start_time": "2019-07-24T13:43:28.174578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SGD': {'train-loss': [2.1094554739379885,\n",
       "   1.9183289733505249,\n",
       "   1.8380871127700806,\n",
       "   1.7885096714019775,\n",
       "   1.7470383148574828,\n",
       "   1.7160822208023072,\n",
       "   1.6819220851898193,\n",
       "   1.657896495399475,\n",
       "   1.6368890549468995,\n",
       "   1.6219465213775635,\n",
       "   1.601559584236145,\n",
       "   1.5830116744613647,\n",
       "   1.5628825957107544,\n",
       "   1.542240098838806,\n",
       "   1.533143710975647,\n",
       "   1.5119280260849,\n",
       "   1.5040777142715454,\n",
       "   1.489886755065918,\n",
       "   1.4751118376922607,\n",
       "   1.4590541507720947,\n",
       "   1.4533490943908691,\n",
       "   1.4385529949188232,\n",
       "   1.4292662885665894,\n",
       "   1.4120302393722535,\n",
       "   1.400716173324585,\n",
       "   1.3930808349227906,\n",
       "   1.3782202801132202,\n",
       "   1.3760324933242798,\n",
       "   1.3629956119155884,\n",
       "   1.3512732209396363,\n",
       "   1.3413743862152099,\n",
       "   1.3346431946182251,\n",
       "   1.325443013496399,\n",
       "   1.3199739004135131,\n",
       "   1.307470503578186,\n",
       "   1.2981657315826416,\n",
       "   1.2916247922515869,\n",
       "   1.2825803047943116,\n",
       "   1.2753215221786498,\n",
       "   1.2642635884857178,\n",
       "   1.2532582985305787,\n",
       "   1.248076017150879,\n",
       "   1.2471706657791137,\n",
       "   1.2318545087432862,\n",
       "   1.2245833098602295,\n",
       "   1.216142540550232,\n",
       "   1.2137450172042847,\n",
       "   1.1997553169250488,\n",
       "   1.1943918302536012,\n",
       "   1.1872372990417481,\n",
       "   1.1807535928344726,\n",
       "   1.17668255027771,\n",
       "   1.164944095916748,\n",
       "   1.15724251121521,\n",
       "   1.148926441116333,\n",
       "   1.1472873804473878,\n",
       "   1.1360625970458984,\n",
       "   1.1261529458618165,\n",
       "   1.1212257388305664,\n",
       "   1.110941660118103,\n",
       "   1.113398994064331,\n",
       "   1.1025526084518433,\n",
       "   1.0970466731262207,\n",
       "   1.0864781119155884,\n",
       "   1.0769067541503907,\n",
       "   1.068353861579895,\n",
       "   1.0617572917175293,\n",
       "   1.0560058853530885,\n",
       "   1.0538897847747803,\n",
       "   1.0453398710632325,\n",
       "   1.0368467699050903,\n",
       "   1.033901796760559,\n",
       "   1.0211920636749268,\n",
       "   1.0131677631378173,\n",
       "   1.0063479085922242,\n",
       "   0.9978368078422546,\n",
       "   0.9916778729248047,\n",
       "   0.981871760520935,\n",
       "   0.9819567958831787,\n",
       "   0.9693285357284546,\n",
       "   0.972067912902832,\n",
       "   0.9584646501541138,\n",
       "   0.9475347787666321,\n",
       "   0.9447517468070984,\n",
       "   0.9342466624641419,\n",
       "   0.9299725671195984,\n",
       "   0.9219470414352418,\n",
       "   0.9109410082817078,\n",
       "   0.9121843576049805,\n",
       "   0.8965499645996093,\n",
       "   0.8990128899955749,\n",
       "   0.8867809331893921,\n",
       "   0.8823167824935914,\n",
       "   0.874817573184967,\n",
       "   0.8649187689971923,\n",
       "   0.8645498662948609,\n",
       "   0.8588125033950805,\n",
       "   0.8479491185188294,\n",
       "   0.8299241659927368,\n",
       "   0.842415726146698,\n",
       "   0.8205844321060181,\n",
       "   0.8150399028015137,\n",
       "   0.8166791876411438,\n",
       "   0.8033671051025391,\n",
       "   0.7971724781036377,\n",
       "   0.7988232350158692,\n",
       "   0.7838751099205017,\n",
       "   0.7761838022041321,\n",
       "   0.7633596449089051,\n",
       "   0.776318664560318,\n",
       "   0.7599641871261597,\n",
       "   0.7531142713356018,\n",
       "   0.7357574468612671,\n",
       "   0.7361329650688171,\n",
       "   0.7254330013656616,\n",
       "   0.718121157245636,\n",
       "   0.712474797372818,\n",
       "   0.713329015083313,\n",
       "   0.7146810481643677,\n",
       "   0.7030073170471192,\n",
       "   0.6851314625358581,\n",
       "   0.7068553051376343,\n",
       "   0.6786337284278869,\n",
       "   0.6795730508613587,\n",
       "   0.6706226773452759,\n",
       "   0.6555954957580566,\n",
       "   0.6475006209182739,\n",
       "   0.6516401868820191,\n",
       "   0.6422613470458984,\n",
       "   0.6824508646965027,\n",
       "   0.6193285856437684,\n",
       "   0.6196684972000122,\n",
       "   0.607334954032898,\n",
       "   0.6104778809356689,\n",
       "   0.6088192049789428,\n",
       "   0.5943709722518921,\n",
       "   0.5862877096748352,\n",
       "   0.5825529968643188,\n",
       "   0.578818848991394,\n",
       "   0.5572014704036713,\n",
       "   0.5572720337677002,\n",
       "   0.573107055234909,\n",
       "   0.5646872904205322,\n",
       "   0.5358932092285156,\n",
       "   0.5451276673316956,\n",
       "   0.5490242063140869,\n",
       "   0.5214939004516601,\n",
       "   0.5190383473491669,\n",
       "   0.5172387949180604,\n",
       "   0.57717534491539,\n",
       "   0.5073289859867096,\n",
       "   0.48060934686660767,\n",
       "   0.5007969363403321,\n",
       "   0.5225968647193908,\n",
       "   0.4600061289787292,\n",
       "   0.45039220542907715,\n",
       "   0.4831931190109253,\n",
       "   0.47575732946395877,\n",
       "   0.44611055136680605,\n",
       "   0.4861936041069031,\n",
       "   0.41704785283088686,\n",
       "   0.45493007358551024,\n",
       "   0.4248012745475769,\n",
       "   0.4158978427553177,\n",
       "   0.4174137958908081,\n",
       "   0.4088981719779968,\n",
       "   0.4517307543087006,\n",
       "   0.4113387702560425,\n",
       "   0.38133371660232546,\n",
       "   0.3912434320449829,\n",
       "   0.39310102890491483,\n",
       "   0.37157606170654295,\n",
       "   0.39626994618415834,\n",
       "   0.38661160327911376,\n",
       "   0.40624599729537963,\n",
       "   0.46708013215065003,\n",
       "   0.37338340487480165,\n",
       "   0.3228101736974716,\n",
       "   0.3558055325317383,\n",
       "   0.3595117968559265,\n",
       "   0.34968724702835086,\n",
       "   0.335218124089241,\n",
       "   0.28501829069137574,\n",
       "   0.3429136397647858,\n",
       "   0.3182086858701706,\n",
       "   0.30004349910736083,\n",
       "   0.3531745100355148,\n",
       "   0.32351786420822143,\n",
       "   0.3431563896846771,\n",
       "   0.2745853565979004,\n",
       "   0.2710606973457336,\n",
       "   0.38704606209754944,\n",
       "   0.3456145422410965,\n",
       "   0.22146591863632203,\n",
       "   0.29233747676849364,\n",
       "   0.3195922851896286,\n",
       "   0.30769146221160887,\n",
       "   0.22562273726940155,\n",
       "   0.22133741003990173,\n",
       "   0.39989487483501435],\n",
       "  'valid_loss': [1.977646411705017,\n",
       "   1.8915011854171753,\n",
       "   1.8492634716033935,\n",
       "   1.8525204811096192,\n",
       "   1.7146900606155395,\n",
       "   1.8031952680587768,\n",
       "   1.6782925693511963,\n",
       "   1.78076372756958,\n",
       "   1.6493014560699464,\n",
       "   1.661248882484436,\n",
       "   1.6954242334365845,\n",
       "   1.612743253326416,\n",
       "   1.5753537776947022,\n",
       "   1.5499640018463134,\n",
       "   1.6008810226440429,\n",
       "   1.5715831680297851,\n",
       "   1.5652508241653442,\n",
       "   1.5130847078323364,\n",
       "   1.5477340133666992,\n",
       "   1.6001782016754151,\n",
       "   1.519305783843994,\n",
       "   1.5614894260406493,\n",
       "   1.5027593551635743,\n",
       "   1.4976188568115234,\n",
       "   1.5296963466644287,\n",
       "   1.4880223079681396,\n",
       "   1.5655065868377687,\n",
       "   1.5141553924560547,\n",
       "   1.4552011486053467,\n",
       "   1.4287569332122803,\n",
       "   1.4259987922668458,\n",
       "   1.4020961500167846,\n",
       "   1.5320340034484863,\n",
       "   1.4007638088226317,\n",
       "   1.5161273977279663,\n",
       "   1.3679620338439942,\n",
       "   1.5568438430786133,\n",
       "   1.414999542617798,\n",
       "   1.3880885774612426,\n",
       "   1.4713218181610108,\n",
       "   1.4668177221298218,\n",
       "   1.5759870170593262,\n",
       "   1.4427144775390626,\n",
       "   1.3753404838562011,\n",
       "   1.3840568580627441,\n",
       "   1.3806634395599364,\n",
       "   1.4634030527114867,\n",
       "   1.5035373435974122,\n",
       "   1.4596922737121583,\n",
       "   1.4691996856689453,\n",
       "   1.4736801836013793,\n",
       "   1.4507608991622924,\n",
       "   1.4872321815490723,\n",
       "   1.5148881288528442,\n",
       "   1.5196347457885742,\n",
       "   1.3719193088531494,\n",
       "   1.385561095046997,\n",
       "   1.426419079399109,\n",
       "   1.34544956035614,\n",
       "   1.6860076637268067,\n",
       "   1.500135883140564,\n",
       "   1.4152214992523193,\n",
       "   1.5932528331756592,\n",
       "   1.400540601348877,\n",
       "   1.491526916885376,\n",
       "   1.414008176422119,\n",
       "   1.3619129642486572,\n",
       "   1.4717331890106202,\n",
       "   1.3528487926483155,\n",
       "   1.6501952667236328,\n",
       "   1.5755729581832885,\n",
       "   1.389894849395752,\n",
       "   1.3644802127838134,\n",
       "   1.4714737773895263,\n",
       "   1.3784742095947267,\n",
       "   1.425644513130188,\n",
       "   1.41123773727417,\n",
       "   1.409304023361206,\n",
       "   1.4755917022705078,\n",
       "   1.7392637050628663,\n",
       "   1.4870547903060913,\n",
       "   1.362934970664978,\n",
       "   1.5008835075378417,\n",
       "   1.535580101776123,\n",
       "   1.3658803825378418,\n",
       "   1.5750396690368653,\n",
       "   1.621234748840332,\n",
       "   1.5668426029205322,\n",
       "   1.458755578994751,\n",
       "   1.4706211349487304,\n",
       "   1.444897441482544,\n",
       "   1.565604351234436,\n",
       "   1.4109365756988526,\n",
       "   1.5148993019104005,\n",
       "   1.5403193195343017,\n",
       "   1.4113424251556397,\n",
       "   1.4184909183502197,\n",
       "   1.654733268737793,\n",
       "   1.503559257888794,\n",
       "   1.4929389736175538,\n",
       "   1.5181635091781616,\n",
       "   1.5765196094512939,\n",
       "   1.539716947555542,\n",
       "   1.5313467079162597,\n",
       "   1.5178792312622071,\n",
       "   1.5558813756942749,\n",
       "   1.5300901458740235,\n",
       "   1.7111657640457154,\n",
       "   1.7748302032470704,\n",
       "   1.4541522359848023,\n",
       "   1.5585173078536987,\n",
       "   1.7261115055084228,\n",
       "   1.620717971801758,\n",
       "   1.5158402662277222,\n",
       "   1.5136669692993163,\n",
       "   1.5933756023406982,\n",
       "   1.5589804874420166,\n",
       "   1.8961411697387696,\n",
       "   1.709253936767578,\n",
       "   1.5822566123962403,\n",
       "   2.346526794433594,\n",
       "   1.725683865737915,\n",
       "   1.7416854888916016,\n",
       "   1.5733759014129638,\n",
       "   1.8296088525772094,\n",
       "   1.5476871837615966,\n",
       "   1.566613035964966,\n",
       "   1.8381878416061401,\n",
       "   2.4225137027740478,\n",
       "   1.666129377746582,\n",
       "   1.5599724353790283,\n",
       "   1.6190859954833985,\n",
       "   1.74781358795166,\n",
       "   1.7482572784423829,\n",
       "   1.7596592393875121,\n",
       "   1.838528337097168,\n",
       "   1.6856095252990724,\n",
       "   1.9169705070495606,\n",
       "   1.627214094543457,\n",
       "   1.6691821865081786,\n",
       "   1.772884170150757,\n",
       "   1.6088936422348024,\n",
       "   1.7618440956115722,\n",
       "   1.7206784326553344,\n",
       "   1.7161507007598877,\n",
       "   1.6821386903762818,\n",
       "   1.99506184425354,\n",
       "   1.7153911735534668,\n",
       "   1.7991411441802978,\n",
       "   1.7952494834899901,\n",
       "   1.8702934684753418,\n",
       "   1.9043909271240234,\n",
       "   2.435474923706055,\n",
       "   1.7497467330932617,\n",
       "   1.8376623531341554,\n",
       "   2.187525706100464,\n",
       "   2.066938243865967,\n",
       "   2.091212825012207,\n",
       "   1.8731315448760986,\n",
       "   1.8209221244812013,\n",
       "   1.9877302391052245,\n",
       "   1.9106624252319335,\n",
       "   2.015356688308716,\n",
       "   1.9105918930053711,\n",
       "   2.0571114334106446,\n",
       "   1.9748067924499513,\n",
       "   1.9356981575012207,\n",
       "   2.014602060699463,\n",
       "   2.113981011199951,\n",
       "   1.9996268173217773,\n",
       "   1.8166280525207519,\n",
       "   2.2659837322235106,\n",
       "   1.919452751159668,\n",
       "   3.6276202236175537,\n",
       "   3.039911083984375,\n",
       "   2.0091851512908936,\n",
       "   2.202876711654663,\n",
       "   1.9390190193176269,\n",
       "   2.1241136898040773,\n",
       "   2.4139265476226806,\n",
       "   2.05640518913269,\n",
       "   1.9711423614501953,\n",
       "   2.2107682510375977,\n",
       "   2.0382233001708983,\n",
       "   2.0546049011230467,\n",
       "   2.106387149429321,\n",
       "   2.059803896331787,\n",
       "   2.2992002635955813,\n",
       "   2.058221767425537,\n",
       "   2.2127106948852537,\n",
       "   2.3829337238311767,\n",
       "   2.1263640743255614,\n",
       "   2.06376509475708,\n",
       "   2.3706428169250486,\n",
       "   2.7521745105743407,\n",
       "   2.1363044929504396,\n",
       "   2.122815613937378,\n",
       "   2.1893027084350587,\n",
       "   2.2517389316558836,\n",
       "   2.2498569664001464],\n",
       "  'train_acc': [0.2307400000190735,\n",
       "   0.31428000001907347,\n",
       "   0.3484600000190735,\n",
       "   0.36951999999046325,\n",
       "   0.38359999999046324,\n",
       "   0.3927999999904633,\n",
       "   0.4062599999809265,\n",
       "   0.41344,\n",
       "   0.4211999999809265,\n",
       "   0.42806000001907346,\n",
       "   0.4363800000190735,\n",
       "   0.441599999961853,\n",
       "   0.4480799999809265,\n",
       "   0.4545000000190735,\n",
       "   0.45970000001907346,\n",
       "   0.46535999999046324,\n",
       "   0.46749999999046327,\n",
       "   0.4730399999809265,\n",
       "   0.48032000003814695,\n",
       "   0.4842999999809265,\n",
       "   0.48769999999046326,\n",
       "   0.49217999998092654,\n",
       "   0.49584,\n",
       "   0.5030999999809265,\n",
       "   0.506899999961853,\n",
       "   0.5066999999904632,\n",
       "   0.513600000038147,\n",
       "   0.5147400000095368,\n",
       "   0.5179199999809265,\n",
       "   0.5227600000190735,\n",
       "   0.5283599999809265,\n",
       "   0.5295999999904633,\n",
       "   0.53122,\n",
       "   0.533239999961853,\n",
       "   0.5388600000381469,\n",
       "   0.54374,\n",
       "   0.54542,\n",
       "   0.5475199999904633,\n",
       "   0.549640000038147,\n",
       "   0.5537200000381469,\n",
       "   0.5586600000190735,\n",
       "   0.560680000038147,\n",
       "   0.5601,\n",
       "   0.567099999961853,\n",
       "   0.5666200000190735,\n",
       "   0.571120000038147,\n",
       "   0.5731600000190735,\n",
       "   0.576419999961853,\n",
       "   0.5780400000190735,\n",
       "   0.581280000038147,\n",
       "   0.5852200000190735,\n",
       "   0.587220000038147,\n",
       "   0.58842,\n",
       "   0.5913800000190735,\n",
       "   0.59564,\n",
       "   0.5945800000381469,\n",
       "   0.5986599999809266,\n",
       "   0.603859999961853,\n",
       "   0.6062600000190735,\n",
       "   0.608219999961853,\n",
       "   0.608880000038147,\n",
       "   0.611879999961853,\n",
       "   0.613960000038147,\n",
       "   0.618360000038147,\n",
       "   0.6204800000190734,\n",
       "   0.624200000038147,\n",
       "   0.624060000038147,\n",
       "   0.62628,\n",
       "   0.62658,\n",
       "   0.632360000038147,\n",
       "   0.6362,\n",
       "   0.636299999961853,\n",
       "   0.63938,\n",
       "   0.643999999961853,\n",
       "   0.64698,\n",
       "   0.651,\n",
       "   0.6505999999809265,\n",
       "   0.6565599999809265,\n",
       "   0.6559,\n",
       "   0.6593599999809265,\n",
       "   0.65872,\n",
       "   0.6615200000381469,\n",
       "   0.6668599999809265,\n",
       "   0.6687200000190735,\n",
       "   0.6704399999809265,\n",
       "   0.6742999999618531,\n",
       "   0.6768399999618531,\n",
       "   0.677680000038147,\n",
       "   0.67716,\n",
       "   0.6830599999618531,\n",
       "   0.6855200000190735,\n",
       "   0.685979999961853,\n",
       "   0.6879599999809265,\n",
       "   0.6913600000190735,\n",
       "   0.697240000038147,\n",
       "   0.698519999961853,\n",
       "   0.6981599999809265,\n",
       "   0.701279999961853,\n",
       "   0.70914,\n",
       "   0.702079999961853,\n",
       "   0.71124,\n",
       "   0.711519999961853,\n",
       "   0.713080000038147,\n",
       "   0.7177999999618531,\n",
       "   0.7195000000190734,\n",
       "   0.7181400000381469,\n",
       "   0.7243600000381469,\n",
       "   0.728500000038147,\n",
       "   0.732979999961853,\n",
       "   0.7292599999809265,\n",
       "   0.732880000038147,\n",
       "   0.7339400000190734,\n",
       "   0.74182,\n",
       "   0.740319999961853,\n",
       "   0.74592,\n",
       "   0.74778,\n",
       "   0.750540000038147,\n",
       "   0.747580000038147,\n",
       "   0.7487399999809266,\n",
       "   0.7561999999809265,\n",
       "   0.7592200000381469,\n",
       "   0.75458,\n",
       "   0.76256,\n",
       "   0.76072,\n",
       "   0.7657600000190735,\n",
       "   0.7709799999618531,\n",
       "   0.77384,\n",
       "   0.7725399999618531,\n",
       "   0.77416,\n",
       "   0.766839999961853,\n",
       "   0.7837799999809265,\n",
       "   0.78316,\n",
       "   0.7885200000381469,\n",
       "   0.788140000038147,\n",
       "   0.7886400000190735,\n",
       "   0.7945199999618531,\n",
       "   0.79628,\n",
       "   0.79784,\n",
       "   0.798679999961853,\n",
       "   0.8083800000381469,\n",
       "   0.80782,\n",
       "   0.7994599999809265,\n",
       "   0.8052999999809265,\n",
       "   0.81556,\n",
       "   0.8113800000190735,\n",
       "   0.8133600000190735,\n",
       "   0.821139999961853,\n",
       "   0.8220599999809265,\n",
       "   0.8216399999809265,\n",
       "   0.8077399999809265,\n",
       "   0.82716,\n",
       "   0.8363600000381469,\n",
       "   0.826579999961853,\n",
       "   0.824119999961853,\n",
       "   0.842119999961853,\n",
       "   0.845900000038147,\n",
       "   0.8378200000381469,\n",
       "   0.8361,\n",
       "   0.8486,\n",
       "   0.8371,\n",
       "   0.857060000038147,\n",
       "   0.84474,\n",
       "   0.8554600000190735,\n",
       "   0.8603200000190735,\n",
       "   0.8595800000190735,\n",
       "   0.8600199999618531,\n",
       "   0.8549600000190735,\n",
       "   0.8596399999809266,\n",
       "   0.8704399999618531,\n",
       "   0.8688599999809266,\n",
       "   0.867960000038147,\n",
       "   0.875160000038147,\n",
       "   0.8707,\n",
       "   0.872960000038147,\n",
       "   0.8671799999809265,\n",
       "   0.8503200000190735,\n",
       "   0.8763199999618531,\n",
       "   0.8922800000190735,\n",
       "   0.8823400000381469,\n",
       "   0.8815599999809265,\n",
       "   0.8865600000190735,\n",
       "   0.887680000038147,\n",
       "   0.908380000038147,\n",
       "   0.8865999999809265,\n",
       "   0.893680000038147,\n",
       "   0.90088,\n",
       "   0.88914,\n",
       "   0.8967,\n",
       "   0.89,\n",
       "   0.9097400000381469,\n",
       "   0.91398,\n",
       "   0.88444,\n",
       "   0.89262,\n",
       "   0.9320399999809265,\n",
       "   0.9111000000190735,\n",
       "   0.8992000000381469,\n",
       "   0.9016999999809265,\n",
       "   0.9277000000190735,\n",
       "   0.931320000038147,\n",
       "   0.878720000038147],\n",
       "  'valid_acc': [0.2925,\n",
       "   0.3172,\n",
       "   0.3304,\n",
       "   0.335,\n",
       "   0.3924,\n",
       "   0.36,\n",
       "   0.4071,\n",
       "   0.3777,\n",
       "   0.4083,\n",
       "   0.4115,\n",
       "   0.4006,\n",
       "   0.4295,\n",
       "   0.4338,\n",
       "   0.455,\n",
       "   0.4358,\n",
       "   0.4336,\n",
       "   0.4378,\n",
       "   0.4638,\n",
       "   0.4546,\n",
       "   0.4232,\n",
       "   0.4492,\n",
       "   0.4487,\n",
       "   0.4633,\n",
       "   0.4622,\n",
       "   0.4515,\n",
       "   0.4779,\n",
       "   0.4394,\n",
       "   0.4621,\n",
       "   0.4786,\n",
       "   0.4878,\n",
       "   0.4939,\n",
       "   0.5005,\n",
       "   0.466,\n",
       "   0.4971,\n",
       "   0.4601,\n",
       "   0.5119,\n",
       "   0.469,\n",
       "   0.491,\n",
       "   0.5005,\n",
       "   0.4846,\n",
       "   0.486,\n",
       "   0.4492,\n",
       "   0.4821,\n",
       "   0.5129,\n",
       "   0.5068,\n",
       "   0.5138,\n",
       "   0.4912,\n",
       "   0.4755,\n",
       "   0.4922,\n",
       "   0.4894,\n",
       "   0.4873,\n",
       "   0.4847,\n",
       "   0.4876,\n",
       "   0.4726,\n",
       "   0.4622,\n",
       "   0.5135,\n",
       "   0.5175,\n",
       "   0.4991,\n",
       "   0.5273,\n",
       "   0.4262,\n",
       "   0.4802,\n",
       "   0.5075,\n",
       "   0.4503,\n",
       "   0.5091,\n",
       "   0.4897,\n",
       "   0.5068,\n",
       "   0.5224,\n",
       "   0.4938,\n",
       "   0.5264,\n",
       "   0.4524,\n",
       "   0.465,\n",
       "   0.5157,\n",
       "   0.5285,\n",
       "   0.5073,\n",
       "   0.5227,\n",
       "   0.5151,\n",
       "   0.5153,\n",
       "   0.5218,\n",
       "   0.4959,\n",
       "   0.4557,\n",
       "   0.5112,\n",
       "   0.5332,\n",
       "   0.4904,\n",
       "   0.4995,\n",
       "   0.532,\n",
       "   0.4942,\n",
       "   0.4817,\n",
       "   0.4889,\n",
       "   0.5162,\n",
       "   0.5065,\n",
       "   0.5056,\n",
       "   0.4899,\n",
       "   0.5243,\n",
       "   0.505,\n",
       "   0.5024,\n",
       "   0.5284,\n",
       "   0.5261,\n",
       "   0.4718,\n",
       "   0.5108,\n",
       "   0.5109,\n",
       "   0.5131,\n",
       "   0.5028,\n",
       "   0.5088,\n",
       "   0.5154,\n",
       "   0.5122,\n",
       "   0.5077,\n",
       "   0.5155,\n",
       "   0.4853,\n",
       "   0.4696,\n",
       "   0.5335,\n",
       "   0.5185,\n",
       "   0.4839,\n",
       "   0.511,\n",
       "   0.5235,\n",
       "   0.5296,\n",
       "   0.5096,\n",
       "   0.5169,\n",
       "   0.4628,\n",
       "   0.5,\n",
       "   0.5155,\n",
       "   0.4113,\n",
       "   0.4933,\n",
       "   0.4729,\n",
       "   0.5122,\n",
       "   0.4798,\n",
       "   0.5255,\n",
       "   0.5266,\n",
       "   0.4701,\n",
       "   0.3886,\n",
       "   0.5099,\n",
       "   0.5353,\n",
       "   0.5208,\n",
       "   0.4953,\n",
       "   0.4945,\n",
       "   0.487,\n",
       "   0.4943,\n",
       "   0.5191,\n",
       "   0.4885,\n",
       "   0.5194,\n",
       "   0.5256,\n",
       "   0.5015,\n",
       "   0.5316,\n",
       "   0.5051,\n",
       "   0.5141,\n",
       "   0.516,\n",
       "   0.5325,\n",
       "   0.4846,\n",
       "   0.5317,\n",
       "   0.5099,\n",
       "   0.5157,\n",
       "   0.4974,\n",
       "   0.4966,\n",
       "   0.4359,\n",
       "   0.5236,\n",
       "   0.5182,\n",
       "   0.4735,\n",
       "   0.478,\n",
       "   0.4744,\n",
       "   0.5236,\n",
       "   0.516,\n",
       "   0.5028,\n",
       "   0.5187,\n",
       "   0.5078,\n",
       "   0.5202,\n",
       "   0.4942,\n",
       "   0.5162,\n",
       "   0.5055,\n",
       "   0.5126,\n",
       "   0.5087,\n",
       "   0.4941,\n",
       "   0.5344,\n",
       "   0.4568,\n",
       "   0.5226,\n",
       "   0.3461,\n",
       "   0.4355,\n",
       "   0.5016,\n",
       "   0.4873,\n",
       "   0.537,\n",
       "   0.5045,\n",
       "   0.4719,\n",
       "   0.5123,\n",
       "   0.5243,\n",
       "   0.4989,\n",
       "   0.5266,\n",
       "   0.522,\n",
       "   0.5143,\n",
       "   0.5224,\n",
       "   0.5008,\n",
       "   0.5282,\n",
       "   0.5155,\n",
       "   0.4984,\n",
       "   0.517,\n",
       "   0.5297,\n",
       "   0.4969,\n",
       "   0.4403,\n",
       "   0.5295,\n",
       "   0.5308,\n",
       "   0.5311,\n",
       "   0.5224,\n",
       "   0.5135]},\n",
       " 'RMSprop': {'train-loss': [2.2120068406677245,\n",
       "   1.9021337926483155,\n",
       "   1.8051428051376344,\n",
       "   1.7318232336044312,\n",
       "   1.6781865549087525,\n",
       "   1.6356453630828858,\n",
       "   1.5927580081939696,\n",
       "   1.5563090217971802,\n",
       "   1.5307556561279296,\n",
       "   1.4989974990844726,\n",
       "   1.4731000191879273,\n",
       "   1.4434259090423585,\n",
       "   1.4208700693893432,\n",
       "   1.3949619129943847,\n",
       "   1.3674927346801757,\n",
       "   1.348954493637085,\n",
       "   1.3261851260757447,\n",
       "   1.3028509580612182,\n",
       "   1.2800156021881104,\n",
       "   1.2651833545303344,\n",
       "   1.241008614654541,\n",
       "   1.2197491106796265,\n",
       "   1.2011131801223756,\n",
       "   1.1816817004776001,\n",
       "   1.1627179836654664,\n",
       "   1.13971375,\n",
       "   1.1220978186225892,\n",
       "   1.0983412013435363,\n",
       "   1.080858009777069,\n",
       "   1.0687869692611693,\n",
       "   1.0530206549453736,\n",
       "   1.0288147540664674,\n",
       "   1.0114451327323914,\n",
       "   0.9981310608291626,\n",
       "   0.9800922857856751,\n",
       "   0.9648206315231324,\n",
       "   0.9477525674438476,\n",
       "   0.9311023205184936,\n",
       "   0.9146768348503113,\n",
       "   0.9061899990081788,\n",
       "   0.8894726301574707,\n",
       "   0.8824668582344055,\n",
       "   0.8619381906509399,\n",
       "   0.8495011246109009,\n",
       "   0.8366340704154969,\n",
       "   0.8249959650993347,\n",
       "   0.8111473544692993,\n",
       "   0.7957966082382202,\n",
       "   0.7823075708389282,\n",
       "   0.7679637432479859,\n",
       "   0.7608593855667114,\n",
       "   0.7519582323265076,\n",
       "   0.7367676358795165,\n",
       "   0.726931600036621,\n",
       "   0.7164409512901306,\n",
       "   0.7056818371009826,\n",
       "   0.7000403268814087,\n",
       "   0.6867029278182983,\n",
       "   0.6754510593032836,\n",
       "   0.6686010118484497,\n",
       "   0.6536850623893737,\n",
       "   0.6537910947799682,\n",
       "   0.6442661333084106,\n",
       "   0.636282479724884,\n",
       "   0.6224383283805848,\n",
       "   0.6154290819740296,\n",
       "   0.6152773585510254,\n",
       "   0.606123080291748,\n",
       "   0.5931110926914215,\n",
       "   0.5877538969802857,\n",
       "   0.5778584556388855,\n",
       "   0.5678941282844543,\n",
       "   0.5711684739303589,\n",
       "   0.5529458798217773,\n",
       "   0.5542299995231629,\n",
       "   0.5442817713832855,\n",
       "   0.5378883359718323,\n",
       "   0.537797309589386,\n",
       "   0.523894376411438,\n",
       "   0.5162444385623932,\n",
       "   0.5136224600791931,\n",
       "   0.5101267387390137,\n",
       "   0.5049549364280701,\n",
       "   0.4953436734580994,\n",
       "   0.4970836722946167,\n",
       "   0.4869539637565613,\n",
       "   0.4842912606430054,\n",
       "   0.48083631361961365,\n",
       "   0.47921908254623413,\n",
       "   0.47093982879638674,\n",
       "   0.4653676571273804,\n",
       "   0.46232682772636413,\n",
       "   0.4558204031944275,\n",
       "   0.448915862903595,\n",
       "   0.44881540595054625,\n",
       "   0.44115122816085817,\n",
       "   0.43972715012550356,\n",
       "   0.43594297636032103,\n",
       "   0.43671804121017455,\n",
       "   0.4323034103393555,\n",
       "   0.4215234796714783,\n",
       "   0.4197977184295654,\n",
       "   0.41734315116882326,\n",
       "   0.4117034119796753,\n",
       "   0.4072170504665375,\n",
       "   0.4020631884288788,\n",
       "   0.397617094745636,\n",
       "   0.4050356968784332,\n",
       "   0.3967260739707947,\n",
       "   0.39409886674880984,\n",
       "   0.38741505512714386,\n",
       "   0.3877151383972168,\n",
       "   0.3886128880119324,\n",
       "   0.38427147062301636,\n",
       "   0.3776043004417419,\n",
       "   0.3717052855491638,\n",
       "   0.36812880066871645,\n",
       "   0.37302261171340945,\n",
       "   0.36342481939315796,\n",
       "   0.3665939568901062,\n",
       "   0.361245319442749,\n",
       "   0.3560650005531311,\n",
       "   0.3561748428821564,\n",
       "   0.35485693510055544,\n",
       "   0.35280404907226565,\n",
       "   0.34581929591178895,\n",
       "   0.3470169406795502,\n",
       "   0.3431243269729614,\n",
       "   0.34081068259239194,\n",
       "   0.32976008774280546,\n",
       "   0.33528981029510496,\n",
       "   0.3302346682548523,\n",
       "   0.3333344515037537,\n",
       "   0.3340616205692291,\n",
       "   0.3226402105140686,\n",
       "   0.32147798214912415,\n",
       "   0.3279307315635681,\n",
       "   0.3171079595184326,\n",
       "   0.3244642649936676,\n",
       "   0.31362376609802245,\n",
       "   0.3160039988231659,\n",
       "   0.3162221884965897,\n",
       "   0.3158686489391327,\n",
       "   0.3061459471797943,\n",
       "   0.3077161713981628,\n",
       "   0.3022445950126648,\n",
       "   0.2966400349521637,\n",
       "   0.3031725100040436,\n",
       "   0.30574524119853974,\n",
       "   0.296992443523407,\n",
       "   0.2962601502227783,\n",
       "   0.29583114588737486,\n",
       "   0.2880987426376343,\n",
       "   0.2959422556257248,\n",
       "   0.2913976197719574,\n",
       "   0.2810162727737427,\n",
       "   0.27833497280597685,\n",
       "   0.28759410864830015,\n",
       "   0.27880214270591736,\n",
       "   0.28817357595443727,\n",
       "   0.27696653579711916,\n",
       "   0.2881435619163513,\n",
       "   0.27647170746326444,\n",
       "   0.27794705673456194,\n",
       "   0.2761557603931427,\n",
       "   0.26810843015670777,\n",
       "   0.2660204808139801,\n",
       "   0.2713853812408447,\n",
       "   0.2758445474910736,\n",
       "   0.26346754529953004,\n",
       "   0.26850975157737733,\n",
       "   0.26399161418914796,\n",
       "   0.2636752480888367,\n",
       "   0.258245938243866,\n",
       "   0.2600485882472992,\n",
       "   0.25840760673999785,\n",
       "   0.2617950992870331,\n",
       "   0.25803134159088137,\n",
       "   0.2599589176464081,\n",
       "   0.24946628359794618,\n",
       "   0.25886128846168516,\n",
       "   0.2507369411373138,\n",
       "   0.2525019215965271,\n",
       "   0.25981211678028104,\n",
       "   0.24906959237098694,\n",
       "   0.24480970768928528,\n",
       "   0.24778976346969606,\n",
       "   0.2436973949623108,\n",
       "   0.2460128847885132,\n",
       "   0.24375495500564576,\n",
       "   0.24288059749126434,\n",
       "   0.24103466046333313,\n",
       "   0.2412949531650543,\n",
       "   0.2441491947555542,\n",
       "   0.23563969034194945,\n",
       "   0.2434730636692047,\n",
       "   0.23081072346687317,\n",
       "   0.23771948295593262,\n",
       "   0.2383873548221588,\n",
       "   0.22968868448257446],\n",
       "  'valid_loss': [2.0753152137756348,\n",
       "   1.9055822261810302,\n",
       "   1.7780429832458495,\n",
       "   1.6877996145248413,\n",
       "   1.668139464569092,\n",
       "   1.6783064817428588,\n",
       "   1.6858124687194824,\n",
       "   1.5693237138748168,\n",
       "   1.7842912128448487,\n",
       "   1.5791980934143066,\n",
       "   1.6125715713500977,\n",
       "   1.5774719142913818,\n",
       "   1.516666262626648,\n",
       "   1.5436130264282226,\n",
       "   1.5125641616821288,\n",
       "   1.6178046865463256,\n",
       "   1.4975490520477295,\n",
       "   1.5236590587615966,\n",
       "   1.5407489624023438,\n",
       "   1.527315077972412,\n",
       "   1.545773842048645,\n",
       "   1.486838487815857,\n",
       "   1.7419025402069093,\n",
       "   1.7059466857910157,\n",
       "   1.4926756023406982,\n",
       "   1.5035942922592163,\n",
       "   1.5125393516540528,\n",
       "   1.5460442840576172,\n",
       "   1.5847939186096192,\n",
       "   1.6138620237350463,\n",
       "   1.5572087703704833,\n",
       "   1.5953392337799073,\n",
       "   1.5552360893249513,\n",
       "   1.6366076690673828,\n",
       "   1.6938402313232421,\n",
       "   1.6058932308197023,\n",
       "   1.6344979057312012,\n",
       "   1.7383700757980347,\n",
       "   1.6800972608566285,\n",
       "   1.8405384441375732,\n",
       "   1.7139924774169921,\n",
       "   1.7276919151306152,\n",
       "   1.8056202659606932,\n",
       "   1.7558967971801758,\n",
       "   1.8609021938323975,\n",
       "   1.880055972290039,\n",
       "   1.8408489650726318,\n",
       "   1.889629995727539,\n",
       "   1.8553106147766114,\n",
       "   1.9417450775146485,\n",
       "   1.8728771144866942,\n",
       "   2.0418873222351075,\n",
       "   2.0967011211395263,\n",
       "   2.0940416427612303,\n",
       "   2.147728207778931,\n",
       "   2.0991408340454103,\n",
       "   2.2228220066070556,\n",
       "   2.040072731399536,\n",
       "   2.1453425041198733,\n",
       "   2.224419031524658,\n",
       "   1.9368434108734132,\n",
       "   2.2216699234008788,\n",
       "   2.1891322174072267,\n",
       "   2.328621931838989,\n",
       "   2.3428993335723876,\n",
       "   2.3246559440612793,\n",
       "   2.233375202178955,\n",
       "   2.1674216175079346,\n",
       "   2.363370363998413,\n",
       "   2.357865528869629,\n",
       "   2.5311422866821287,\n",
       "   2.512749923324585,\n",
       "   2.481526448059082,\n",
       "   2.714380005645752,\n",
       "   2.55844959564209,\n",
       "   2.61201879196167,\n",
       "   2.5826744159698487,\n",
       "   2.472460210418701,\n",
       "   2.4935024234771728,\n",
       "   2.694211156463623,\n",
       "   2.9101963424682618,\n",
       "   2.6576911926269533,\n",
       "   2.646855114364624,\n",
       "   2.649918246459961,\n",
       "   2.737688593673706,\n",
       "   2.8509205825805664,\n",
       "   2.7235836204528807,\n",
       "   2.6385863807678223,\n",
       "   2.815840158843994,\n",
       "   2.9009593677520753,\n",
       "   2.9704386085510253,\n",
       "   2.9044531219482423,\n",
       "   2.90680969619751,\n",
       "   2.9818819595336916,\n",
       "   3.0128412631988524,\n",
       "   3.121127974700928,\n",
       "   3.05431301651001,\n",
       "   3.194139953994751,\n",
       "   3.041622758102417,\n",
       "   2.999095706176758,\n",
       "   3.2138029930114747,\n",
       "   3.3255312404632567,\n",
       "   2.997631548690796,\n",
       "   3.211631158828735,\n",
       "   3.1818912441253664,\n",
       "   3.159595637512207,\n",
       "   3.3656040679931643,\n",
       "   3.405648445892334,\n",
       "   3.3767572494506837,\n",
       "   3.2796704162597656,\n",
       "   3.3739671447753907,\n",
       "   3.13735697555542,\n",
       "   3.230014259338379,\n",
       "   3.3389254501342776,\n",
       "   3.3860626476287843,\n",
       "   3.5178299270629885,\n",
       "   3.4306037170410155,\n",
       "   3.2713634338378905,\n",
       "   3.3600472091674805,\n",
       "   3.3487346504211426,\n",
       "   3.5513032165527343,\n",
       "   3.583229864501953,\n",
       "   3.4793604637145994,\n",
       "   3.3870317008972166,\n",
       "   3.559566928100586,\n",
       "   3.4943681465148924,\n",
       "   3.5123289489746092,\n",
       "   3.6541504257202146,\n",
       "   3.644770124053955,\n",
       "   3.4410826782226565,\n",
       "   3.7795230377197266,\n",
       "   3.7466850662231446,\n",
       "   3.885649965667725,\n",
       "   3.6751463382720946,\n",
       "   3.7237576972961426,\n",
       "   3.6003249603271485,\n",
       "   3.669934161376953,\n",
       "   3.8490847259521486,\n",
       "   3.6142709506988524,\n",
       "   3.7330461029052735,\n",
       "   3.593158529281616,\n",
       "   3.8235531661987303,\n",
       "   3.6249894134521483,\n",
       "   3.764600975036621,\n",
       "   3.7596715827941893,\n",
       "   3.838937599182129,\n",
       "   3.833801272583008,\n",
       "   3.9378161842346193,\n",
       "   4.009821850585937,\n",
       "   3.946819337463379,\n",
       "   3.94260943069458,\n",
       "   4.128609992980957,\n",
       "   3.7997333786010743,\n",
       "   4.028763356018066,\n",
       "   3.8344534996032715,\n",
       "   3.984459149169922,\n",
       "   4.203044494628906,\n",
       "   4.067003916931152,\n",
       "   3.9196240821838377,\n",
       "   3.9630519065856933,\n",
       "   4.007004885101319,\n",
       "   4.100805492401123,\n",
       "   4.041254856109619,\n",
       "   3.904912872314453,\n",
       "   3.9369792205810548,\n",
       "   3.9920154258728027,\n",
       "   3.954804237365723,\n",
       "   4.151239160919189,\n",
       "   3.958472922515869,\n",
       "   4.214637062072754,\n",
       "   4.070797404479981,\n",
       "   4.167504473876953,\n",
       "   4.157780979919433,\n",
       "   4.170936970520019,\n",
       "   4.416251805877685,\n",
       "   4.215279506683349,\n",
       "   4.039620335388183,\n",
       "   4.301641876220703,\n",
       "   4.3065394691467285,\n",
       "   4.3667781478881835,\n",
       "   4.253224296569824,\n",
       "   4.294772451019287,\n",
       "   4.184713458251953,\n",
       "   4.333823323059082,\n",
       "   4.155777705383301,\n",
       "   4.278930754852295,\n",
       "   4.15148295211792,\n",
       "   4.258936965942383,\n",
       "   4.3951095207214355,\n",
       "   4.124996105957031,\n",
       "   4.395735669708252,\n",
       "   4.399031860733032,\n",
       "   4.386880671691895,\n",
       "   4.342321170043945,\n",
       "   4.610932115936279,\n",
       "   4.514565049743652,\n",
       "   4.433322889709473,\n",
       "   4.4336848571777345,\n",
       "   4.4233788414001465,\n",
       "   4.326830661010742],\n",
       "  'train_acc': [0.20321999999046325,\n",
       "   0.3127399999809265,\n",
       "   0.34885999998092654,\n",
       "   0.3812999999809265,\n",
       "   0.39997999999046324,\n",
       "   0.41452,\n",
       "   0.4328800000190735,\n",
       "   0.44300000001907347,\n",
       "   0.45337999998092654,\n",
       "   0.46445999996185305,\n",
       "   0.47330000001907346,\n",
       "   0.4837599999809265,\n",
       "   0.49263999996185304,\n",
       "   0.50064,\n",
       "   0.5126,\n",
       "   0.5173399999809265,\n",
       "   0.5275600000190734,\n",
       "   0.5349,\n",
       "   0.5425399999809265,\n",
       "   0.5490600000381469,\n",
       "   0.5543399999809265,\n",
       "   0.5633,\n",
       "   0.5705599999904633,\n",
       "   0.577499999961853,\n",
       "   0.5858799999809265,\n",
       "   0.5932399999809265,\n",
       "   0.5983400000190735,\n",
       "   0.6066800000381469,\n",
       "   0.6159799999809266,\n",
       "   0.6156599999809265,\n",
       "   0.6225400000190735,\n",
       "   0.630699999961853,\n",
       "   0.6368399999809266,\n",
       "   0.640799999961853,\n",
       "   0.65016,\n",
       "   0.6565200000381469,\n",
       "   0.659759999961853,\n",
       "   0.6644399999809265,\n",
       "   0.6718799999809265,\n",
       "   0.67488,\n",
       "   0.6821,\n",
       "   0.683359999961853,\n",
       "   0.69288,\n",
       "   0.6924399999809265,\n",
       "   0.699099999961853,\n",
       "   0.7027199999618531,\n",
       "   0.709080000038147,\n",
       "   0.7133200000190735,\n",
       "   0.720739999961853,\n",
       "   0.722339999961853,\n",
       "   0.7272400000381469,\n",
       "   0.72988,\n",
       "   0.7331199999618531,\n",
       "   0.7380000000381469,\n",
       "   0.74278,\n",
       "   0.744719999961853,\n",
       "   0.75078,\n",
       "   0.752280000038147,\n",
       "   0.757839999961853,\n",
       "   0.75956,\n",
       "   0.7646199999809266,\n",
       "   0.7626200000190735,\n",
       "   0.7676599999809265,\n",
       "   0.770959999961853,\n",
       "   0.776900000038147,\n",
       "   0.77732,\n",
       "   0.779920000038147,\n",
       "   0.7807600000381469,\n",
       "   0.7867599999809265,\n",
       "   0.78948,\n",
       "   0.7901599999809266,\n",
       "   0.796459999961853,\n",
       "   0.795719999961853,\n",
       "   0.802920000038147,\n",
       "   0.8011599999809265,\n",
       "   0.80274,\n",
       "   0.80552,\n",
       "   0.8047400000190735,\n",
       "   0.8116,\n",
       "   0.8128399999809265,\n",
       "   0.818219999961853,\n",
       "   0.8170399999618531,\n",
       "   0.818739999961853,\n",
       "   0.8205799999809266,\n",
       "   0.8198200000190735,\n",
       "   0.82546,\n",
       "   0.8259399999618531,\n",
       "   0.8279600000190734,\n",
       "   0.8280399999809265,\n",
       "   0.8312800000190735,\n",
       "   0.83364,\n",
       "   0.833680000038147,\n",
       "   0.8388599999809265,\n",
       "   0.8409199999809265,\n",
       "   0.838980000038147,\n",
       "   0.8419000000190735,\n",
       "   0.84352,\n",
       "   0.845720000038147,\n",
       "   0.8447800000190735,\n",
       "   0.84552,\n",
       "   0.84778,\n",
       "   0.85002,\n",
       "   0.8533199999809266,\n",
       "   0.8532599999809265,\n",
       "   0.8555600000190735,\n",
       "   0.8589399999809265,\n",
       "   0.858860000038147,\n",
       "   0.8565600000381469,\n",
       "   0.8585600000381469,\n",
       "   0.8603999999809265,\n",
       "   0.8635800000190735,\n",
       "   0.8622399999809265,\n",
       "   0.8617800000190735,\n",
       "   0.8632599999809265,\n",
       "   0.865360000038147,\n",
       "   0.8694799999809265,\n",
       "   0.869719999961853,\n",
       "   0.8675599999809265,\n",
       "   0.871199999961853,\n",
       "   0.8707000000190734,\n",
       "   0.87322,\n",
       "   0.8749400000190735,\n",
       "   0.8743799999809265,\n",
       "   0.874620000038147,\n",
       "   0.8761000000381469,\n",
       "   0.87708,\n",
       "   0.87912,\n",
       "   0.8802800000190735,\n",
       "   0.88028,\n",
       "   0.8850600000190735,\n",
       "   0.8817399999809266,\n",
       "   0.883800000038147,\n",
       "   0.8825599999618531,\n",
       "   0.88446,\n",
       "   0.8848,\n",
       "   0.8879799999809265,\n",
       "   0.88656,\n",
       "   0.8885000000190735,\n",
       "   0.8862199999618531,\n",
       "   0.890839999961853,\n",
       "   0.889860000038147,\n",
       "   0.89212,\n",
       "   0.8901599999809265,\n",
       "   0.893959999961853,\n",
       "   0.892620000038147,\n",
       "   0.8948599999618531,\n",
       "   0.896140000038147,\n",
       "   0.89448,\n",
       "   0.893259999961853,\n",
       "   0.8977600000190735,\n",
       "   0.897240000038147,\n",
       "   0.8962800000190735,\n",
       "   0.89998,\n",
       "   0.8987200000190735,\n",
       "   0.9005400000190735,\n",
       "   0.9016800000190734,\n",
       "   0.90454,\n",
       "   0.8999200000190735,\n",
       "   0.903620000038147,\n",
       "   0.9008199999809265,\n",
       "   0.9040799999809265,\n",
       "   0.90104,\n",
       "   0.9039599999618531,\n",
       "   0.9039600000190735,\n",
       "   0.904100000038147,\n",
       "   0.907380000038147,\n",
       "   0.9093999999618531,\n",
       "   0.90774,\n",
       "   0.9058,\n",
       "   0.9110799999618531,\n",
       "   0.90916,\n",
       "   0.90838,\n",
       "   0.90924,\n",
       "   0.9129199999809265,\n",
       "   0.9116400000190735,\n",
       "   0.912939999961853,\n",
       "   0.9114200000190735,\n",
       "   0.911099999961853,\n",
       "   0.9120400000190735,\n",
       "   0.914699999961853,\n",
       "   0.9122000000190735,\n",
       "   0.9120200000190735,\n",
       "   0.91434,\n",
       "   0.914499999961853,\n",
       "   0.91554,\n",
       "   0.9174,\n",
       "   0.9164199999618531,\n",
       "   0.9177399999618531,\n",
       "   0.91704,\n",
       "   0.917719999961853,\n",
       "   0.91762,\n",
       "   0.9171400000190735,\n",
       "   0.9181200000190735,\n",
       "   0.9169599999618531,\n",
       "   0.9204799999809266,\n",
       "   0.91774,\n",
       "   0.921340000038147,\n",
       "   0.92058,\n",
       "   0.921000000038147,\n",
       "   0.922900000038147],\n",
       "  'valid_acc': [0.2296,\n",
       "   0.3259,\n",
       "   0.3595,\n",
       "   0.3892,\n",
       "   0.4048,\n",
       "   0.3955,\n",
       "   0.3954,\n",
       "   0.4389,\n",
       "   0.3776,\n",
       "   0.4473,\n",
       "   0.4303,\n",
       "   0.4289,\n",
       "   0.4615,\n",
       "   0.459,\n",
       "   0.4578,\n",
       "   0.4325,\n",
       "   0.4792,\n",
       "   0.4638,\n",
       "   0.4689,\n",
       "   0.4703,\n",
       "   0.4597,\n",
       "   0.4869,\n",
       "   0.4171,\n",
       "   0.4479,\n",
       "   0.4921,\n",
       "   0.4935,\n",
       "   0.4838,\n",
       "   0.4934,\n",
       "   0.4777,\n",
       "   0.477,\n",
       "   0.4877,\n",
       "   0.4873,\n",
       "   0.4931,\n",
       "   0.4895,\n",
       "   0.4822,\n",
       "   0.4972,\n",
       "   0.4869,\n",
       "   0.4687,\n",
       "   0.4932,\n",
       "   0.4772,\n",
       "   0.4894,\n",
       "   0.4891,\n",
       "   0.4847,\n",
       "   0.4868,\n",
       "   0.4708,\n",
       "   0.4846,\n",
       "   0.4857,\n",
       "   0.4797,\n",
       "   0.4838,\n",
       "   0.4903,\n",
       "   0.4887,\n",
       "   0.4791,\n",
       "   0.4773,\n",
       "   0.4713,\n",
       "   0.4805,\n",
       "   0.4713,\n",
       "   0.4725,\n",
       "   0.4898,\n",
       "   0.486,\n",
       "   0.4702,\n",
       "   0.4949,\n",
       "   0.4692,\n",
       "   0.4803,\n",
       "   0.4817,\n",
       "   0.4807,\n",
       "   0.4661,\n",
       "   0.4815,\n",
       "   0.4806,\n",
       "   0.4923,\n",
       "   0.4982,\n",
       "   0.4839,\n",
       "   0.4719,\n",
       "   0.4758,\n",
       "   0.4764,\n",
       "   0.4821,\n",
       "   0.469,\n",
       "   0.4669,\n",
       "   0.4956,\n",
       "   0.4849,\n",
       "   0.485,\n",
       "   0.4629,\n",
       "   0.4834,\n",
       "   0.4861,\n",
       "   0.4694,\n",
       "   0.4856,\n",
       "   0.4632,\n",
       "   0.4666,\n",
       "   0.4867,\n",
       "   0.4849,\n",
       "   0.4784,\n",
       "   0.4678,\n",
       "   0.4856,\n",
       "   0.4764,\n",
       "   0.4805,\n",
       "   0.483,\n",
       "   0.4612,\n",
       "   0.4857,\n",
       "   0.4527,\n",
       "   0.4561,\n",
       "   0.4819,\n",
       "   0.4684,\n",
       "   0.4775,\n",
       "   0.4747,\n",
       "   0.4668,\n",
       "   0.4795,\n",
       "   0.4831,\n",
       "   0.4607,\n",
       "   0.4727,\n",
       "   0.4554,\n",
       "   0.4786,\n",
       "   0.4767,\n",
       "   0.4766,\n",
       "   0.4801,\n",
       "   0.4699,\n",
       "   0.4831,\n",
       "   0.4645,\n",
       "   0.4677,\n",
       "   0.4797,\n",
       "   0.4837,\n",
       "   0.4759,\n",
       "   0.4673,\n",
       "   0.479,\n",
       "   0.4833,\n",
       "   0.4711,\n",
       "   0.4899,\n",
       "   0.4606,\n",
       "   0.4766,\n",
       "   0.4808,\n",
       "   0.483,\n",
       "   0.482,\n",
       "   0.4668,\n",
       "   0.4684,\n",
       "   0.4601,\n",
       "   0.4819,\n",
       "   0.4799,\n",
       "   0.4705,\n",
       "   0.47,\n",
       "   0.4803,\n",
       "   0.4803,\n",
       "   0.4735,\n",
       "   0.4736,\n",
       "   0.4753,\n",
       "   0.4869,\n",
       "   0.4771,\n",
       "   0.4792,\n",
       "   0.474,\n",
       "   0.4774,\n",
       "   0.4796,\n",
       "   0.4726,\n",
       "   0.4796,\n",
       "   0.4817,\n",
       "   0.4595,\n",
       "   0.4674,\n",
       "   0.4673,\n",
       "   0.4788,\n",
       "   0.4706,\n",
       "   0.477,\n",
       "   0.4764,\n",
       "   0.4753,\n",
       "   0.4668,\n",
       "   0.4772,\n",
       "   0.4694,\n",
       "   0.474,\n",
       "   0.4785,\n",
       "   0.4756,\n",
       "   0.4654,\n",
       "   0.4777,\n",
       "   0.4743,\n",
       "   0.481,\n",
       "   0.4681,\n",
       "   0.474,\n",
       "   0.4566,\n",
       "   0.4669,\n",
       "   0.4802,\n",
       "   0.4648,\n",
       "   0.4784,\n",
       "   0.4645,\n",
       "   0.4516,\n",
       "   0.4728,\n",
       "   0.4696,\n",
       "   0.4659,\n",
       "   0.4727,\n",
       "   0.4754,\n",
       "   0.4766,\n",
       "   0.4728,\n",
       "   0.4799,\n",
       "   0.4712,\n",
       "   0.4636,\n",
       "   0.4671,\n",
       "   0.4694,\n",
       "   0.4639,\n",
       "   0.4665,\n",
       "   0.4821,\n",
       "   0.4704,\n",
       "   0.4717,\n",
       "   0.4724,\n",
       "   0.4681,\n",
       "   0.4603,\n",
       "   0.4646,\n",
       "   0.4734]},\n",
       " 'AdaGrad': {'train-loss': [14.413298564147949,\n",
       "   14.506069316711425,\n",
       "   14.505969906921386,\n",
       "   11.527790843276977,\n",
       "   2.0760242211151123,\n",
       "   1.8677783584976195,\n",
       "   1.775790460548401,\n",
       "   1.711346923751831,\n",
       "   1.6650653797149657,\n",
       "   1.6263835675811769,\n",
       "   1.5959340431976319,\n",
       "   1.5655902338409424,\n",
       "   1.5394218753433226,\n",
       "   1.5176350230026245,\n",
       "   1.4960146514511108,\n",
       "   1.4739729776763917,\n",
       "   1.4536287353134156,\n",
       "   1.4407566484832763,\n",
       "   1.4194430849838258,\n",
       "   1.4100646269607544,\n",
       "   1.3935147145462037,\n",
       "   1.3778072732543944,\n",
       "   1.3625294966125487,\n",
       "   1.3504003293228148,\n",
       "   1.3368401761627198,\n",
       "   1.3267384726715088,\n",
       "   1.3142573334884644,\n",
       "   1.3010664212799072,\n",
       "   1.2940099608612061,\n",
       "   1.281261803779602,\n",
       "   1.2710688191986084,\n",
       "   1.2622225980758668,\n",
       "   1.248911932106018,\n",
       "   1.2414235874938966,\n",
       "   1.2301397777557372,\n",
       "   1.221593058128357,\n",
       "   1.210862150001526,\n",
       "   1.2034287811279296,\n",
       "   1.1939607418060303,\n",
       "   1.1862601739501952,\n",
       "   1.1730693436050414,\n",
       "   1.173658040008545,\n",
       "   1.1592466175842284,\n",
       "   1.1505485525894166,\n",
       "   1.1451024200820923,\n",
       "   1.136184515953064,\n",
       "   1.1278169131469726,\n",
       "   1.1205600722122193,\n",
       "   1.1132693376159668,\n",
       "   1.1059888018035888,\n",
       "   1.0961075510406495,\n",
       "   1.093509665298462,\n",
       "   1.082650820388794,\n",
       "   1.0764188241958619,\n",
       "   1.0698156060028077,\n",
       "   1.0601993069648743,\n",
       "   1.0580198383712769,\n",
       "   1.0497541794204712,\n",
       "   1.0411366205596924,\n",
       "   1.0343337382507325,\n",
       "   1.0298002620315552,\n",
       "   1.021242553577423,\n",
       "   1.0191705969047546,\n",
       "   1.0101410435867308,\n",
       "   1.0019329899597167,\n",
       "   0.999269200630188,\n",
       "   0.9896797086715698,\n",
       "   0.9859692387390137,\n",
       "   0.9798805186843872,\n",
       "   0.97146958984375,\n",
       "   0.9692658541107178,\n",
       "   0.9634459859848022,\n",
       "   0.9591828008270263,\n",
       "   0.9490004086494446,\n",
       "   0.9409908884048462,\n",
       "   0.9375556021308898,\n",
       "   0.9316255770683288,\n",
       "   0.9295207287406921,\n",
       "   0.9214451901245118,\n",
       "   0.9177206015396118,\n",
       "   0.9123092864227295,\n",
       "   0.9095801134872437,\n",
       "   0.899476369972229,\n",
       "   0.8949837979888916,\n",
       "   0.8903069034957886,\n",
       "   0.8858680575561524,\n",
       "   0.8813005072784423,\n",
       "   0.8737236525726318,\n",
       "   0.8693943962287903,\n",
       "   0.8652954605484009,\n",
       "   0.8606176578521728,\n",
       "   0.8581424880599976,\n",
       "   0.8515037953567505,\n",
       "   0.8447523666191101,\n",
       "   0.8423396485900879,\n",
       "   0.839097042312622,\n",
       "   0.8304195187759399,\n",
       "   0.827204139099121,\n",
       "   0.8248145262336731,\n",
       "   0.819062909564972,\n",
       "   0.8147438188552857,\n",
       "   0.8126111408615112,\n",
       "   0.8072729742050171,\n",
       "   0.8036859363174439,\n",
       "   0.7988358061981201,\n",
       "   0.794902144317627,\n",
       "   0.7891287097549439,\n",
       "   0.787963959312439,\n",
       "   0.7794018794250488,\n",
       "   0.7768606442451477,\n",
       "   0.7727920929908753,\n",
       "   0.7687750820159912,\n",
       "   0.765724321422577,\n",
       "   0.7613452206611633,\n",
       "   0.7561330974197388,\n",
       "   0.7543168162918091,\n",
       "   0.7495524361610413,\n",
       "   0.7452019540977478,\n",
       "   0.7398083306503296,\n",
       "   0.740416111087799,\n",
       "   0.7325931865310669,\n",
       "   0.7296307837104797,\n",
       "   0.7275411271858215,\n",
       "   0.7217130035400391,\n",
       "   0.7182970878219604,\n",
       "   0.7159630942153931,\n",
       "   0.7103308198165894,\n",
       "   0.7091643682861328,\n",
       "   0.7048783198928833,\n",
       "   0.7010979150009156,\n",
       "   0.696640718383789,\n",
       "   0.6913946990013122,\n",
       "   0.6876436898994446,\n",
       "   0.6845450053787231,\n",
       "   0.6819674592590332,\n",
       "   0.6792085579109192,\n",
       "   0.676370929145813,\n",
       "   0.669762184715271,\n",
       "   0.6664554432296753,\n",
       "   0.6632350666427612,\n",
       "   0.6606157319641113,\n",
       "   0.6562119540405273,\n",
       "   0.6557674550247192,\n",
       "   0.6512090539550781,\n",
       "   0.6470695477676391,\n",
       "   0.6440692035675049,\n",
       "   0.6406980631637573,\n",
       "   0.6345705177307129,\n",
       "   0.6343619492721557,\n",
       "   0.6302372802734375,\n",
       "   0.6281106176757812,\n",
       "   0.6232204818725586,\n",
       "   0.6212520068359375,\n",
       "   0.6181079586219788,\n",
       "   0.6172988270759583,\n",
       "   0.6123719623184204,\n",
       "   0.6108076271438598,\n",
       "   0.6076760105514526,\n",
       "   0.6037514968681336,\n",
       "   0.6002697970962524,\n",
       "   0.5963345283126831,\n",
       "   0.593142011756897,\n",
       "   0.5898920247268676,\n",
       "   0.586661094379425,\n",
       "   0.5826240191268921,\n",
       "   0.582543461856842,\n",
       "   0.5804195869445801,\n",
       "   0.57682066822052,\n",
       "   0.5739611248588562,\n",
       "   0.5714962116336822,\n",
       "   0.5653026253128052,\n",
       "   0.5652247869491577,\n",
       "   0.561680869102478,\n",
       "   0.5578573041152954,\n",
       "   0.5560479769039154,\n",
       "   0.5526488519096374,\n",
       "   0.5513263959884643,\n",
       "   0.5456923787307739,\n",
       "   0.544025601348877,\n",
       "   0.541341943435669,\n",
       "   0.5391546952819825,\n",
       "   0.5362652384090424,\n",
       "   0.5346802890205383,\n",
       "   0.531160654964447,\n",
       "   0.5274622309684753,\n",
       "   0.5248847292518616,\n",
       "   0.5219125421905517,\n",
       "   0.5202653686141968,\n",
       "   0.5186308205413819,\n",
       "   0.5136061679267884,\n",
       "   0.5124849809455871,\n",
       "   0.5114881533050537,\n",
       "   0.5081554290771484,\n",
       "   0.5057033147239685,\n",
       "   0.5010151334381103,\n",
       "   0.5005780859375,\n",
       "   0.4980689647483826,\n",
       "   0.4954332098388672,\n",
       "   0.49289607639312744,\n",
       "   0.4905238554763794],\n",
       "  'valid_loss': [14.506285955810547,\n",
       "   14.506285955810547,\n",
       "   14.505490570068359,\n",
       "   2.3515971305847168,\n",
       "   1.904177543258667,\n",
       "   1.804706604385376,\n",
       "   1.7814501720428466,\n",
       "   1.6866871084213257,\n",
       "   1.7873068073272704,\n",
       "   1.7131054897308349,\n",
       "   1.593559983062744,\n",
       "   1.5995222665786744,\n",
       "   1.6844067817687989,\n",
       "   1.5897263320922852,\n",
       "   1.5257395082473755,\n",
       "   1.5151718212127685,\n",
       "   1.4748576488494873,\n",
       "   1.50643072681427,\n",
       "   1.6253137809753417,\n",
       "   1.5027553058624268,\n",
       "   1.4446240198135376,\n",
       "   1.4390111484527588,\n",
       "   1.477494673728943,\n",
       "   1.474085598564148,\n",
       "   1.4855875122070312,\n",
       "   1.4261462955474853,\n",
       "   1.4477159936904906,\n",
       "   1.4459271461486816,\n",
       "   1.610837158203125,\n",
       "   1.4726859729766846,\n",
       "   1.491347467803955,\n",
       "   1.3869287363052367,\n",
       "   1.3958214986801147,\n",
       "   1.436472961807251,\n",
       "   1.3950973218917846,\n",
       "   1.4597831008911133,\n",
       "   1.3793114826202393,\n",
       "   1.4247091690063476,\n",
       "   1.41559478225708,\n",
       "   1.4406114461898805,\n",
       "   1.4102682069778443,\n",
       "   1.3815651329040528,\n",
       "   1.3942607526779174,\n",
       "   1.3903561241149902,\n",
       "   1.3926114227294921,\n",
       "   1.4041406198501587,\n",
       "   1.4277215250015258,\n",
       "   1.3775032218933105,\n",
       "   1.392540534210205,\n",
       "   1.427961855697632,\n",
       "   1.3945113367080688,\n",
       "   1.457371015548706,\n",
       "   1.379451558303833,\n",
       "   1.3870772397994995,\n",
       "   1.409916028213501,\n",
       "   1.3847252193450927,\n",
       "   1.4035446201324462,\n",
       "   1.403358118057251,\n",
       "   1.426934771347046,\n",
       "   1.4179916481018067,\n",
       "   1.424364148902893,\n",
       "   1.4291464357376098,\n",
       "   1.470068907546997,\n",
       "   1.3831654205322266,\n",
       "   1.4004429969787597,\n",
       "   1.4175310327529906,\n",
       "   1.4191572267532349,\n",
       "   1.4253614234924317,\n",
       "   1.4777455280303955,\n",
       "   1.4511929943084716,\n",
       "   1.4171537269592285,\n",
       "   1.4317046892166139,\n",
       "   1.5007338842391968,\n",
       "   1.4595515195846558,\n",
       "   1.4705778465270996,\n",
       "   1.4614683141708373,\n",
       "   1.5045806819915772,\n",
       "   1.4629427019119263,\n",
       "   1.4608173776626587,\n",
       "   1.4635887966156005,\n",
       "   1.5071171876907348,\n",
       "   1.4547670440673828,\n",
       "   1.5450735235214232,\n",
       "   1.4983689500808717,\n",
       "   1.4631411348342895,\n",
       "   1.4437335834503173,\n",
       "   1.469860950279236,\n",
       "   1.4759101932525636,\n",
       "   1.4862828636169434,\n",
       "   1.4802091117858887,\n",
       "   1.5378761415481568,\n",
       "   1.466829268836975,\n",
       "   1.5118890502929687,\n",
       "   1.498491056060791,\n",
       "   1.5059843456268311,\n",
       "   1.5034686614990234,\n",
       "   1.5023460662841797,\n",
       "   1.5173193725585938,\n",
       "   1.5653672004699708,\n",
       "   1.5239163761138916,\n",
       "   1.540140078353882,\n",
       "   1.5231151275634767,\n",
       "   1.610094786453247,\n",
       "   1.5261963871002198,\n",
       "   1.5343036254882811,\n",
       "   1.5298681095123292,\n",
       "   1.557109722328186,\n",
       "   1.5393773780822755,\n",
       "   1.5945403705596923,\n",
       "   1.5484021808624266,\n",
       "   1.6139023138046265,\n",
       "   1.564698989868164,\n",
       "   1.5762435926437377,\n",
       "   1.6041932685852052,\n",
       "   1.5907952484130858,\n",
       "   1.595939617729187,\n",
       "   1.6106796886444092,\n",
       "   1.6124764911651612,\n",
       "   1.5995672849655151,\n",
       "   1.6515690090179442,\n",
       "   1.6341839372634888,\n",
       "   1.618616665649414,\n",
       "   1.6080515853881836,\n",
       "   1.646058673286438,\n",
       "   1.6797123176574706,\n",
       "   1.65168853225708,\n",
       "   1.661196174621582,\n",
       "   1.6793749740600585,\n",
       "   1.6630380393981934,\n",
       "   1.6710887411117554,\n",
       "   1.6427865257263183,\n",
       "   1.6842708837509155,\n",
       "   1.6527015602111816,\n",
       "   1.6914490278244019,\n",
       "   1.739916259765625,\n",
       "   1.7255960428237915,\n",
       "   1.6966005891799927,\n",
       "   1.6996314414978027,\n",
       "   1.7083549732208252,\n",
       "   1.732517121696472,\n",
       "   1.7233302959442138,\n",
       "   1.75807180519104,\n",
       "   1.7577522228240967,\n",
       "   1.7232009960174561,\n",
       "   1.770135445022583,\n",
       "   1.7400190666198732,\n",
       "   1.7541774387359619,\n",
       "   1.713413893699646,\n",
       "   1.7784978456497191,\n",
       "   1.762212077331543,\n",
       "   1.7322733030319213,\n",
       "   1.796703980255127,\n",
       "   1.7776620513916015,\n",
       "   1.761764192199707,\n",
       "   1.815286608505249,\n",
       "   1.763170573425293,\n",
       "   1.9587342975616455,\n",
       "   1.8137448818206787,\n",
       "   1.8475116037368775,\n",
       "   1.827718419265747,\n",
       "   1.8621646549224853,\n",
       "   1.8218334976196289,\n",
       "   1.813240968322754,\n",
       "   1.8176495796203613,\n",
       "   1.8421024040222167,\n",
       "   1.892848898124695,\n",
       "   1.9457833484649658,\n",
       "   1.839589790725708,\n",
       "   1.863049930381775,\n",
       "   1.8569543682098388,\n",
       "   1.9677953298568727,\n",
       "   1.9395526309967042,\n",
       "   1.8782844058990478,\n",
       "   1.9130894264221192,\n",
       "   1.8851768590927125,\n",
       "   1.8995870567321778,\n",
       "   1.88672109375,\n",
       "   1.9921693923950194,\n",
       "   1.9059121543884276,\n",
       "   1.9288590702056885,\n",
       "   1.9801721591949464,\n",
       "   1.9382980243682861,\n",
       "   1.9199688346862793,\n",
       "   1.9332791576385497,\n",
       "   1.9814023094177247,\n",
       "   1.959795945739746,\n",
       "   2.000447080230713,\n",
       "   2.028442584037781,\n",
       "   1.9564277042388916,\n",
       "   1.9637057903289794,\n",
       "   1.9976628837585448,\n",
       "   1.9883850593566894,\n",
       "   1.9917341678619385,\n",
       "   1.9899677593231202,\n",
       "   2.01190573387146,\n",
       "   2.01439481048584,\n",
       "   2.0413064323425294,\n",
       "   2.0392299827575684,\n",
       "   2.0383261684417726,\n",
       "   2.0692602516174317],\n",
       "  'train_acc': [0.10008000000119209,\n",
       "   0.10000000000476837,\n",
       "   0.10000000000953674,\n",
       "   0.10424000000476837,\n",
       "   0.23352000000953674,\n",
       "   0.32547999999046323,\n",
       "   0.3595600000190735,\n",
       "   0.38459999998092653,\n",
       "   0.4022200000095367,\n",
       "   0.4199200000095367,\n",
       "   0.4318199999809265,\n",
       "   0.44376000000953675,\n",
       "   0.4520000000190735,\n",
       "   0.4595199999904633,\n",
       "   0.46728000000953673,\n",
       "   0.47822000003814696,\n",
       "   0.48225999999046326,\n",
       "   0.4868799999904633,\n",
       "   0.4956,\n",
       "   0.4990800000190735,\n",
       "   0.5029799999809265,\n",
       "   0.5101599999809265,\n",
       "   0.5159,\n",
       "   0.518780000038147,\n",
       "   0.5266200000190735,\n",
       "   0.5300800000381469,\n",
       "   0.5324999999809266,\n",
       "   0.5392399999809265,\n",
       "   0.5393599999809265,\n",
       "   0.5452799999618531,\n",
       "   0.5481799999809265,\n",
       "   0.55216,\n",
       "   0.5593800000381469,\n",
       "   0.56038,\n",
       "   0.563019999961853,\n",
       "   0.5680199999809266,\n",
       "   0.571879999961853,\n",
       "   0.574200000038147,\n",
       "   0.578299999961853,\n",
       "   0.580539999961853,\n",
       "   0.5852599999809265,\n",
       "   0.5852600000381469,\n",
       "   0.5907399999809265,\n",
       "   0.5937600000190735,\n",
       "   0.5962600000381469,\n",
       "   0.5990200000381469,\n",
       "   0.599960000038147,\n",
       "   0.60334,\n",
       "   0.6065800000381469,\n",
       "   0.60966,\n",
       "   0.6125799999809265,\n",
       "   0.6146999999809265,\n",
       "   0.6186800000381469,\n",
       "   0.6209,\n",
       "   0.624859999961853,\n",
       "   0.6247800000190735,\n",
       "   0.6272200000381469,\n",
       "   0.63192,\n",
       "   0.6329999999809265,\n",
       "   0.635519999961853,\n",
       "   0.638839999961853,\n",
       "   0.6410799999809265,\n",
       "   0.642520000038147,\n",
       "   0.644700000038147,\n",
       "   0.6495000000190735,\n",
       "   0.6510399999809265,\n",
       "   0.6541000000190735,\n",
       "   0.65588,\n",
       "   0.6566399999618531,\n",
       "   0.6603399999809265,\n",
       "   0.661560000038147,\n",
       "   0.6634400000190734,\n",
       "   0.6649400000190735,\n",
       "   0.6692599999809266,\n",
       "   0.6723800000190735,\n",
       "   0.674380000038147,\n",
       "   0.6750800000381469,\n",
       "   0.6745599999809265,\n",
       "   0.6788599999809265,\n",
       "   0.68042,\n",
       "   0.682360000038147,\n",
       "   0.6818400000381469,\n",
       "   0.6866199999809265,\n",
       "   0.689319999961853,\n",
       "   0.691100000038147,\n",
       "   0.691520000038147,\n",
       "   0.6933600000190735,\n",
       "   0.6958599999809265,\n",
       "   0.6986600000190735,\n",
       "   0.69938,\n",
       "   0.6997999999809266,\n",
       "   0.7026000000381469,\n",
       "   0.7044599999809266,\n",
       "   0.7071599999618531,\n",
       "   0.7090399999618531,\n",
       "   0.709959999961853,\n",
       "   0.7113599999809265,\n",
       "   0.71162,\n",
       "   0.71492,\n",
       "   0.7175799999809265,\n",
       "   0.719559999961853,\n",
       "   0.7185199999809265,\n",
       "   0.72012,\n",
       "   0.72258,\n",
       "   0.7244199999809265,\n",
       "   0.72542,\n",
       "   0.72702,\n",
       "   0.7283800000190734,\n",
       "   0.732139999961853,\n",
       "   0.7340000000190735,\n",
       "   0.7351199999809265,\n",
       "   0.734859999961853,\n",
       "   0.735920000038147,\n",
       "   0.737720000038147,\n",
       "   0.7407000000190734,\n",
       "   0.740540000038147,\n",
       "   0.741599999961853,\n",
       "   0.7442399999618531,\n",
       "   0.7451,\n",
       "   0.74388,\n",
       "   0.7486400000190735,\n",
       "   0.750160000038147,\n",
       "   0.750639999961853,\n",
       "   0.752700000038147,\n",
       "   0.755740000038147,\n",
       "   0.7552400000190734,\n",
       "   0.7565600000381469,\n",
       "   0.7573599999809265,\n",
       "   0.7587399999809266,\n",
       "   0.7594999999809265,\n",
       "   0.762340000038147,\n",
       "   0.7642000000381469,\n",
       "   0.7666400000381469,\n",
       "   0.766179999961853,\n",
       "   0.7674799999809265,\n",
       "   0.7688799999809265,\n",
       "   0.7697399999809265,\n",
       "   0.773279999961853,\n",
       "   0.7732799999809266,\n",
       "   0.77518,\n",
       "   0.77462,\n",
       "   0.776919999961853,\n",
       "   0.777219999961853,\n",
       "   0.779039999961853,\n",
       "   0.7810799999809265,\n",
       "   0.7823600000190735,\n",
       "   0.7828000000190735,\n",
       "   0.7852000000381469,\n",
       "   0.7864,\n",
       "   0.7882999999809265,\n",
       "   0.7869400000190735,\n",
       "   0.7895800000190735,\n",
       "   0.79164,\n",
       "   0.7923000000381469,\n",
       "   0.793360000038147,\n",
       "   0.7935799999809265,\n",
       "   0.7944599999809265,\n",
       "   0.796140000038147,\n",
       "   0.79572,\n",
       "   0.7986199999809265,\n",
       "   0.80046,\n",
       "   0.8016400000190735,\n",
       "   0.802239999961853,\n",
       "   0.8035599999809265,\n",
       "   0.8059399999809265,\n",
       "   0.80506,\n",
       "   0.80592,\n",
       "   0.807540000038147,\n",
       "   0.8091200000190735,\n",
       "   0.809999999961853,\n",
       "   0.8118399999618531,\n",
       "   0.8122199999809265,\n",
       "   0.8122999999809265,\n",
       "   0.8146000000190735,\n",
       "   0.815459999961853,\n",
       "   0.8158199999809265,\n",
       "   0.81686,\n",
       "   0.819240000038147,\n",
       "   0.820139999961853,\n",
       "   0.81994,\n",
       "   0.82116,\n",
       "   0.8225800000190735,\n",
       "   0.824239999961853,\n",
       "   0.8253600000190735,\n",
       "   0.8259199999809265,\n",
       "   0.827280000038147,\n",
       "   0.828299999961853,\n",
       "   0.828420000038147,\n",
       "   0.8299399999809265,\n",
       "   0.8312200000190735,\n",
       "   0.833080000038147,\n",
       "   0.83138,\n",
       "   0.83388,\n",
       "   0.834659999961853,\n",
       "   0.83698,\n",
       "   0.836940000038147,\n",
       "   0.83866,\n",
       "   0.8372199999809266,\n",
       "   0.83946,\n",
       "   0.8387000000190735],\n",
       "  'valid_acc': [0.1,\n",
       "   0.1,\n",
       "   0.1,\n",
       "   0.1221,\n",
       "   0.3115,\n",
       "   0.3551,\n",
       "   0.3503,\n",
       "   0.3986,\n",
       "   0.3553,\n",
       "   0.3847,\n",
       "   0.431,\n",
       "   0.4216,\n",
       "   0.4015,\n",
       "   0.4372,\n",
       "   0.4553,\n",
       "   0.46,\n",
       "   0.4767,\n",
       "   0.4622,\n",
       "   0.4306,\n",
       "   0.464,\n",
       "   0.4896,\n",
       "   0.4918,\n",
       "   0.4807,\n",
       "   0.4753,\n",
       "   0.4798,\n",
       "   0.4963,\n",
       "   0.4924,\n",
       "   0.4918,\n",
       "   0.4423,\n",
       "   0.4769,\n",
       "   0.4823,\n",
       "   0.5147,\n",
       "   0.5098,\n",
       "   0.5001,\n",
       "   0.5069,\n",
       "   0.4938,\n",
       "   0.5168,\n",
       "   0.5067,\n",
       "   0.5074,\n",
       "   0.5021,\n",
       "   0.5115,\n",
       "   0.5154,\n",
       "   0.5216,\n",
       "   0.519,\n",
       "   0.5232,\n",
       "   0.5162,\n",
       "   0.4941,\n",
       "   0.5159,\n",
       "   0.5242,\n",
       "   0.5106,\n",
       "   0.5164,\n",
       "   0.5027,\n",
       "   0.5249,\n",
       "   0.5223,\n",
       "   0.5167,\n",
       "   0.5249,\n",
       "   0.5159,\n",
       "   0.5184,\n",
       "   0.5166,\n",
       "   0.517,\n",
       "   0.5163,\n",
       "   0.5186,\n",
       "   0.5033,\n",
       "   0.5325,\n",
       "   0.5289,\n",
       "   0.5227,\n",
       "   0.522,\n",
       "   0.5233,\n",
       "   0.5129,\n",
       "   0.5195,\n",
       "   0.5194,\n",
       "   0.5188,\n",
       "   0.5064,\n",
       "   0.5197,\n",
       "   0.5189,\n",
       "   0.5228,\n",
       "   0.5055,\n",
       "   0.5185,\n",
       "   0.5156,\n",
       "   0.5212,\n",
       "   0.5162,\n",
       "   0.5183,\n",
       "   0.5034,\n",
       "   0.514,\n",
       "   0.523,\n",
       "   0.5278,\n",
       "   0.5236,\n",
       "   0.5199,\n",
       "   0.5221,\n",
       "   0.5238,\n",
       "   0.5146,\n",
       "   0.5264,\n",
       "   0.5191,\n",
       "   0.5225,\n",
       "   0.5262,\n",
       "   0.5227,\n",
       "   0.5214,\n",
       "   0.5213,\n",
       "   0.514,\n",
       "   0.5258,\n",
       "   0.5163,\n",
       "   0.5258,\n",
       "   0.5027,\n",
       "   0.5273,\n",
       "   0.5221,\n",
       "   0.5265,\n",
       "   0.519,\n",
       "   0.5303,\n",
       "   0.518,\n",
       "   0.5271,\n",
       "   0.5171,\n",
       "   0.5236,\n",
       "   0.5235,\n",
       "   0.5113,\n",
       "   0.5251,\n",
       "   0.5209,\n",
       "   0.5187,\n",
       "   0.5197,\n",
       "   0.5191,\n",
       "   0.518,\n",
       "   0.5146,\n",
       "   0.5226,\n",
       "   0.5252,\n",
       "   0.5148,\n",
       "   0.5103,\n",
       "   0.5144,\n",
       "   0.5133,\n",
       "   0.5167,\n",
       "   0.5135,\n",
       "   0.5171,\n",
       "   0.5234,\n",
       "   0.517,\n",
       "   0.5212,\n",
       "   0.5164,\n",
       "   0.5098,\n",
       "   0.514,\n",
       "   0.5202,\n",
       "   0.5185,\n",
       "   0.5168,\n",
       "   0.5098,\n",
       "   0.5156,\n",
       "   0.5096,\n",
       "   0.5127,\n",
       "   0.5196,\n",
       "   0.5075,\n",
       "   0.517,\n",
       "   0.5189,\n",
       "   0.5269,\n",
       "   0.5117,\n",
       "   0.5171,\n",
       "   0.518,\n",
       "   0.513,\n",
       "   0.5189,\n",
       "   0.5169,\n",
       "   0.5062,\n",
       "   0.5218,\n",
       "   0.4909,\n",
       "   0.5171,\n",
       "   0.5138,\n",
       "   0.5208,\n",
       "   0.5078,\n",
       "   0.5154,\n",
       "   0.5157,\n",
       "   0.5183,\n",
       "   0.5165,\n",
       "   0.5078,\n",
       "   0.499,\n",
       "   0.5186,\n",
       "   0.5124,\n",
       "   0.5221,\n",
       "   0.4983,\n",
       "   0.5076,\n",
       "   0.518,\n",
       "   0.5131,\n",
       "   0.5162,\n",
       "   0.5158,\n",
       "   0.5214,\n",
       "   0.4995,\n",
       "   0.5156,\n",
       "   0.5139,\n",
       "   0.5071,\n",
       "   0.5111,\n",
       "   0.5177,\n",
       "   0.5137,\n",
       "   0.5107,\n",
       "   0.5128,\n",
       "   0.5079,\n",
       "   0.5042,\n",
       "   0.5146,\n",
       "   0.5122,\n",
       "   0.5105,\n",
       "   0.5125,\n",
       "   0.5158,\n",
       "   0.513,\n",
       "   0.513,\n",
       "   0.5151,\n",
       "   0.5082,\n",
       "   0.5104,\n",
       "   0.5069,\n",
       "   0.508]},\n",
       " 'Adam': {'train-loss': [1.9363879929351806,\n",
       "   1.7214596603393555,\n",
       "   1.6282164019775391,\n",
       "   1.5667758525848388,\n",
       "   1.5225575102996827,\n",
       "   1.4876155011367798,\n",
       "   1.4501766646194458,\n",
       "   1.416749439163208,\n",
       "   1.3919448975372315,\n",
       "   1.3624655995941162,\n",
       "   1.338413429336548,\n",
       "   1.3282679671096802,\n",
       "   1.2939483899688722,\n",
       "   1.2661579503631593,\n",
       "   1.2417998801422119,\n",
       "   1.223920655670166,\n",
       "   1.2012719190216063,\n",
       "   1.1865747226715089,\n",
       "   1.1589694077301025,\n",
       "   1.136661894454956,\n",
       "   1.1243745922088624,\n",
       "   1.1054675427246095,\n",
       "   1.08586350440979,\n",
       "   1.0587379584503174,\n",
       "   1.0467239462661744,\n",
       "   1.029791710395813,\n",
       "   1.0085932252120973,\n",
       "   1.0027958551406861,\n",
       "   0.9799135146331787,\n",
       "   0.9590410762405396,\n",
       "   0.9316792686843872,\n",
       "   0.9146416586494446,\n",
       "   0.9053208828544617,\n",
       "   0.8894608000564576,\n",
       "   0.865489134941101,\n",
       "   0.8550373813629151,\n",
       "   0.8373565809249878,\n",
       "   0.822275694103241,\n",
       "   0.8148455464553833,\n",
       "   0.7917173988723755,\n",
       "   0.7625030412292481,\n",
       "   0.7702497766494751,\n",
       "   0.748701413230896,\n",
       "   0.7294654089546203,\n",
       "   0.7132919668388367,\n",
       "   0.7105686626243591,\n",
       "   0.6861304641532898,\n",
       "   0.6830253071594238,\n",
       "   0.668125978565216,\n",
       "   0.6576482298469544,\n",
       "   0.6421038772773743,\n",
       "   0.6144483290481567,\n",
       "   0.6301859918785095,\n",
       "   0.5922383588027954,\n",
       "   0.5957769821739197,\n",
       "   0.5731718194389344,\n",
       "   0.5654101253509521,\n",
       "   0.5686774699401855,\n",
       "   0.5472445031166077,\n",
       "   0.5441658109283447,\n",
       "   0.5183048349380494,\n",
       "   0.5257802808761597,\n",
       "   0.5099477301406861,\n",
       "   0.5006049502658844,\n",
       "   0.5067614161872864,\n",
       "   0.4830028249549866,\n",
       "   0.46841178953170776,\n",
       "   0.4586168249893188,\n",
       "   0.4666963564300537,\n",
       "   0.44218116983413697,\n",
       "   0.4749402255821228,\n",
       "   0.4482692402267456,\n",
       "   0.4306548014163971,\n",
       "   0.4135952525997162,\n",
       "   0.41124079679489134,\n",
       "   0.41103492398262026,\n",
       "   0.421177391834259,\n",
       "   0.4151076042175293,\n",
       "   0.3794413707637787,\n",
       "   0.37297036445617676,\n",
       "   0.37955064703941344,\n",
       "   0.3869666478538513,\n",
       "   0.3658602594089508,\n",
       "   0.3714659960746765,\n",
       "   0.36045215253829954,\n",
       "   0.3831242963600159,\n",
       "   0.35465958877563475,\n",
       "   0.33805761138916013,\n",
       "   0.3460123902988434,\n",
       "   0.33397146475791933,\n",
       "   0.33696953120231626,\n",
       "   0.3483624073123932,\n",
       "   0.32170121017456055,\n",
       "   0.32501282209396365,\n",
       "   0.31535814858436584,\n",
       "   0.31288608283996583,\n",
       "   0.3039836920833588,\n",
       "   0.3190147877883911,\n",
       "   0.32038196981430056,\n",
       "   0.32906251852035523,\n",
       "   0.29012470046520233,\n",
       "   0.2947136262321472,\n",
       "   0.27403554993629453,\n",
       "   0.2998932607746124,\n",
       "   0.2883267117881775,\n",
       "   0.2798064486503601,\n",
       "   0.2810701403236389,\n",
       "   0.28762670836448667,\n",
       "   0.30667949233055114,\n",
       "   0.2975185136699677,\n",
       "   0.2506952824306488,\n",
       "   0.2840650667476654,\n",
       "   0.26162980065345764,\n",
       "   0.2641607483482361,\n",
       "   0.2663992000579834,\n",
       "   0.25061754574775696,\n",
       "   0.2487172718334198,\n",
       "   0.28610687966346743,\n",
       "   0.2409927353954315,\n",
       "   0.2673793572974205,\n",
       "   0.24301350186347961,\n",
       "   0.23263801853656768,\n",
       "   0.22673369694709777,\n",
       "   0.256901073141098,\n",
       "   0.2601164810371399,\n",
       "   0.2251036842250824,\n",
       "   0.23937709290504455,\n",
       "   0.23141570631027222,\n",
       "   0.25258267374038695,\n",
       "   0.21578333332061767,\n",
       "   0.257430434923172,\n",
       "   0.22133174293994903,\n",
       "   0.22811550609588624,\n",
       "   0.22898556997776032,\n",
       "   0.212398440284729,\n",
       "   0.2687969125843048,\n",
       "   0.23511522921562195,\n",
       "   0.20686621065378188,\n",
       "   0.19574430174827576,\n",
       "   0.1905868705034256,\n",
       "   0.22764167182922362,\n",
       "   0.25579441831588745,\n",
       "   0.21065157084465028,\n",
       "   0.1974517379951477,\n",
       "   0.19922689491271972,\n",
       "   0.22292941134929656,\n",
       "   0.19433380084514618,\n",
       "   0.22236613471984865,\n",
       "   0.20126980544567108,\n",
       "   0.22804202268600463,\n",
       "   0.22081983012199402,\n",
       "   0.19537156623363494,\n",
       "   0.16390858154296875,\n",
       "   0.16298358178138733,\n",
       "   0.21617430035591126,\n",
       "   0.206658531870842,\n",
       "   0.2045330117225647,\n",
       "   0.18084127789497376,\n",
       "   0.2025178974199295,\n",
       "   0.15108083693027496,\n",
       "   0.14315773952484132,\n",
       "   0.23054619801044465,\n",
       "   0.2157072695350647,\n",
       "   0.17881555103063584,\n",
       "   0.1758958457183838,\n",
       "   0.15129169523239136,\n",
       "   0.19073964905261995,\n",
       "   0.24261776334285737,\n",
       "   0.17777750286102295,\n",
       "   0.18713390112876893,\n",
       "   0.14938633799552917,\n",
       "   0.18014250996112824,\n",
       "   0.2057832014465332,\n",
       "   0.16718189316749574,\n",
       "   0.1695979749727249,\n",
       "   0.17642640697717665,\n",
       "   0.15721350645542145,\n",
       "   0.16463898586153983,\n",
       "   0.1867133336353302,\n",
       "   0.2123238959121704,\n",
       "   0.1687900703859329,\n",
       "   0.14828603637695312,\n",
       "   0.18681683356285095,\n",
       "   0.13974730233192445,\n",
       "   0.17862587423324586,\n",
       "   0.19607974165916442,\n",
       "   0.15382111156225203,\n",
       "   0.15508098398685455,\n",
       "   0.15878905160427093,\n",
       "   0.1736352121067047,\n",
       "   0.14597773530960084,\n",
       "   0.14424649946451187,\n",
       "   0.12023557047367096,\n",
       "   0.23844474396705628,\n",
       "   0.17982010686397554,\n",
       "   0.16928385059833526,\n",
       "   0.11991140444993972,\n",
       "   0.13574639905929564,\n",
       "   0.15953371209621428,\n",
       "   0.18649566495895387],\n",
       "  'valid_loss': [1.8048279697418212,\n",
       "   1.6645048355102539,\n",
       "   1.6006717235565187,\n",
       "   1.5431138647079468,\n",
       "   1.5345425708770752,\n",
       "   1.478237984085083,\n",
       "   1.466903550338745,\n",
       "   1.4494865409851074,\n",
       "   1.462443434715271,\n",
       "   1.4285588657379151,\n",
       "   1.4023928062438964,\n",
       "   1.4206723983764649,\n",
       "   1.408050254058838,\n",
       "   1.4220117893218993,\n",
       "   1.3876408199310302,\n",
       "   1.4139706707000732,\n",
       "   1.4096754833221437,\n",
       "   1.4160917137145996,\n",
       "   1.3848667556762695,\n",
       "   1.41424487285614,\n",
       "   1.3726047269821167,\n",
       "   1.3711873657226563,\n",
       "   1.391287784576416,\n",
       "   1.4204677503585816,\n",
       "   1.4297483505249022,\n",
       "   1.4075977582931518,\n",
       "   1.4012459804534911,\n",
       "   1.4301358346939086,\n",
       "   1.4433532537460327,\n",
       "   1.463630966758728,\n",
       "   1.4201757247924804,\n",
       "   1.4706577140808106,\n",
       "   1.449436881828308,\n",
       "   1.4580425479888917,\n",
       "   1.4913756916046144,\n",
       "   1.5078005949020385,\n",
       "   1.5223289321899414,\n",
       "   1.5245471286773682,\n",
       "   1.5443437393188477,\n",
       "   1.5434365184783936,\n",
       "   1.616642315673828,\n",
       "   1.602655504989624,\n",
       "   1.660595760345459,\n",
       "   1.692190223312378,\n",
       "   1.6548839902877808,\n",
       "   1.7269185276031493,\n",
       "   1.7255127700805664,\n",
       "   1.7212290842056275,\n",
       "   1.7942403657913208,\n",
       "   1.8201556034088135,\n",
       "   1.7383300327301026,\n",
       "   1.8670646772384643,\n",
       "   1.8307408561706544,\n",
       "   1.8937460872650147,\n",
       "   1.868207716369629,\n",
       "   1.9455860198974608,\n",
       "   2.0125774761199953,\n",
       "   1.9860182403564453,\n",
       "   2.0312854961395264,\n",
       "   1.9675544536590577,\n",
       "   2.0482757759094237,\n",
       "   2.065198425292969,\n",
       "   2.145764710998535,\n",
       "   2.0882366931915284,\n",
       "   2.1388436027526856,\n",
       "   2.1896676876068115,\n",
       "   2.3053350303649904,\n",
       "   2.257189447021484,\n",
       "   2.3221769145965574,\n",
       "   2.3546661197662355,\n",
       "   2.3846621952056886,\n",
       "   2.3201128030776976,\n",
       "   2.39140686416626,\n",
       "   2.4250212219238283,\n",
       "   2.4566140087127684,\n",
       "   2.5356836196899413,\n",
       "   2.469495016479492,\n",
       "   2.5496682678222657,\n",
       "   2.557516317939758,\n",
       "   2.5461121612548827,\n",
       "   2.5546539770126344,\n",
       "   2.623682486343384,\n",
       "   2.6603894876480103,\n",
       "   2.66478625869751,\n",
       "   2.696933525848389,\n",
       "   2.7051799827575684,\n",
       "   2.778143974685669,\n",
       "   2.7391693906784056,\n",
       "   2.7966315177917482,\n",
       "   2.8547445671081544,\n",
       "   2.816056615447998,\n",
       "   2.822715477371216,\n",
       "   2.7874133308410642,\n",
       "   2.7947607788085937,\n",
       "   2.909570494842529,\n",
       "   2.9079928867340086,\n",
       "   3.033505534362793,\n",
       "   3.070980172729492,\n",
       "   2.9965674388885497,\n",
       "   2.919743794631958,\n",
       "   2.973926682281494,\n",
       "   3.0976115394592285,\n",
       "   3.0667333431243895,\n",
       "   3.13158097076416,\n",
       "   3.079775740814209,\n",
       "   3.2347023117065428,\n",
       "   3.142859489440918,\n",
       "   3.2247197074890135,\n",
       "   3.11487633934021,\n",
       "   3.1381845710754392,\n",
       "   3.1914503997802735,\n",
       "   3.2629380378723143,\n",
       "   3.1763518268585207,\n",
       "   3.2070141819000244,\n",
       "   3.3434870552062987,\n",
       "   3.344805569458008,\n",
       "   3.323783416748047,\n",
       "   3.3274699821472167,\n",
       "   3.321149967956543,\n",
       "   3.3362647132873535,\n",
       "   3.308852446746826,\n",
       "   3.4391703834533693,\n",
       "   3.5411670066833496,\n",
       "   3.4159408660888673,\n",
       "   3.4514554485321045,\n",
       "   3.4834381088256836,\n",
       "   3.510449819946289,\n",
       "   3.490743384552002,\n",
       "   3.525773285675049,\n",
       "   3.5010246227264403,\n",
       "   3.5030762214660642,\n",
       "   3.60670601272583,\n",
       "   3.5427433486938478,\n",
       "   3.61600941696167,\n",
       "   3.5797207275390623,\n",
       "   3.670063035583496,\n",
       "   3.568403566741943,\n",
       "   3.5616733543395998,\n",
       "   3.618727091217041,\n",
       "   3.6842276100158693,\n",
       "   3.710363271331787,\n",
       "   3.616920880126953,\n",
       "   3.650928373336792,\n",
       "   3.631293670654297,\n",
       "   3.5975852142333986,\n",
       "   3.6109435134887695,\n",
       "   3.776989828491211,\n",
       "   3.7098149742126463,\n",
       "   3.774541012573242,\n",
       "   3.6368288604736327,\n",
       "   3.6911001674652097,\n",
       "   3.7534227710723878,\n",
       "   3.7652262260437013,\n",
       "   3.797511240768433,\n",
       "   3.779011548614502,\n",
       "   3.8508502655029297,\n",
       "   3.875598599243164,\n",
       "   3.8766377967834473,\n",
       "   3.842534142303467,\n",
       "   3.871310241317749,\n",
       "   4.076191103363037,\n",
       "   3.9268288471221924,\n",
       "   3.9639228240966795,\n",
       "   3.8547707191467286,\n",
       "   3.887272166442871,\n",
       "   4.056444845962524,\n",
       "   3.9918316761016848,\n",
       "   3.9403819351196288,\n",
       "   3.9844449333190917,\n",
       "   4.002889730834961,\n",
       "   3.8668266998291014,\n",
       "   3.942831076812744,\n",
       "   3.942219319534302,\n",
       "   4.035690606689453,\n",
       "   4.048472937011719,\n",
       "   4.025632761383057,\n",
       "   3.964127398300171,\n",
       "   3.9736978721618654,\n",
       "   4.018531484222412,\n",
       "   3.9833782264709474,\n",
       "   4.0473910903930665,\n",
       "   4.097052701568604,\n",
       "   4.14389263381958,\n",
       "   4.0446157371521,\n",
       "   4.090746357727051,\n",
       "   4.0425118000030515,\n",
       "   4.125952238464356,\n",
       "   4.069227049255371,\n",
       "   4.098889433288575,\n",
       "   4.1219859817504885,\n",
       "   4.120706383514404,\n",
       "   4.204314323043823,\n",
       "   4.386105239105224,\n",
       "   4.085115251159668,\n",
       "   4.068088830566406,\n",
       "   4.095126049804687,\n",
       "   4.203216673278809,\n",
       "   4.0995304420471195,\n",
       "   4.193455065155029,\n",
       "   4.251674779891967],\n",
       "  'train_acc': [0.2955800000190735,\n",
       "   0.38194,\n",
       "   0.4184199999809265,\n",
       "   0.44008000000953673,\n",
       "   0.4540600000095367,\n",
       "   0.47013999999046324,\n",
       "   0.48103999999046326,\n",
       "   0.4939,\n",
       "   0.499280000038147,\n",
       "   0.5134999999904633,\n",
       "   0.519279999961853,\n",
       "   0.525280000038147,\n",
       "   0.5375600000190734,\n",
       "   0.5499600000381469,\n",
       "   0.556599999961853,\n",
       "   0.562759999961853,\n",
       "   0.57012,\n",
       "   0.5764999999618531,\n",
       "   0.5850199999809265,\n",
       "   0.595079999961853,\n",
       "   0.5993799999809265,\n",
       "   0.60502,\n",
       "   0.613200000038147,\n",
       "   0.6225000000190735,\n",
       "   0.6297199999809265,\n",
       "   0.633679999961853,\n",
       "   0.63828,\n",
       "   0.64264,\n",
       "   0.64738,\n",
       "   0.6564200000190735,\n",
       "   0.6680799999809265,\n",
       "   0.673220000038147,\n",
       "   0.6759200000190735,\n",
       "   0.683220000038147,\n",
       "   0.6902000000190734,\n",
       "   0.693119999961853,\n",
       "   0.6982999999809265,\n",
       "   0.70594,\n",
       "   0.7087399999809265,\n",
       "   0.716179999961853,\n",
       "   0.7271199999809265,\n",
       "   0.7229000000190735,\n",
       "   0.73244,\n",
       "   0.738320000038147,\n",
       "   0.7435800000190735,\n",
       "   0.7458800000381469,\n",
       "   0.753980000038147,\n",
       "   0.7540600000190735,\n",
       "   0.760480000038147,\n",
       "   0.762139999961853,\n",
       "   0.7705399999809265,\n",
       "   0.7802,\n",
       "   0.772359999961853,\n",
       "   0.7876400000190735,\n",
       "   0.7881999999618531,\n",
       "   0.7953199999809265,\n",
       "   0.7989600000381469,\n",
       "   0.7958599999618531,\n",
       "   0.8036200000381469,\n",
       "   0.8053,\n",
       "   0.8146000000190735,\n",
       "   0.8113400000190735,\n",
       "   0.8163599999809266,\n",
       "   0.8188599999809265,\n",
       "   0.818000000038147,\n",
       "   0.8265799999809265,\n",
       "   0.8316000000190735,\n",
       "   0.834220000038147,\n",
       "   0.8322000000190735,\n",
       "   0.8413200000190735,\n",
       "   0.829679999961853,\n",
       "   0.83914,\n",
       "   0.84516,\n",
       "   0.85034,\n",
       "   0.8507200000190734,\n",
       "   0.850659999961853,\n",
       "   0.846860000038147,\n",
       "   0.850200000038147,\n",
       "   0.8634399999809265,\n",
       "   0.8668399999809265,\n",
       "   0.862739999961853,\n",
       "   0.8607000000190735,\n",
       "   0.868280000038147,\n",
       "   0.86538,\n",
       "   0.869919999961853,\n",
       "   0.8628399999809265,\n",
       "   0.872459999961853,\n",
       "   0.879979999961853,\n",
       "   0.8769799999809265,\n",
       "   0.88114,\n",
       "   0.8787200000190735,\n",
       "   0.87498,\n",
       "   0.8859999999809265,\n",
       "   0.884020000038147,\n",
       "   0.887019999961853,\n",
       "   0.8872200000381469,\n",
       "   0.890620000038147,\n",
       "   0.8841,\n",
       "   0.8851999999618531,\n",
       "   0.8822400000190734,\n",
       "   0.89518,\n",
       "   0.8949000000190734,\n",
       "   0.9013199999809265,\n",
       "   0.893940000038147,\n",
       "   0.896680000038147,\n",
       "   0.9007999999809265,\n",
       "   0.90102,\n",
       "   0.8961000000381469,\n",
       "   0.890399999961853,\n",
       "   0.8932599999809265,\n",
       "   0.9105600000190734,\n",
       "   0.8974400000190735,\n",
       "   0.90558,\n",
       "   0.90536,\n",
       "   0.903999999961853,\n",
       "   0.9091400000190735,\n",
       "   0.910600000038147,\n",
       "   0.898180000038147,\n",
       "   0.9138999999809265,\n",
       "   0.905499999961853,\n",
       "   0.9128600000381469,\n",
       "   0.9162999999809265,\n",
       "   0.9186399999618531,\n",
       "   0.9093400000190734,\n",
       "   0.9081799999809265,\n",
       "   0.918480000038147,\n",
       "   0.91348,\n",
       "   0.9182999999618531,\n",
       "   0.9100199999809265,\n",
       "   0.923680000038147,\n",
       "   0.9081,\n",
       "   0.921240000038147,\n",
       "   0.9180799999618531,\n",
       "   0.91974,\n",
       "   0.924779999961853,\n",
       "   0.905139999961853,\n",
       "   0.9164000000190735,\n",
       "   0.9262799999618531,\n",
       "   0.9291800000190735,\n",
       "   0.932780000038147,\n",
       "   0.9196400000381469,\n",
       "   0.9099199999809265,\n",
       "   0.924020000038147,\n",
       "   0.929920000038147,\n",
       "   0.928160000038147,\n",
       "   0.921140000038147,\n",
       "   0.9303200000190734,\n",
       "   0.921500000038147,\n",
       "   0.929100000038147,\n",
       "   0.918800000038147,\n",
       "   0.9229999999809265,\n",
       "   0.9305199999809265,\n",
       "   0.941520000038147,\n",
       "   0.943219999961853,\n",
       "   0.9238800000190734,\n",
       "   0.9265399999809265,\n",
       "   0.927240000038147,\n",
       "   0.935660000038147,\n",
       "   0.927919999961853,\n",
       "   0.9478799999809265,\n",
       "   0.9499199999809265,\n",
       "   0.9188600000190735,\n",
       "   0.924539999961853,\n",
       "   0.937440000038147,\n",
       "   0.9377400000190735,\n",
       "   0.9457199999618531,\n",
       "   0.933440000038147,\n",
       "   0.9160200000190735,\n",
       "   0.9380200000190735,\n",
       "   0.933580000038147,\n",
       "   0.9482400000190735,\n",
       "   0.9369,\n",
       "   0.9277399999618531,\n",
       "   0.9419399999809265,\n",
       "   0.939359999961853,\n",
       "   0.9367200000381469,\n",
       "   0.9443400000190735,\n",
       "   0.941560000038147,\n",
       "   0.9351000000190735,\n",
       "   0.928159999961853,\n",
       "   0.94168,\n",
       "   0.9473200000190735,\n",
       "   0.9348200000190735,\n",
       "   0.9509999999809265,\n",
       "   0.937319999961853,\n",
       "   0.932079999961853,\n",
       "   0.946679999961853,\n",
       "   0.94594,\n",
       "   0.94426,\n",
       "   0.9396199999809265,\n",
       "   0.9487999999809266,\n",
       "   0.949320000038147,\n",
       "   0.9577399999809265,\n",
       "   0.920380000038147,\n",
       "   0.9372399999809266,\n",
       "   0.941560000038147,\n",
       "   0.957340000038147,\n",
       "   0.95248,\n",
       "   0.9435799999809266,\n",
       "   0.9346399999809265],\n",
       "  'valid_acc': [0.3552,\n",
       "   0.4065,\n",
       "   0.4287,\n",
       "   0.4515,\n",
       "   0.4594,\n",
       "   0.4715,\n",
       "   0.4756,\n",
       "   0.4864,\n",
       "   0.4783,\n",
       "   0.4946,\n",
       "   0.5006,\n",
       "   0.4949,\n",
       "   0.4971,\n",
       "   0.4931,\n",
       "   0.5078,\n",
       "   0.5024,\n",
       "   0.5089,\n",
       "   0.5054,\n",
       "   0.5093,\n",
       "   0.5094,\n",
       "   0.5262,\n",
       "   0.5234,\n",
       "   0.5126,\n",
       "   0.5159,\n",
       "   0.5136,\n",
       "   0.5203,\n",
       "   0.5225,\n",
       "   0.5098,\n",
       "   0.5183,\n",
       "   0.5187,\n",
       "   0.5259,\n",
       "   0.5235,\n",
       "   0.5254,\n",
       "   0.5219,\n",
       "   0.5152,\n",
       "   0.5152,\n",
       "   0.5251,\n",
       "   0.5155,\n",
       "   0.5246,\n",
       "   0.5251,\n",
       "   0.5073,\n",
       "   0.5141,\n",
       "   0.5117,\n",
       "   0.5152,\n",
       "   0.5179,\n",
       "   0.5044,\n",
       "   0.5082,\n",
       "   0.5208,\n",
       "   0.5128,\n",
       "   0.5077,\n",
       "   0.5181,\n",
       "   0.5063,\n",
       "   0.5096,\n",
       "   0.508,\n",
       "   0.5134,\n",
       "   0.508,\n",
       "   0.5126,\n",
       "   0.5101,\n",
       "   0.5007,\n",
       "   0.5108,\n",
       "   0.5101,\n",
       "   0.5114,\n",
       "   0.5114,\n",
       "   0.5152,\n",
       "   0.505,\n",
       "   0.5096,\n",
       "   0.4974,\n",
       "   0.5092,\n",
       "   0.5026,\n",
       "   0.509,\n",
       "   0.4908,\n",
       "   0.5035,\n",
       "   0.4948,\n",
       "   0.5038,\n",
       "   0.5072,\n",
       "   0.506,\n",
       "   0.5027,\n",
       "   0.5034,\n",
       "   0.5074,\n",
       "   0.5062,\n",
       "   0.502,\n",
       "   0.5036,\n",
       "   0.498,\n",
       "   0.507,\n",
       "   0.5001,\n",
       "   0.5125,\n",
       "   0.4996,\n",
       "   0.508,\n",
       "   0.4955,\n",
       "   0.5104,\n",
       "   0.503,\n",
       "   0.5069,\n",
       "   0.4961,\n",
       "   0.5027,\n",
       "   0.5049,\n",
       "   0.5017,\n",
       "   0.5037,\n",
       "   0.5015,\n",
       "   0.5014,\n",
       "   0.5036,\n",
       "   0.4932,\n",
       "   0.4974,\n",
       "   0.4996,\n",
       "   0.4962,\n",
       "   0.5057,\n",
       "   0.4938,\n",
       "   0.4994,\n",
       "   0.4927,\n",
       "   0.491,\n",
       "   0.4928,\n",
       "   0.5006,\n",
       "   0.4965,\n",
       "   0.5027,\n",
       "   0.5044,\n",
       "   0.502,\n",
       "   0.4928,\n",
       "   0.4937,\n",
       "   0.4924,\n",
       "   0.4962,\n",
       "   0.4962,\n",
       "   0.5053,\n",
       "   0.4974,\n",
       "   0.4998,\n",
       "   0.4971,\n",
       "   0.4998,\n",
       "   0.4959,\n",
       "   0.4955,\n",
       "   0.4989,\n",
       "   0.5013,\n",
       "   0.5019,\n",
       "   0.4889,\n",
       "   0.4988,\n",
       "   0.4995,\n",
       "   0.5019,\n",
       "   0.4947,\n",
       "   0.4897,\n",
       "   0.4957,\n",
       "   0.5022,\n",
       "   0.5002,\n",
       "   0.4985,\n",
       "   0.4915,\n",
       "   0.4956,\n",
       "   0.4948,\n",
       "   0.5033,\n",
       "   0.506,\n",
       "   0.4998,\n",
       "   0.499,\n",
       "   0.5055,\n",
       "   0.4961,\n",
       "   0.4966,\n",
       "   0.4952,\n",
       "   0.5005,\n",
       "   0.5039,\n",
       "   0.5024,\n",
       "   0.4951,\n",
       "   0.4929,\n",
       "   0.4917,\n",
       "   0.4982,\n",
       "   0.4925,\n",
       "   0.4982,\n",
       "   0.492,\n",
       "   0.494,\n",
       "   0.4907,\n",
       "   0.5003,\n",
       "   0.4994,\n",
       "   0.4928,\n",
       "   0.4932,\n",
       "   0.4984,\n",
       "   0.4974,\n",
       "   0.4959,\n",
       "   0.5013,\n",
       "   0.4965,\n",
       "   0.5061,\n",
       "   0.5009,\n",
       "   0.4939,\n",
       "   0.4969,\n",
       "   0.4994,\n",
       "   0.4946,\n",
       "   0.4927,\n",
       "   0.4978,\n",
       "   0.4925,\n",
       "   0.4931,\n",
       "   0.4931,\n",
       "   0.4949,\n",
       "   0.4941,\n",
       "   0.502,\n",
       "   0.4975,\n",
       "   0.4953,\n",
       "   0.4978,\n",
       "   0.4892,\n",
       "   0.4983,\n",
       "   0.4973,\n",
       "   0.4953,\n",
       "   0.492,\n",
       "   0.4965,\n",
       "   0.5029,\n",
       "   0.4959,\n",
       "   0.4888,\n",
       "   0.4935,\n",
       "   0.4922]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T13:45:34.614572Z",
     "start_time": "2019-07-24T13:45:34.189852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "color_bar = [\"r\", \"g\", \"b\", \"y\", \"m\", \"k\"]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cond in enumerate(results.keys()):\n",
    "    plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n",
    "    plt.plot(range(len(results[cond]['valid_loss'])),results[cond]['valid_loss'], '--', label=cond, color=color_bar[i])\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cond in enumerate(results.keys()):\n",
    "    plt.plot(range(len(results[cond]['train_acc'])),results[cond]['train_acc'], '-', label=cond, color=color_bar[i])\n",
    "    plt.plot(range(len(results[cond]['valid_acc'])),results[cond]['valid_acc'], '--', label=cond, color=color_bar[i])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "在這裡原本要跑LR的迴圈，當我想看增加層數的改變，所以改成增加層數的語法。\n",
    "\n",
    "但有保留LR的迴圈，想跑LR迴圈的人可以自己改參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:40:00.544102Z",
     "start_time": "2019-07-24T14:39:56.583440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "\n",
    "# Disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:40:01.564066Z",
     "start_time": "2019-07-24T14:40:01.259029Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:40:02.165891Z",
     "start_time": "2019-07-24T14:40:02.160401Z"
    }
   },
   "outputs": [],
   "source": [
    "## 資料前處理\n",
    "def preproc_x(x, flatten=True):\n",
    "    x = x / 255.\n",
    "    if flatten:\n",
    "        x = x.reshape((len(x), -1))\n",
    "    return x\n",
    "\n",
    "def preproc_y(y, num_classes=10):\n",
    "    if y.shape[-1] == 1:\n",
    "        y = keras.utils.to_categorical(y, num_classes)\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:40:04.338987Z",
     "start_time": "2019-07-24T14:40:02.815774Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "\n",
    "# Preproc the inputs\n",
    "x_train = preproc_x(x_train)\n",
    "x_test = preproc_x(x_test)\n",
    "\n",
    "# Preprc the outputs\n",
    "y_train = preproc_y(y_train)\n",
    "y_test = preproc_y(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:40:05.270881Z",
     "start_time": "2019-07-24T14:40:05.261493Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_shape, output_units=10, num_neurons=[2048, 1024, 512, 256, 128, 64, 32]):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "    \n",
    "    for i, n_units in enumerate(num_neurons):\n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(units=n_units, activation='relu', name='hidden_layer'+ str(i+1))(input_layer)\n",
    "        else:\n",
    "            x = keras.layers.Dense(units=n_units, activation='relu', name='hidden_layer'+ str(i+1))(x)\n",
    "            \n",
    "    out = keras.layers.Dense(units=output_units, activation='softmax', name='output')(x)\n",
    "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:40:06.252037Z",
     "start_time": "2019-07-24T14:40:06.247569Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = [0.001]\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "# MOMENTUM = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T14:57:11.313461Z",
     "start_time": "2019-07-24T14:40:07.957020Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 22:40:07.966478  3196 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0724 22:40:07.968173  3196 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0724 22:40:07.989757  3196 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0724 22:40:07.992237  3196 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0724 22:40:07.995709  3196 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0724 22:40:08.126650  3196 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expertiment with： 0.001 LEARNING_RATE\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 2048)              6293504   \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "hidden_layer4 (Dense)        (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "hidden_layer5 (Dense)        (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "hidden_layer6 (Dense)        (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "hidden_layer7 (Dense)        (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 9,091,370\n",
      "Trainable params: 9,091,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 22:40:08.317612  3196 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 16s 326us/step - loss: 2.0499 - acc: 0.2283 - val_loss: 1.8833 - val_acc: 0.2943\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 14s 277us/step - loss: 1.8193 - acc: 0.3341 - val_loss: 1.7769 - val_acc: 0.3596\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 14s 278us/step - loss: 1.7195 - acc: 0.3757 - val_loss: 1.6933 - val_acc: 0.3907\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 14s 279us/step - loss: 1.6427 - acc: 0.4086 - val_loss: 1.6806 - val_acc: 0.3950\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 15s 294us/step - loss: 1.5673 - acc: 0.4362 - val_loss: 1.5449 - val_acc: 0.4494\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 1.5243 - acc: 0.4528 - val_loss: 1.5365 - val_acc: 0.4530\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 14s 280us/step - loss: 1.4792 - acc: 0.4694 - val_loss: 1.4673 - val_acc: 0.4749\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 14s 281us/step - loss: 1.4391 - acc: 0.4841 - val_loss: 1.4746 - val_acc: 0.4771\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 14s 282us/step - loss: 1.4007 - acc: 0.4996 - val_loss: 1.4365 - val_acc: 0.4910\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 14s 282us/step - loss: 1.3712 - acc: 0.5104 - val_loss: 1.4211 - val_acc: 0.4982\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 14s 283us/step - loss: 1.3407 - acc: 0.5217 - val_loss: 1.4469 - val_acc: 0.4893\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 14s 282us/step - loss: 1.2997 - acc: 0.5359 - val_loss: 1.3892 - val_acc: 0.5168\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 14s 281us/step - loss: 1.2728 - acc: 0.5455 - val_loss: 1.3888 - val_acc: 0.5073\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 1.2406 - acc: 0.5554 - val_loss: 1.3912 - val_acc: 0.5167\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 14s 282us/step - loss: 1.1977 - acc: 0.5728 - val_loss: 1.4097 - val_acc: 0.5101\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 14s 284us/step - loss: 1.1626 - acc: 0.5846 - val_loss: 1.3598 - val_acc: 0.5258\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 1.1291 - acc: 0.5956 - val_loss: 1.3933 - val_acc: 0.5172\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 1.1022 - acc: 0.6049 - val_loss: 1.4027 - val_acc: 0.5254\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 14s 284us/step - loss: 1.0697 - acc: 0.6178 - val_loss: 1.4072 - val_acc: 0.5191\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 1.0227 - acc: 0.6300 - val_loss: 1.4084 - val_acc: 0.5303\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 15s 297us/step - loss: 0.9857 - acc: 0.6468 - val_loss: 1.4205 - val_acc: 0.5291\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.9524 - acc: 0.6580 - val_loss: 1.4419 - val_acc: 0.5344\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 0.9060 - acc: 0.6750 - val_loss: 1.4635 - val_acc: 0.5199\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 14s 284us/step - loss: 0.8618 - acc: 0.6909 - val_loss: 1.4700 - val_acc: 0.5296\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 0.8295 - acc: 0.7033 - val_loss: 1.5378 - val_acc: 0.5259\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 0.8017 - acc: 0.7111 - val_loss: 1.5611 - val_acc: 0.5302\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 0.7597 - acc: 0.7265 - val_loss: 1.6007 - val_acc: 0.5188\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 0.7109 - acc: 0.7438 - val_loss: 1.6914 - val_acc: 0.5245\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 0.6807 - acc: 0.7562 - val_loss: 1.7573 - val_acc: 0.5133\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 0.6579 - acc: 0.7637 - val_loss: 1.7724 - val_acc: 0.5064\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.6117 - acc: 0.7800 - val_loss: 1.7950 - val_acc: 0.5265\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.5848 - acc: 0.7898 - val_loss: 1.9207 - val_acc: 0.5116\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.5456 - acc: 0.8061 - val_loss: 2.0719 - val_acc: 0.5062\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.5197 - acc: 0.8157 - val_loss: 2.0713 - val_acc: 0.5144\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.4906 - acc: 0.8249 - val_loss: 2.0990 - val_acc: 0.5142\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.4525 - acc: 0.8392 - val_loss: 2.1364 - val_acc: 0.5204\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 0.4453 - acc: 0.8406 - val_loss: 2.2130 - val_acc: 0.5139\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 0.4153 - acc: 0.8530 - val_loss: 2.3725 - val_acc: 0.5102\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 15s 304us/step - loss: 0.4049 - acc: 0.8565 - val_loss: 2.4083 - val_acc: 0.5166\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.3708 - acc: 0.8681 - val_loss: 2.4175 - val_acc: 0.5120\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 16s 311us/step - loss: 0.3574 - acc: 0.8732 - val_loss: 2.4049 - val_acc: 0.5046\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 15s 298us/step - loss: 0.3473 - acc: 0.8774 - val_loss: 2.5606 - val_acc: 0.5096\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.3156 - acc: 0.8891 - val_loss: 2.5830 - val_acc: 0.4992\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.3129 - acc: 0.8889 - val_loss: 2.5424 - val_acc: 0.5056\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.3221 - acc: 0.8854 - val_loss: 2.8084 - val_acc: 0.5142\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.2810 - acc: 0.9012 - val_loss: 2.7353 - val_acc: 0.5102\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 15s 298us/step - loss: 0.2949 - acc: 0.8979 - val_loss: 2.6999 - val_acc: 0.5114\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.2715 - acc: 0.9055 - val_loss: 2.6199 - val_acc: 0.5124\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 0.2516 - acc: 0.9135 - val_loss: 2.9626 - val_acc: 0.5168\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 15s 290us/step - loss: 0.2691 - acc: 0.9058 - val_loss: 2.7288 - val_acc: 0.5116\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.2402 - acc: 0.9158 - val_loss: 2.9943 - val_acc: 0.5064\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.2378 - acc: 0.9168 - val_loss: 2.9321 - val_acc: 0.5097\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.2070 - acc: 0.9281 - val_loss: 3.0325 - val_acc: 0.5057\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.2297 - acc: 0.9213 - val_loss: 2.9856 - val_acc: 0.5038\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.2069 - acc: 0.9275 - val_loss: 3.0339 - val_acc: 0.5089\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.2109 - acc: 0.9265 - val_loss: 3.1522 - val_acc: 0.5129\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.2058 - acc: 0.9274 - val_loss: 3.1274 - val_acc: 0.5102\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.1911 - acc: 0.9330 - val_loss: 3.2375 - val_acc: 0.5014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.1964 - acc: 0.9321 - val_loss: 3.1025 - val_acc: 0.5123\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 0.1968 - acc: 0.9323 - val_loss: 3.1822 - val_acc: 0.4963\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 14s 287us/step - loss: 0.1769 - acc: 0.9388 - val_loss: 3.2951 - val_acc: 0.5124\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 14s 287us/step - loss: 0.1761 - acc: 0.9392 - val_loss: 3.1796 - val_acc: 0.5122\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 14s 287us/step - loss: 0.1775 - acc: 0.9396 - val_loss: 3.1524 - val_acc: 0.5217\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 0.1681 - acc: 0.9425 - val_loss: 3.3179 - val_acc: 0.5116\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 14s 285us/step - loss: 0.1341 - acc: 0.9533 - val_loss: 3.5092 - val_acc: 0.5066\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 0.1809 - acc: 0.9387 - val_loss: 3.3045 - val_acc: 0.5087\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 14s 287us/step - loss: 0.1436 - acc: 0.9519 - val_loss: 3.4201 - val_acc: 0.5069\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.1549 - acc: 0.9474 - val_loss: 3.4374 - val_acc: 0.5071\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.1627 - acc: 0.9447 - val_loss: 3.3924 - val_acc: 0.5164\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 14s 290us/step - loss: 0.1313 - acc: 0.9550 - val_loss: 3.4938 - val_acc: 0.5099\n",
      "Epoch 71/200\n",
      "41984/50000 [========================>.....] - ETA: 2s - loss: 0.1779 - acc: 0.9389"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-377881841395>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m               shuffle=True)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# collect results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for lr in LEARNING_RATE:\n",
    "    keras.backend.clear_session() # 清除舊的Graph\n",
    "    print('Expertiment with： ' + str(lr) + ' LEARNING_RATE')\n",
    "    model = build_mlp(input_shape=x_train.shape[1:])\n",
    "    model.summary()\n",
    "    optimizer = keras.optimizers.Adam(lr=lr)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "    \n",
    "    # collect results\n",
    "    train_loss = model.history.history[\"loss\"]\n",
    "    valid_loss = model.history.history[\"val_loss\"]\n",
    "    train_acc = model.history.history[\"acc\"]\n",
    "    valid_acc = model.history.history[\"val_acc\"]\n",
    "    \n",
    "    exp_name_tag = opt\n",
    "    results[exp_name_tag] = {'train-loss':train_loss,\n",
    "                             'valid_loss':valid_loss,\n",
    "                             'train_acc':train_acc,\n",
    "                             'valid_acc':valid_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "實在不想等了，自己按停止。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T15:18:36.902351Z",
     "start_time": "2019-07-24T14:57:56.628938Z"
    },
    "code_folding": [
     0,
     19
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expertiment with： 0.001 LEARNING_RATE\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 2048)              6293504   \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "hidden_layer4 (Dense)        (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "hidden_layer5 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "hidden_layer6 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "hidden_layer7 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 9,247,754\n",
      "Trainable params: 9,247,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 17s 330us/step - loss: 2.0332 - acc: 0.2417 - val_loss: 1.8009 - val_acc: 0.3398\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 1.7863 - acc: 0.3502 - val_loss: 1.7009 - val_acc: 0.3792\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 1.6948 - acc: 0.3871 - val_loss: 1.6476 - val_acc: 0.4092\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 15s 298us/step - loss: 1.6198 - acc: 0.4184 - val_loss: 1.5490 - val_acc: 0.4338\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 1.5615 - acc: 0.4393 - val_loss: 1.5318 - val_acc: 0.4520\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 1.5105 - acc: 0.4573 - val_loss: 1.5820 - val_acc: 0.4372\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 15s 294us/step - loss: 1.4664 - acc: 0.4733 - val_loss: 1.5391 - val_acc: 0.4437\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 1.4236 - acc: 0.4887 - val_loss: 1.4674 - val_acc: 0.4743\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 14s 290us/step - loss: 1.3925 - acc: 0.4999 - val_loss: 1.4362 - val_acc: 0.4892\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 1.3462 - acc: 0.5152 - val_loss: 1.4265 - val_acc: 0.4918\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 1.3201 - acc: 0.5266 - val_loss: 1.4321 - val_acc: 0.4912\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 1.2962 - acc: 0.5357 - val_loss: 1.3832 - val_acc: 0.5146\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 1.2516 - acc: 0.5515 - val_loss: 1.3968 - val_acc: 0.5092\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 1.2343 - acc: 0.5568 - val_loss: 1.3804 - val_acc: 0.5110\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 1.1943 - acc: 0.5676 - val_loss: 1.4118 - val_acc: 0.5115\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 1.1573 - acc: 0.5818 - val_loss: 1.4130 - val_acc: 0.5083\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 1.1333 - acc: 0.5893 - val_loss: 1.4255 - val_acc: 0.5076\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 1.1000 - acc: 0.6017 - val_loss: 1.3962 - val_acc: 0.5226\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 1.0617 - acc: 0.6170 - val_loss: 1.4025 - val_acc: 0.5133\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 15s 296us/step - loss: 1.0152 - acc: 0.6333 - val_loss: 1.4390 - val_acc: 0.5236\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 15s 294us/step - loss: 1.0009 - acc: 0.6372 - val_loss: 1.3938 - val_acc: 0.5289\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.9567 - acc: 0.6538 - val_loss: 1.4803 - val_acc: 0.5198\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 15s 294us/step - loss: 0.9148 - acc: 0.6698 - val_loss: 1.5152 - val_acc: 0.5134\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.8846 - acc: 0.6807 - val_loss: 1.5242 - val_acc: 0.5225\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 15s 297us/step - loss: 0.8552 - acc: 0.6905 - val_loss: 1.5468 - val_acc: 0.5127\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.8141 - acc: 0.7036 - val_loss: 1.6101 - val_acc: 0.5187\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 15s 297us/step - loss: 0.7783 - acc: 0.7175 - val_loss: 1.6575 - val_acc: 0.5029\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 15s 304us/step - loss: 0.7435 - acc: 0.7291 - val_loss: 1.6823 - val_acc: 0.5154\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 15s 303us/step - loss: 0.7147 - acc: 0.7400 - val_loss: 1.7611 - val_acc: 0.4992\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.6888 - acc: 0.7493 - val_loss: 1.7534 - val_acc: 0.5086\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 15s 305us/step - loss: 0.6496 - acc: 0.7647 - val_loss: 1.8942 - val_acc: 0.5105\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 15s 297us/step - loss: 0.6288 - acc: 0.7725 - val_loss: 1.8982 - val_acc: 0.5098\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 15s 294us/step - loss: 0.5956 - acc: 0.7856 - val_loss: 1.9029 - val_acc: 0.5120\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.5487 - acc: 0.8020 - val_loss: 2.0300 - val_acc: 0.5117\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 15s 298us/step - loss: 0.5506 - acc: 0.7997 - val_loss: 2.0768 - val_acc: 0.5086\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 15s 296us/step - loss: 0.5049 - acc: 0.8186 - val_loss: 2.1722 - val_acc: 0.5090\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.4817 - acc: 0.8269 - val_loss: 2.2456 - val_acc: 0.4952\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 0.4940 - acc: 0.8215 - val_loss: 2.2238 - val_acc: 0.5045\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 14s 290us/step - loss: 0.4454 - acc: 0.8398 - val_loss: 2.2896 - val_acc: 0.5028\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 0.4417 - acc: 0.8398 - val_loss: 2.2672 - val_acc: 0.5063\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.4075 - acc: 0.8541 - val_loss: 2.2469 - val_acc: 0.5047\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 15s 299us/step - loss: 0.3947 - acc: 0.8588 - val_loss: 2.4694 - val_acc: 0.5089\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.3800 - acc: 0.8642 - val_loss: 2.4587 - val_acc: 0.5062\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 15s 296us/step - loss: 0.3762 - acc: 0.8649 - val_loss: 2.4404 - val_acc: 0.5027\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 16s 324us/step - loss: 0.3700 - acc: 0.8689 - val_loss: 2.5848 - val_acc: 0.5046\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 17s 331us/step - loss: 0.3395 - acc: 0.8784 - val_loss: 2.7449 - val_acc: 0.5008\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 15s 299us/step - loss: 0.3152 - acc: 0.8861 - val_loss: 2.6659 - val_acc: 0.5041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.2988 - acc: 0.8945 - val_loss: 2.8064 - val_acc: 0.5078\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 14s 290us/step - loss: 0.3064 - acc: 0.8901 - val_loss: 2.7603 - val_acc: 0.5055\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 0.2859 - acc: 0.8981 - val_loss: 2.8999 - val_acc: 0.4983\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.2974 - acc: 0.8941 - val_loss: 2.7954 - val_acc: 0.4968\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 15s 306us/step - loss: 0.2757 - acc: 0.9020 - val_loss: 2.9687 - val_acc: 0.4952\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 15s 294us/step - loss: 0.2636 - acc: 0.9066 - val_loss: 2.9262 - val_acc: 0.5046\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 0.2843 - acc: 0.9001 - val_loss: 2.9049 - val_acc: 0.4970\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 15s 290us/step - loss: 0.2544 - acc: 0.9092 - val_loss: 2.8994 - val_acc: 0.4989\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 0.2444 - acc: 0.9139 - val_loss: 2.9671 - val_acc: 0.5030\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 15s 290us/step - loss: 0.2363 - acc: 0.9189 - val_loss: 3.0073 - val_acc: 0.5048\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.2467 - acc: 0.9142 - val_loss: 2.9707 - val_acc: 0.4900\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 15s 294us/step - loss: 0.2220 - acc: 0.9216 - val_loss: 3.0987 - val_acc: 0.5007\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.2286 - acc: 0.9204 - val_loss: 3.0099 - val_acc: 0.4983\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 15s 299us/step - loss: 0.2255 - acc: 0.9211 - val_loss: 2.9131 - val_acc: 0.4902\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 16s 322us/step - loss: 0.2200 - acc: 0.9225 - val_loss: 2.9422 - val_acc: 0.4942\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 15s 305us/step - loss: 0.2019 - acc: 0.9290 - val_loss: 3.1265 - val_acc: 0.4998\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 15s 303us/step - loss: 0.1920 - acc: 0.9323 - val_loss: 3.2469 - val_acc: 0.4937\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 15s 302us/step - loss: 0.2011 - acc: 0.9286 - val_loss: 3.0026 - val_acc: 0.4948\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.1797 - acc: 0.9381 - val_loss: 3.2727 - val_acc: 0.5038\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 15s 296us/step - loss: 0.1922 - acc: 0.9336 - val_loss: 3.1939 - val_acc: 0.4914\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 15s 296us/step - loss: 0.1860 - acc: 0.9350 - val_loss: 3.2071 - val_acc: 0.4987\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.1875 - acc: 0.9342 - val_loss: 3.3315 - val_acc: 0.5030\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 15s 290us/step - loss: 0.1766 - acc: 0.9388 - val_loss: 3.3868 - val_acc: 0.4935\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 15s 302us/step - loss: 0.1747 - acc: 0.9400 - val_loss: 3.2855 - val_acc: 0.5021\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 16s 318us/step - loss: 0.1629 - acc: 0.9439 - val_loss: 3.3745 - val_acc: 0.4964\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 15s 297us/step - loss: 0.1794 - acc: 0.9371 - val_loss: 3.1235 - val_acc: 0.5016\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 15s 296us/step - loss: 0.1620 - acc: 0.9443 - val_loss: 3.2006 - val_acc: 0.5080\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 0.1817 - acc: 0.9378 - val_loss: 3.2307 - val_acc: 0.4980\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.1717 - acc: 0.9405 - val_loss: 3.3788 - val_acc: 0.4895\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.1707 - acc: 0.9405 - val_loss: 3.1951 - val_acc: 0.4968\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.1737 - acc: 0.9394 - val_loss: 3.2548 - val_acc: 0.4994\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 15s 296us/step - loss: 0.1591 - acc: 0.9462 - val_loss: 3.2764 - val_acc: 0.4950\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 15s 295us/step - loss: 0.1470 - acc: 0.9499 - val_loss: 3.3044 - val_acc: 0.4964\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 15s 294us/step - loss: 0.1531 - acc: 0.9483 - val_loss: 3.6974 - val_acc: 0.4940\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 15s 294us/step - loss: 0.1559 - acc: 0.9474 - val_loss: 3.3685 - val_acc: 0.4979\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 15s 293us/step - loss: 0.1367 - acc: 0.9531 - val_loss: 3.4675 - val_acc: 0.4966\n",
      "Epoch 84/200\n",
      "25088/50000 [==============>...............] - ETA: 6s - loss: 0.1580 - acc: 0.9467"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d9fb653c8f2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m               shuffle=True)\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# collect results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def build_mlp(input_shape, output_units=10, num_neurons=[2048, 1024, 512, 256, 256, 256, 256]):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "    \n",
    "    for i, n_units in enumerate(num_neurons):\n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(units=n_units, activation='relu', name='hidden_layer'+ str(i+1))(input_layer)\n",
    "        else:\n",
    "            x = keras.layers.Dense(units=n_units, activation='relu', name='hidden_layer'+ str(i+1))(x)\n",
    "            \n",
    "    out = keras.layers.Dense(units=output_units, activation='softmax', name='output')(x)\n",
    "    model = keras.models.Model(inputs=[input_layer], outputs=[out])\n",
    "    return model\n",
    "\n",
    "LEARNING_RATE = [0.001]\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "# MOMENTUM = 0.75\n",
    "\n",
    "results = {}\n",
    "for lr in LEARNING_RATE:\n",
    "    keras.backend.clear_session() # 清除舊的Graph\n",
    "    print('Expertiment with： ' + str(lr) + ' LEARNING_RATE')\n",
    "    model = build_mlp(input_shape=x_train.shape[1:])\n",
    "    model.summary()\n",
    "    optimizer = keras.optimizers.Adam(lr=lr)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "    \n",
    "    # collect results\n",
    "    train_loss = model.history.history[\"loss\"]\n",
    "    valid_loss = model.history.history[\"val_loss\"]\n",
    "    train_acc = model.history.history[\"acc\"]\n",
    "    valid_acc = model.history.history[\"val_acc\"]\n",
    "    \n",
    "    exp_name_tag = opt\n",
    "    results[exp_name_tag] = {'train-loss':train_loss,\n",
    "                             'valid_loss':valid_loss,\n",
    "                             'train_acc':train_acc,\n",
    "                             'valid_acc':valid_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_loss的值越來越高，繼續算下去不會變的更好，先在這邊停止了~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
