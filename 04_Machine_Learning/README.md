[TOC]

# å»ºç½®æ¨¡å‹(Modeling)

## ç°¡ä»‹

æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„å°ˆæ¡ˆå¯ä»¥ä¾æ“šã€Œæ˜¯å¦æœ‰ç›®æ¨™è®Šæ•¸ã€ä»¥åŠã€Œæ¨¡å‹çš„ç”¢å‡ºç‚ºæ•¸å€¼æˆ–åˆ†é¡è³‡æ–™ã€ï¼Œå°‡æ¨¡å‹å€åˆ†ç‚ºä»¥ä¸‹å››å€‹é¡å‹

- åˆ†é¡æ¨¡å‹
- å›æ­¸æ¨¡å‹
- åˆ†ç¾¤æ¨¡å‹
- é™ç¶­æ¨¡å‹

![](C:/Users/TLYu0419/Documents/Github/DataScience/images/four-corners.png)

- Supervised Learning

  - where we have inputs, and one (or more) response variable(s).
  - å¦‚æœæˆ‘å€‘çš„è³‡æ–™å·²ç¶“æœ‰æ˜ç¢ºçš„ç›®æ¨™è®Šæ•¸ï¼Œæˆ‘å€‘å¯ä»¥ç›´æ¥è®“æ¨¡å‹å°ˆæ³¨åœ¨ç›®æ¨™è®Šæ•¸çš„è®ŠåŒ–
  - æ‰¾å‡ºè®“è¨“ç·´â½¬æ¨™æœ€ä½³çš„æ¨¡å‹åƒæ•¸
    - æ¨¡å‹çš„åƒæ•¸çµ„åˆå¯èƒ½æœ‰ç„¡é™å¤šçµ„ï¼Œæˆ‘å€‘å¯ä»¥â½¤æš´â¼’æ³•æ¯å€‹åƒæ•¸éƒ½è©¦çœ‹çœ‹ï¼Œå¾ä¸­æ‰¾åˆ°è®“æå¤±å‡½æ•¸æœ€â¼©çš„åƒæ•¸
    - ä½†æ˜¯é€™æ¨£éå¸¸æ²’æœ‰æ•ˆç‡ï¼Œæœ‰è¨±å¤šåƒæ˜¯æ¢¯åº¦ä¸‹é™ (Gradient Descent)ã€å¢é‡è¨“ç·´ (Additive Training) ç­‰â½…å¼ï¼Œé€™äº›æ¼”ç®—æ³•å¯ä»¥å¹«æˆ‘å€‘æ‰¾åˆ°å¯èƒ½çš„æœ€ä½³æ¨¡å‹åƒæ•¸

- Unsupervised Learning

  - where we have inputs, but not response variables.

  - åœ¨ä¸æ¸…æ¥šè³‡æ–™ç‰¹æ€§ã€å•é¡Œå®šç¾©ã€æ²’æœ‰æ¨™è¨˜çš„æƒ…æ³ä¸‹ï¼Œéç›£ç£å¼å­¸ç¿’æŠ€è¡“å¯ä»¥å¹«åŠ©æˆ‘å€‘ç†æ¸…è³‡æ–™è„ˆçµ¡

  - ç‰¹å¾µæ•¸å¤ªé¾â¼¤çš„æƒ…æ³ä¸‹ï¼Œéç›£ç£å¼å­¸ç¿’å¯ä»¥å¹«åŠ©æ¦‚å¿µæŠ½è±¡åŒ–ï¼Œâ½¤æ›´ç°¡æ½”çš„ç‰¹å¾µæè¿°è³‡æ–™

    - å®¢â¼¾åˆ†ç¾¤

      åœ¨è³‡æ–™æ²’æœ‰ä»»ä½•æ¨™è¨˜ï¼Œæˆ–æ˜¯å•é¡Œé‚„æ²’å®šç¾©æ¸…æ¥šå‰ï¼Œå¯â½¤åˆ†ç¾¤çš„â½…å¼å¹«åŠ©ç†æ¸…è³‡æ–™ç‰¹æ€§ã€‚

    - ç‰¹å¾µæŠ½è±¡åŒ–

      ç‰¹å¾µæ•¸å¤ªå¤šé›£æ–¼ç†è§£åŠå‘ˆç¾çš„æƒ…æ³ä¸‹ï¼Œè—‰ç”±æŠ½è±¡åŒ–çš„æŠ€è¡“å¹«åŠ©é™ä½è³‡æ–™ç¶­åº¦ï¼ŒåŒæ™‚ä¸å¤±å»åŸæœ‰çš„è³‡è¨Šï¼Œçµ„åˆæˆæ–°çš„ç‰¹å¾µã€‚

    - è³¼ç‰©ç±ƒåˆ†æ

      è³‡æ–™æ¢å‹˜çš„ç¶“å…¸æ¡ˆä¾‹ï¼Œé©â½¤æ–¼ç·šä¸‹æˆ–ç·šä¸Šé›¶å”®çš„å•†å“çµ„åˆæ¨è–¦ã€‚

    - éçµæ§‹åŒ–è³‡æ–™åˆ†æ

      éçµæ§‹åŒ–è³‡æ–™å¦‚â½‚å­—ã€å½±åƒç­‰ï¼Œå¯ä»¥è—‰ç”±â¼€äº›éç›£ç£å¼å­¸ç¿’çš„æŠ€è¡“ï¼Œå¹«åŠ©å‘ˆç¾åŠæè¿°è³‡æ–™ã€‚

- æ©Ÿå™¨å­¸ç¿’æ¨¡å‹æœ‰å¾ˆå¤šï¼Œç•¶è¨“ç·´æˆæœ¬å¾ˆå°çš„æ™‚å€™ï¼Œå»ºè­°å‡ä½œå˜—è©¦ï¼Œä¸åƒ…å¯ä»¥æ¸¬è©¦æ•ˆæœï¼Œé‚„å¯ä»¥å­¸ç¿’å„ç¨®æ¨¡å‹çš„ä½¿ç”¨æŠ€å·§ã€‚
- å¹¸é‹çš„æ˜¯ï¼Œé€™äº›æ¨¡å‹éƒ½å·²ç¶“æœ‰ç¾æˆçš„å·¥å…·ï¼ˆå¦‚scikit-learnã€XGBoostã€LightGBMç­‰ï¼‰å¯ä»¥ä½¿ç”¨ï¼Œä¸ç”¨è‡ªå·±é‡è¤‡é€ è¼ªå­ã€‚
- ä½†æ˜¯æˆ‘å€‘æ‡‰è©²è¦çŸ¥é“å„å€‹æ¨¡å‹çš„åŸç†ï¼Œé€™æ¨£åœ¨èª¿åƒçš„æ™‚å€™æ‰æœƒéŠåˆƒæœ‰é¤˜ã€‚

- èª¿åƒ


- ä¹‹å‰æ¥è§¸åˆ°çš„æ‰€æœ‰æ¨¡å‹éƒ½æœ‰è¶…åƒæ•¸éœ€è¦è¨­ç½®
  - LASSOï¼ŒRidge: Î± çš„â¼¤â¼©
  - æ±ºç­–æ¨¹ï¼šæ¨¹çš„æ·±åº¦ã€ç¯€é»æœ€â¼©æ¨£æœ¬æ•¸
  - éš¨æ©Ÿæ£®æ—ï¼šæ¨¹çš„æ•¸é‡
- é€™äº›è¶…åƒæ•¸éƒ½æœƒå½±éŸ¿æ¨¡å‹è¨“ç·´çš„çµæœï¼Œå»ºè­°å…ˆä½¿â½¤é è¨­å€¼ï¼Œå†æ…¢æ…¢é€²â¾èª¿æ•´
- è¶…åƒæ•¸æœƒå½±éŸ¿çµæœï¼Œä½†æå‡çš„æ•ˆæœæœ‰é™ï¼Œè³‡æ–™æ¸…ç†èˆ‡ç‰¹å¾µâ¼¯ç¨‹æ‰èƒ½æœ€æœ‰æ•ˆçš„æå‡æº–ç¢ºç‡ï¼Œèª¿æ•´åƒæ•¸åªæ˜¯â¼€å€‹åŠ åˆ†çš„â¼¯å…·ã€‚



## Supervised learning

### Linear Regression

Linear Regression models describe the relationship between a set of variables and a real value outcome. For example, input of the mileage, engine size, and the number of cylinders of a car can be used to predict the price of the car using a regression model. Regression differs from classification in how it's error is defined. In classification, the predicted class is not the class in which the model is making an error. In regression, for example, if the actual price of a car is 5000 and we have two models which predict the price to be 4500 and 6000, then we would prefer the former because it is less erroneous than 6,000. We need to define a loss function for the model, such as Least Squares or Absolute Value. The drawback of regression is that it assumes that a single straight line is appropriate as a summary of the data.

- ä¾æ“šè§£é‡‹è®Šæ•¸çš„æ•¸é‡å¯ä»¥å†ç´°åˆ†æˆ Simple Linear Regression å’Œ Multiple Linear Regressionï¼Œç•¶åªæœ‰ä¸€å€‹è§£é‡‹è®Šæ•¸æ™‚ç‚ºSimpleï¼Œæœ‰å…©å€‹ä»¥ä¸Šæ˜¯å‰‡æ˜¯ Multiple
- ç·šæ€§å›æ­¸é€šéä½¿ç”¨æœ€ä½³çš„æ“¬åˆç›´ç·šï¼ˆåˆè¢«ç¨±ç‚ºå›æ­¸ç·šï¼‰ï¼Œå»ºç«‹å› è®Šæ•¸ Y å’Œä¸€å€‹æˆ–å¤šå€‹å¼•æ•¸ X ä¹‹é–“çš„é—œä¿‚ã€‚
- å®ƒçš„é‹ç®—å¼ç‚ºï¼š$Y = a + bX + e$  ï¼Œå…¶ä¸­ $a$ ç‚ºç›´ç·šæˆªè·ï¼Œ$b$ ç‚ºç›´ç·šæ–œç‡ï¼Œ$e$ ç‚ºèª¤å·®é …ã€‚å¦‚æœçµ¦å‡ºäº†è‡ªè®Šé‡ $X$ ï¼Œå°±èƒ½é€šéé€™å€‹ç·šæ€§å›æ­¸è¡¨é”å¼è¨ˆç®—å‡ºé æ¸¬å€¼ï¼Œå³å› è®Šæ•¸ $Y$ã€‚
- é€éæœ€å°å¹³æ–¹æ³•(Ordinal Least Square, OLS)æœŸæœ›å°‡$\sum(Y-\hat{Y})^2$æœ€å°åŒ–

$$
  b = \frac{Cov_{XY}}{S_x^2} = \frac{\sum^n_{i=1}(x_i - \bar{x})(y_i - \bar{y})}{\sum^n_{i=1}(x_i - \bar{x})^2}
$$

$$
  a = \bar{Y} - b\bar{X}
$$

- è¨“ç·´é€Ÿåº¦éå¸¸å¿«ï¼Œä½†é ˆæ³¨æ„è³‡æ–™å…±ç·šæ€§ã€è³‡æ–™æ¨™æº–åŒ–ç­‰é™åˆ¶ã€‚é€šå¸¸å¯ä½œç‚º baseline æ¨¡å‹ä½œç‚ºåƒè€ƒé»

- Assumptions of a Linear 

  - Linearityï¼šè³‡æ–™å‘ˆç·šæ€§é—œä¿‚
  - HomoScedasticityï¼šè³‡æ–™è¦æœ‰ç›¸åŒçš„æ–¹å·®
  - Multivariate normalityï¼šè³‡æ–™è¦å‘ˆç¾å¤šå…ƒæ­£æ…‹åˆ†ä½ˆ
  - Independence of errorsï¼šå„å€‹ç¶­åº¦ä¸Šçš„èª¤å·®ç›¸äº’ç¨ç«‹
  - Lack of Multicollinearityï¼šæ²’æœ‰ä¸€å€‹è‡ªè®Šæ•¸å’Œå¦å¤–çš„è‡ªè®Šæ•¸å­˜ç·šä¸Šæ€§é—œä¿‚

- è¦ç‰¹åˆ¥æ³¨æ„çš„æ˜¯Coefficientsä»£è¡¨çš„æ˜¯åœ¨**å›ºå®š**å…¶ä»–è®Šæ•¸å¾Œï¼Œæ¯å–®ä½è®Šæ•¸å°ä¾è®Šæ•¸çš„å½±éŸ¿ç¨‹åº¦ï¼Œåªæœ‰åœ¨è®Šæ•¸åŒå–®ä½åŒç´šè·æ™‚ï¼Œæ‰èƒ½æ¯”è¼ƒå“ªä¸€å€‹å°ä¾è®Šæ•¸é€ æˆçš„é‡è¼ƒå¤§ã€‚

- Scikit-learn ä¸­çš„ linear regression

  ```python
  from sklearn.linear_model import LinearRegression
  reg = LinearRegression()
  reg.fit(X, y)
  y_pred = reg.predict(X_test)
  ```

- é›–ç„¶ç·šæ€§æ¨¡å‹ç›¸è¼ƒå…¶ä»–æ¨¡å‹ä¸å®¹æ˜“æœ‰overfitinngçš„å•é¡Œï¼Œä½†ç•¶åƒæ•¸ä¸€å¤šæ™‚ä»ç„¶æœƒæœ‰overfitçš„å•é¡Œ

- Backward Elimination in Python

  ```python
  import statsmodels.formula.api as sm
  def backwardElimination(x, sl):
      numVars = len(x[0])
      for i in range(0, numVars):
          regressor_OLS = sm.OLS(y, x).fit()
          maxVar = max(regressor_OLS.pvalues).astype(float)
          if maxVar > sl:
              for j in range(0, numVars - i):
                  if (regressor_OLS.pvalues[j].astype(float) == maxVar):
                      x = np.delete(x, j, 1)
      regressor_OLS.summary()
      return x
  
  
  SL = 0.05
  X_opt = X[:, [0, 1, 2, 3, 4, 5]]
  X_Modeled = backwardElimination(X_opt, SL)
  ```
  
- Ref

  - [Linear Regression With Gradient Descent From Scratch.ipynb](https://github.com/TLYu0419/DataScience/blob/master/Machine_Learning/Linear Regression With Gradient Descent From Scratch.ipynb)
  - [R Example](http://r-statistics.co/Linear-Regression.html)

### Polynomial Regression

Polynomial Regression is the same concept as linear regression except that it uses a curved line instead of a straight line (which is used by linear regression). Polynomial regression learns more parameters to draw a non-linear regression line. It is beneficial for data that cannot be summarized by a straight line.The number of parameters (also called degrees) has to be determined. A higher degree model is more complex but can over fit the data.

- [Python Example](https://www.geeksforgeeks.org/python-implementation-of-polynomial-regression/)

### Poisson Regression

Poisson Regression assumes that the predicted variables follows a Poisson Distribution. Hence, the values of the predicted variable are positive integers. The Poisson distribution assumes that the count of larger numbers is rare and smaller values are more frequent. Poisson regression is used for modelling rare occurrence events and count variables, such as incidents of cancer in a demographic or the number of times power shuts down at NASA.

- [R Example](https://www.r-bloggers.com/generalized-linear-models-poisson-regression/)

### Ordinary Least Squares (OLS) Regression

Least Squares is a special type of Regression model which uses squares of the error terms as a measure of how accurate the model is. Least Squares Regression uses a squared loss. It computes the difference between the predicted and the actual value, squares it, and repeats this step for all data points. A sum of the all the errors is computed. This sum is the overall representation of how accurate the model is.Next, the parameters of the model are tweaked such that this squared error is minimized so that there can be no improvement. For this model, it is appropriate to preprocess the data to remove any outliers, and only one of a set of variables which are highly correlated to each other should be used.

- [R Example](https://www.r-bloggers.com/ordinary-least-squares-ols-linear-regression-in-r/)

### Ordinal Regression

Also called ranking learning, ordinal regression takes a set of ordinal values as input. Ordinal variables are on an arbitrary scale and the useful information is their relative ordering. For example, ordinal regression can be used to predict the rating of a musical on a scale of 1 to 5 using ratings provided by surveys. Ordinal Regression is frequently used in social science because surveys ask participants to rank an entity on a scale.

- [R Example](https://www.r-bloggers.com/how-to-perform-ordinal-logistic-regression-in-r/)

### Support Vector Regression

Support Vector Regression works on the same principle as Support Vector Machine except the output is a number instead of a class. It is computationally cheaper, with a complexity of O^2*K where K is the number of support vectors, than logistic regression.



- [Support Vector Regression](https://core.ac.uk/download/pdf/81523322.pdf)

- [Rç­†è¨˜ â€“ (14)Support Vector Machine/Regression(æ”¯æŒå‘é‡æ©ŸSVM)](https://rpubs.com/skydome20/R-Note14-SVM-SVR)

- [Support Vector Machines Tutorial â€“ Learn to implement SVM in Python](https://data-flair.training/blogs/svm-support-vector-machine-tutorial/)

- Find Maximum Margin

- ç‚ºä»€éº¼è¦æŠŠè³‡æ–™æŠ•å½±åˆ°æ›´é«˜ç¶­åº¦çš„å¹³é¢(kernel)?

  - å› ç‚ºè¤‡é›œçš„è³‡æ–™æ²’è¾¦æ³•ç”¨ç·šæ€§ä¾†åˆ†å‰²å‡ºä¹¾æ·¨çš„è³‡æ–™

- The Kernel Trick

  - sigmaè¶Šå¤§ï¼Œæœ‰è¶Šå¤šè³‡æ–™é»æœƒæå‡
  - æœ‰é€™éº¼å¤šç¨®é¡çš„kernelï¼Œä½ è¦ç”¨ä»€éº¼kernelå‡½æ•¸åœ¨ä½ çš„è³‡æ–™ä¸Š?ä½ æŒ‘åˆ°kerneläº†ï¼Œkernelåƒæ•¸æ€éº¼èª¿æ•´ï¼Ÿ

- Types of Kernel Functions

  - linear Kernel
    - å„ªé»æ˜¯æ¨¡å‹è¼ƒç‚ºç°¡å–®ï¼Œä¹Ÿå› æ­¤æ¯”è¼ƒå®‰å…¨ï¼Œä¸å®¹æ˜“ overfitï¼›å¯ä»¥ç®—å‡ºç¢ºåˆ‡çš„ W åŠ Support Vectorsï¼Œè§£é‡‹æ€§è¼ƒå¥½ã€‚
    - ç¼ºé»å°±æ˜¯ï¼Œé™åˆ¶æœƒè¼ƒå¤šï¼Œå¦‚æœè³‡æ–™é»éç·šæ€§å¯åˆ†å°±æ²’ç”¨ã€‚
  - Gaussian RBG Kernel
    - æœ€å¾Œæ˜¯ Gaussian Kernelï¼Œå„ªé»å°±æ˜¯ç„¡é™å¤šç¶­çš„è½‰æ›ï¼Œåˆ†é¡èƒ½åŠ›ç•¶ç„¶æ›´å¥½ï¼Œè€Œä¸”éœ€è¦é¸æ“‡çš„åƒæ•¸çš„è¼ƒå°‘ã€‚ä½†ç¼ºé»å°±æ˜¯ç„¡æ³•è¨ˆç®—å‡ºç¢ºåˆ‡çš„ w åŠ support vectorsï¼Œé æ¸¬æ™‚éƒ½è¦é€é kernel function ä¾†è¨ˆç®—ï¼Œä¹Ÿå› æ­¤æ¯”è¼ƒæ²’æœ‰è§£é‡‹æ€§ï¼Œè€Œä¸”ä¹Ÿæ˜¯æœƒç™¼ç”Ÿ overfitã€‚æ¯”èµ· Polynomail SVMï¼ŒGaussian SVM æ¯”è¼ƒå¸¸ç”¨ã€‚
  - Sigmoid Kernel
  - Polynomial Kernel
    - ç”±æ–¼å¯ä»¥é€²è¡Œ Q æ¬¡è½‰æ›ï¼Œåˆ†é¡èƒ½åŠ›æœƒæ¯” Linear Kernel å¥½ã€‚ç¼ºé»å°±æ˜¯é«˜æ¬¡è½‰æ›å¯èƒ½æœƒæœ‰ä¸€äº›æ•¸å­—å•é¡Œç”¢ç”Ÿï¼Œé€ æˆè¨ˆç®—çµæœæ€ªç•°ã€‚ç„¶å¾Œå¤ªå¤šåƒæ•¸è¦é¸ï¼Œæ¯”è¼ƒé›£ä½¿ç”¨ã€‚

- Python Code

  ```python
  # SVR
  from sklearn.svm import SVR
  regressor = SVR(kernel = 'rbf')
  regressor.fit(X, y)
  ```

  

- Ref

  - [æ©Ÿå™¨å­¸ç¿’: Kernel å‡½æ•¸](https://medium.com/@chih.sheng.huang821/æ©Ÿå™¨å­¸ç¿’-kernel-å‡½æ•¸-47c94095171)
  - [æ—è»’ç”°æ•™æˆæ©Ÿå™¨å­¸ç¿’æŠ€æ³• Machine Learning Techniques ç¬¬ 3 è¬›å­¸ç¿’ç­†è¨˜](https://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-fa-machine-learning-techniques-di-3-jiang-xue-xi-bi-ji/)
  - [è³‡æ–™åˆ†æ&æ©Ÿå™¨å­¸ç¿’ ç¬¬3.4è¬›ï¼šæ”¯æ´å‘é‡æ©Ÿ(Support Vector Machine)ä»‹ç´¹ | by Yeh James | JamesLearningNote | Medium](https://medium.com/jameslearningnote/è³‡æ–™åˆ†æ-æ©Ÿå™¨å­¸ç¿’-ç¬¬3-4è¬›-æ”¯æ´å‘é‡æ©Ÿ-support-vector-machine-ä»‹ç´¹-9c6c6925856b)
  - [7.2 æ”¯æŒå‘é‡æœº | ç¼–ç¨‹ä¹‹æ³•ï¼šé¢è¯•å’Œç®—æ³•å¿ƒå¾— (gitbooks.io)](https://wizardforcel.gitbooks.io/the-art-of-programming-by-july/content/07.02.svm.html)

  





- [R Example](https://www.kdnuggets.com/2017/03/building-regression-models-support-vector-regression.html)

### Gradient Descent Regression

*Gradient Descent Regression uses gradient descent to optimize the model (as opposed to, for example, Ordinary Least Squares). Gradient Descent is an algorithm to reduce the cost function by finding the gradient of the cost at every iteration of the algorithm using the entire dataset.

- [R Example](https://www.r-bloggers.com/linear-regression-by-gradient-descent/)

### Stepwise Regression

Stepwise regression solves the problem of determining the variables, from the available variables, that should be used in a regression model. It uses F-tests and t-tests to determine the importance of a variable. R-squared, which explains the ratio of the predicted variable explained by a variable, is also used.Stepwise regression can either incrementally add and/or remove a variable from the entire dataset to the model such that the cost function is reduced.

- [R Example](http://r-statistics.co/Model-Selection-in-R.html)

### Lasso Regression 

- Least absoulute selection and shrinkage operator

Often times, the data we need to model demands a more complex representation which is not easy to characterize with the simple OLS regression model. Hence, to produce a more accurate representation of the data, we can add a penalty term to the OLS equation. This method is also known as L1 regularization.The penalty term imposes a constraint on the total sum of the absolute values of the model parameters. The goal of the model is to minimize the error represented in Fig. 6 which is the same as minimizing the SSE with an additional constraint. If your linear model contains many predictor variables or if these variables are correlated, the traditional OLS parameter estimates have large variance, thus making the model unreliable. This leads to an over-fitted model. A penalty term causes the regression coefficients for these unimportant variables to shrink towards zero. This process allows the model to identify the variables strongly associated with the output variable, thereby reducing the variance.Lambda, a tuning parameter, is used to control the strength of the model penalty in Lasso Regression. As lambda increases, more coefficients are reduced to zero. This feature selection process can help alleviate multi-collinearity because Lasso tends to select only one of the correlated features and shrink the other to zero. Lasso is generally used when we have a greater number of features, because it automatically performs feature selection.

![Fig 6: Lasso Regression Loss Function](https://datasciencedojo.com/wp-content/uploads/lasso.png)

- å…ˆå‰å­¸ç¿’åˆ°çš„å›æ­¸æ¨¡å‹ï¼Œæˆ‘å€‘åªæœ‰æåˆ°æå¤±å‡½æ•¸æœƒâ½¤ MSE æˆ– MAEï¼Œç‚ºäº†é¿å… Over-fittingï¼Œæˆ‘å€‘å¯ä»¥æŠŠæ­£å‰‡åŒ–åŠ å…¥â½¬æ¨™å‡½æ•¸ä¸­ï¼Œæ­¤æ™‚â½¬æ¨™å‡½æ•¸ = æå¤±å‡½æ•¸ + æ­£å‰‡åŒ–

- LASSO ç‚º Linear Regression åŠ ä¸Š L1

- æ­£å‰‡åŒ–å¯ä»¥æ‡²ç½°æ¨¡å‹çš„è¤‡é›œåº¦ï¼Œç•¶æ¨¡å‹è¶Šè¤‡é›œæ™‚å…¶å€¼å°±æœƒè¶Šâ¼¤

  - æ­£å‰‡åŒ–å‡½æ•¸

    - L1ï¼š $\alpha \sum|weights|$

      å‘é‡ä¸­å„å…ƒç´ çµ•å°å€¼ä¹‹å’Œã€‚åˆå«åšç¨€ç–è¦å‰‡é‹ç®—å…ƒï¼ˆLasso regularizationï¼‰ã€‚é—œéµåœ¨æ–¼èƒ½å¤ å¯¦ç¾ç‰¹å¾µçš„è‡ªå‹•é¸æ“‡ï¼Œåƒæ•¸ç¨€ç–å¯ä»¥é¿å…éå¿…è¦çš„ç‰¹å¾µå¼•å…¥çš„é›œè¨Š

  - L1 æœƒè¶¨å‘æ–¼ç”¢ç”Ÿå°‘é‡çš„ç‰¹å¾µï¼Œè€Œå…¶ä»–çš„ç‰¹å¾µéƒ½æ˜¯ 0(é¸è¼ƒå°‘åƒæ•¸ï¼Œé¸å‡ºçš„åƒæ•¸å½±éŸ¿åŠ›å¤§)

- Sklearn ä½¿â½¤ Lasso Regression

  ```python
  from sklearn.linear_model import Lasso
  reg = Lasso(alpha=0.1)
  reg.fit(X, y)
  print(reg.coef_) # å°å‡ºè¨“ç·´å¾Œçš„æ¨¡å‹åƒæ•¸
  ```

- 

### Ridge Regression (L2)

Ridge regression uses ridge regularization to prepare a regression model. Ridge regularization adds the square of the coefficients to the cost function. It is effective if there are multiple coefficients with large values. It makes the values of the coefficients of the indiscriminate variables small.

- Ridge ç‚º Linear Regression åŠ ä¸Š L2

- æ­£å‰‡åŒ–å‡½æ•¸

  - L2ï¼š $\alpha \sum(weights)^2$

    L2 æ­£å‰‡åŒ–ã€‚ä½¿å¾—æ¯å€‹å…ƒç´ éƒ½ç›¡å¯èƒ½çš„å°ï¼Œä½†æ˜¯éƒ½ä¸ç‚ºé›¶ã€‚åœ¨å›æ­¸è£¡é¢ï¼Œæœ‰äººæŠŠä»–çš„å›æ­¸å«åšå¶ºå›æ­¸ï¼ˆRidge Regressionï¼‰ï¼Œä¹Ÿæœ‰äººå«ä»– â€œæ¬Šå€¼è¡°æ¸›â€ï¼ˆweight decayï¼‰ 

  - L1 å’Œ L2 é€™å…©ç¨®éƒ½æ˜¯å¸Œæœ›æ¨¡å‹çš„åƒæ•¸æ•¸å€¼ä¸è¦å¤ªâ¼¤ï¼ŒåŸå› æ˜¯åƒæ•¸çš„æ•¸å€¼è®Šâ¼©ï¼Œå™ªâ¾³å°æœ€çµ‚è¼¸å‡ºçš„çµæœå½±éŸ¿è¶Šâ¼©ï¼Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½â¼’ï¼Œä½†ä¹Ÿè®“æ¨¡å‹çš„æ“¬åˆèƒ½â¼’ä¸‹é™ã€‚

- Sklearn ä½¿â½¤ Ridge Regression

  ```python
  from sklearn.linear_model import Ridge
  reg = Ridge (alpha=0.1)
  reg.fit(X, y)
  print(reg.coef_) # å°å‡ºè¨“ç·´å¾Œçš„æ¨¡å‹åƒæ•¸
  ```





### LASSO, Ridge Regression

- å›æ­¸æ¨¡å‹èˆ‡æ­£è¦åŒ–

  - 

- æ­£å‰‡åŒ–å‡½æ•¸

  - â½¤ä¾†è¡¡é‡æ¨¡å‹çš„è¤‡é›œåº¦

  - è©²æ€éº¼è¡¡é‡ï¼Ÿæœ‰ L1 èˆ‡ L2 å…©ç¨®å‡½æ•¸

    - L1ï¼š $\alpha \sum|weights|$

      å‘é‡ä¸­å„å…ƒç´ çµ•å°å€¼ä¹‹å’Œã€‚åˆå«åšç¨€ç–è¦å‰‡é‹ç®—å…ƒï¼ˆLasso regularizationï¼‰ã€‚é—œéµåœ¨æ–¼èƒ½å¤ å¯¦ç¾ç‰¹å¾µçš„è‡ªå‹•é¸æ“‡ï¼Œåƒæ•¸ç¨€ç–å¯ä»¥é¿å…éå¿…è¦çš„ç‰¹å¾µå¼•å…¥çš„é›œè¨Š

    - L2ï¼š $\alpha \sum(weights)^2$

      L2 æ­£å‰‡åŒ–ã€‚ä½¿å¾—æ¯å€‹å…ƒç´ éƒ½ç›¡å¯èƒ½çš„å°ï¼Œä½†æ˜¯éƒ½ä¸ç‚ºé›¶ã€‚åœ¨å›æ­¸è£¡é¢ï¼Œæœ‰äººæŠŠä»–çš„å›æ­¸å«åšå¶ºå›æ­¸ï¼ˆRidge Regressionï¼‰ï¼Œä¹Ÿæœ‰äººå«ä»– â€œæ¬Šå€¼è¡°æ¸›â€ï¼ˆweight decayï¼‰ 

  - L2 æœƒé¸æ“‡æ›´å¤šçš„ç‰¹å¾µï¼Œé€™äº›ç‰¹å¾µéƒ½æœƒæ¥è¿‘æ–¼ 0(é¸è¼ƒå¤šåƒæ•¸ï¼Œé¸å‡ºçš„åƒæ•¸å½±éŸ¿åŠ›å°)
  
- [L1èŒƒæ•°ä¸L2èŒƒæ•°çš„åŒºåˆ« - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/28023308)

### Elastic Net Regression

Elastic Net generalizes the idea of both Ridge and Lasso regression since it combines the penalties from both L1 ( Lasso) and L2 (Ridge) regularization. Elastic Net aims at minimizing the loss function represented in Fig. 7. ğª is the tuning parameter which can be changed to implement both Ridge and Lasso regression alternatively or simultaneously to optimize the elastic net. If you plug in ğª = 0, the penalty function corresponds to ridge and ğª = 1 corresponds to Lasso regularization.In the case of correlated independent variables in a dataset, the Elastic Net will group these variables together. Now if any one of the variable of this group is strongly associated with the dependent variable, then the entire group will be a part of the model, because selecting only one of those variables (like what we did in Lasso) might result in losing some useful information, leading to a poor model performance. Hence, elastic net produces grouping in case of multi-collinearity.The size of the respective penalty terms Lambda and alpha can be tuned via cross-validation to find the model's best fit.

![Fig. 7: Elastic Net Loss Function](https://datasciencedojo.com/wp-content/uploads/elastic-net-loss-function.png)[R Example](https://daviddalpiaz.github.io/r4sl/elastic-net.html)

### Bayesian Linear Regression

In the Bayesian world, linear regression is formulated using probability distributions rather than point estimates. The dependent variable, Y, is not estimated as a single value, but is assumed to be drawn from a probability distribution. Y is generated from a normal distribution with a mean and variance. Bayesian Linear Regression aims to find the posterior distribution for the model parameters rather than determining a single "optimal" value for the model. In contrast to OLS, there is a posterior distribution for the model parameters that is proportional to the likelihood of the data multiplied by the prior probability of the parameters. One of the advantages of this approach is that if we have domain knowledge (Priors), or a an idea about the model parameters, we can include them in our model.The major  advantage of Bayesian processing is that you can incorporate the use of previous or assumed knowledge and update the current state of beliefs. You can incorporate prior information about a parameter and form a prior distribution for future analysis. One of the shortcomings of Bayesian analysis is that it does not tell you how to select a prior. There is no single correct way to choose a prior. This approach requires skills to translate subjective prior beliefs into a mathematically formulated prior. Any misunderstanding can generate misleading results.

- [R Example](https://www.r-bloggers.com/bayesian-linear-regression-analysis-without-tears-r/)

### Least-Angled Regression (LARS)

Least-Angled Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. This type of regression is useful when we have a high dimensional data. It's very similar to stepwise regression which finds out the best set of independent variables.

- [Python Example](https://plot.ly/scikit-learn/plot-lasso-lars/)

### Neural Network Regression

As the name suggests, neural networks are inspired by the brain. They form a network of interconnected nodes arranged in layers that make up a model. Neural networks are used to approzimate functions when the input data is too large for standard machine learning approaches.Fig. 8 represents the basic structure of a feed forward neural network. The input layer has number of nodes equal to a dimension of input data features. Each hidden layer consists of an arbitrary number of nodes. The number of the layers depends on the architecture and the scope of the problem.  And output layer consists of one node only if it is a regression problem. A neuron holds a number which represents the value of the corresponding feature of the input data, also known as activation. For each node of a single layer, input from each node of the previous layer is mixed in different proportions, and then passed into each node of the subsequent layer in a feed forward neural network, and so on until the signal reaches the final layer where a regression decision is made. All these are matrix operations.The questions then comes down to the network parameters which needs to be tuned such that it minimizes the loss between the predicted outcome and the true value. In large models, there can be millions of parameters to optimize. Gradient descent is used as the optimization function to adjust the weights/parameters in order to minimize the total error in the network. The gradient describes the relationship between the networkâ€™s error and a single weight, that is, how does the error vary as the weight is adjusted. As the training process continues, the network adjusts many weights/parameters such they can map the input data to produce an output which is as close as possible to the original output.Neural networks can run regression if given any prior information to predict a future event. For instance, you can predict heart attacks based on the vital stats data of a person. Moreover, you can also predict the likelihood that a customer will leave or not, based on web activity and metadata.

![Fig. 8: Neural Network Regression](https://datasciencedojo.com/wp-content/uploads/neural-network-regression.png)

- [Python Example](https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/)

- [Azure ML](https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/neural-network-regression)

### Locally Estimated Scatterplot Smoothing (LOESS)

LOESS is a highly flexible non-parametric regression technique. It makes as little assumptions as possible and tries to capture a general pattern. It is used to make an assessment of the relationship of two variables especially in large datasets.

- [R Example](http://r-statistics.co/Loess-Regression-With-R.html)

### Multivariate Adaptive Regression Splines (MARS)

MARS is a non-parametric model that fits a regression line in two phases. The first phase is a forward pass in which MARS starts with only an intercept and incrementally adds basis functions to it to improve the model. The brute force methodology of the first pass makes an overfit model which is pruned in the backward pass. In the backward pass any term from the model can be deleted.

- [R Example](http://uc-r.github.io/mars)

### Locally Weighted Regression (LWL)

This is a non-parametric model which makes local functions. It uses a set of weights, each for a subset of the data to make predictions on it. The use of higher weights for neighboring data points and lower weights for far away data, instead of using global patterns, makes it an accurate and flexible measure.

- [R Example](https://www.kdnuggets.com/2017/03/building-regression-models-support-vector-regression.html)

### Quantile Regression

Generally regression models predict the mean but this algorithm predicts the distribution of the data. It can be used to predict the distribution of prices given a certain input, for example what would be the 25th and 75th percentile of the distribution of a car price given some attributes.

- [Python Example](https://scikit-garden.github.io/examples/QuantileRegressionForests/)

### Principal Component Regression (PCR)

Principal Component Regression is an extension of Principal Component Analysis and Multiple Linear Regression. PCR models a target variable when there are a large number of predictor variables, and those predictors are highly correlated or even collinear. This method constructs new predictor variables, known as components, as linear combinations of the original predictor variables. PCR creates components to explain the observed variability in the predictor variables, without considering the target variable at all.In the first step, the principal components are calculated. The scores of the most important principal components are used as the basis for the multiple linear regression with the target variable. The most important point in PCR is the proper selection of the eigenvectors to be included. A plot of the eigenvalues usually indicates to the "best" number of eigenvectors.The benefit of PCR over multiple linear regression is that the noise remains in the residuals, since the eigenvectors with low eigenvalues represent only parts of the data with low variance. Moreover, the regression coefficients are more stable. This is because the eigenvectors are orthogonal to each other.

- [R Example](https://poissonisfish.wordpress.com/2017/01/23/principal-component-analysis-in-r/)

### Partial Least Squares Regression

Partial least squares regression (PLS regression) is developed from principal components regression. It works in a similar fashion as it finds a linear regression model by projecting the predicted variables and the predictor variables to a new space instead of finding hyperplanes of maximum variance between the target and predictor variables. While, PCR creates components to explain the observed variability in the predictors, without considering the target variable at all. PLS Regression, on the other hand, does take the response variable into account, and often leads to models that are able to fit the target variable with fewer components. However, it depends on the context of the model if using PLS Regression over PCR would offer a more parsimonious model.

- [R Example](https://rpubs.com/omicsdata/pls)



### Decision Tree

A decision tree classification algorithm uses a training dataset to stratify or segment the predictor space into multiple regions. Each such region has only a subset of the training dataset. To predict the outcome for a given (test) observation, first, we determine which of these regions it belongs to. Once its region is identified, its outcome class is predicted as being the same as the mode (say, â€˜most commonâ€™) of the outcome classes of all the training observations that are included in that region. The rules used to stratify the predictor space can be graphically described in a tree-like flow-chart, hence the name of the algorithm. The only difference being that these decision trees are drawn upside down.Decision tree classification models can easily handle qualitative predictors without the need to create dummy variables. Missing values are not a problem either. Interestingly, decision tree algorithms are used for regression models as well. The same library that you would use to build a classification model, can also be used to build a regression model after changing some of the parameters.          Although the decision tree-based classification models are very easy to interpret, they are not robust.  One major problem with decision trees is their high variance. One small change in the training dataset can give an entirely different decision trees model. Another issue is that their predictive accuracy is generally lower than some other classification models, such as â€œRandom Forestâ€ models (for which decision trees are the building blocks).

![Fig. 1: Decision Tree Example](https://datasciencedojo.com/wp-content/uploads/decision-tree-example.png)

- å¾è¨“ç·´è³‡æ–™ä¸­æ‰¾å‡ºè¦å‰‡ï¼Œè®“æ¯â¼€æ¬¡æ±ºç­–èƒ½ä½¿è¨Šæ¯å¢ç›Š(Information Gain) æœ€â¼¤åŒ–

  - è¨Šæ¯å¢ç›Šè¶Šâ¼¤ä»£è¡¨åˆ‡åˆ†å¾Œçš„å…©ç¾¤è³‡æ–™ï¼Œç¾¤å…§ç›¸ä¼¼ç¨‹åº¦è¶Šâ¾¼

    - è¨Šæ¯å¢ç›Š (Information Gain): æ±ºç­–æ¨¹æ¨¡å‹æœƒâ½¤ features åˆ‡åˆ†è³‡æ–™ï¼Œè©²é¸â½¤å“ªå€‹ feature ä¾†åˆ‡åˆ†å‰‡æ˜¯ç”±è¨Šæ¯å¢ç›Šçš„â¼¤â¼©æ±ºå®šçš„ã€‚å¸Œæœ›åˆ‡åˆ†å¾Œçš„è³‡æ–™ç›¸ä¼¼ç¨‹åº¦å¾ˆâ¾¼ï¼Œé€šå¸¸ä½¿â½¤å‰å°¼ä¿‚æ•¸ä¾†è¡¡é‡ç›¸ä¼¼ç¨‹åº¦ã€‚

  - è¡¡é‡è³‡æ–™ç›¸ä¼¼: Gini vs. Entropy

    - å…©è€…éƒ½å¯ä»¥è¡¨ç¤ºæ•¸æ“šçš„ä¸ç¢ºå®šæ€§ï¼Œä¸ç´”åº¦

      - Gini æŒ‡æ•¸çš„è¨ˆç®—ä¸éœ€è¦å°æ•¸é‹ç®—ï¼Œæ›´åŠ é«˜æ•ˆï¼›

      - Gini æŒ‡æ•°æ›´åå‘æ–¼é€£çºŒå±æ€§ï¼ŒEntropy æ›´åå‘æ–¼é›¢æ•£å±¬æ€§ã€‚

        $Gini = 1 - \sum_j p_j^2$

        $Entropy = - \sum_jp_j log_2 p_j$

  - æ±ºç­–æ¨¹çš„ç‰¹å¾µé‡è¦æ€§ (Feature importance)

    - æˆ‘å€‘å¯ä»¥å¾æ§‹å»ºæ¨¹çš„éç¨‹ä¸­ï¼Œé€é feature è¢«â½¤ä¾†åˆ‡åˆ†çš„æ¬¡æ•¸ï¼Œä¾†å¾—çŸ¥å“ªäº›features æ˜¯ç›¸å°æœ‰â½¤çš„
    - æ‰€æœ‰ feature importance çš„ç¸½å’Œç‚º 1
    - å¯¦å‹™ä¸Šå¯ä»¥ä½¿â½¤ feature importance ä¾†äº†è§£æ¨¡å‹å¦‚ä½•é€²â¾åˆ†é¡

  - ä½¿â½¤ Sklearn å»ºç«‹æ±ºç­–æ¨¹æ¨¡å‹

    ```python
    from sklearn.tree import DecisionTreeRegressor
    regressor = DecisionTreeRegressor(random_state = 0)
    regressor.fit(X, y)
    
    from sklearn.tree import DecisionTreeClassifier
      classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
      classifier.fit(X_train, y_train)
    ```

    - Criterion: è¡¡é‡è³‡æ–™ç›¸ä¼¼ç¨‹åº¦çš„ metric
      - clfï¼šginiï¼Œentropy
    - Max_depth: æ¨¹èƒ½â½£é•·çš„æœ€æ·±é™åˆ¶
    - Min_samples_split: â¾„å°‘è¦å¤šå°‘æ¨£æœ¬ä»¥ä¸Šæ‰é€²â¾åˆ‡åˆ†
    - Min_samples_lear: æœ€çµ‚çš„è‘‰â¼¦ (ç¯€é») ä¸Šâ¾„å°‘è¦æœ‰å¤šå°‘æ¨£æœ¬

  

- [R Tutorial](https://blog.datasciencedojo.com/classification-decision-trees/)

### Decision Stump

A Decision Stump is a decision tree of 1 level. They are also called 1-rules and use one feature to arrive to a decision. Independently, a Decision Stump is a 'weak' learner, but they can be effective when used as one of the models in bagging and boosting techniques, like AdaBoost.If the data is discrete it can be divided in terms of frequency and continuous data can be divided by a threshold value. The graph on the left-hand side of this image shows a dataset divided linearly by a decision stump.

![Fig. 2: A dataset divided linearly by a decision stump.](https://i.stack.imgur.com/brE2F.png)

- [R Example](https://www.r-bloggers.com/the-power-of-decision-stumps/)

### Naive Bayes

Naive Bayes Classifier is based on the Bayes Theorem. The Bayes Theorem says the conditional probability of an outcome can be computed using the conditional probability of the cause of the outcome.The probability of an event x occurring, given that event C has occurred in the prior probability. It is the knowledge that something has already happened. Using the prior probability, we can compute the posterior probability - which is the probability that event C will occur given that x has occurred. The Naive Bayes classifier uses the input variable to choose the class with the highest posterior probability.The algorithm is called naive because it makes an assumption about the distribution of the data. The distribution can be Gaussian, Bernoulli or Multinomial. Another drawback of Naive Bayes is that continuous features have to be preprocessed and discretized by binning, which can discard useful information.

- å®šç†ï¼š

  - $P(A|B)$: 

    Posterior Probabilityï¼š The Probability of A being true given that B is true

  - $P(B|A)$: 

    Likelihoodï¼š The probability of B being true given that A is true

  - $P(A)$ï¼š 

    Prior Probabilityï¼š The probability of A being true

  - $ P(B)$: 

    Marginal Likelihoodï¼š The probability of B Being true

$$
P(A|B) = \frac{P(B|A) * P(A)}{P(B)}
$$

- Questionï¼š

  - Why Naive?

    Independence assumptionï¼šåœ¨è¨ˆç®—marginalçš„æ™‚å€™æœƒç”¨featuresä¾†ç®—æ¨£æœ¬çš„ç›¸ä¼¼åº¦ã€‚å¦‚æœæ¨£æœ¬å½¼æ­¤é–“ä¸ç¨ç«‹æœƒå½±éŸ¿åˆ°è¨ˆç®—çš„çµæœ(åå‘å–®ä¸€ç¶­åº¦ä½†æœ‰è¨±å¤šé¡ä¼¼ç‰¹å¾µçš„ç¶­åº¦)ã€‚

  - P(X)?

    Randomly select from dataset will exhibit the features similar to the datapoint
    $$
    P(X) = \frac{Number of Similar Observations}{ Total Observations}
    $$

- Python Code

  ```python
  from sklearn.naive_bayes import GaussianNB
  classifier = GaussianNB()
  classifier.fit(X_train, y_train)
  ```

  

- [Tutorial](https://blog.datasciencedojo.com/unfolding-naive-bayes-from-scratch-part-1/)
- [R Example](https://rpubs.com/riazakhan94/naive_bayes_classifier_e1071)

### Gaussian Naive Bayes

The Gaussian Naive Bayes algorithm assumes that all the features have a Gaussian (Normal / Bell Curve) distribution. This is suited for continuous data e.g Daily Temperature, Height. The Gaussian distribution has 68% of the data in 1 standard deviation of the mean, and 96% within 2 standard deviations. Data that is not normally distributed produces low accuracy when used in a Gaussian Naive Bayes classifier, and a Naive Bayes classifier with a different distribution can be used.

- [Python Example](https://www.antoniomallia.it/lets-implement-a-gaussian-naive-bayes-classifier-in-python.html)

### Bernoulli Naive Bayes

The Bernoulli Distribution is used for binary variables - variables which can have 1 of 2 values. It denotes the probability of of each of the variables occurring. A Bernoulli Naive Bayes classifier is appropriate for binary variables, like Gender or Deceased.

- [Python Example](https://chrisalbon.com/machine_learning/naive_bayes/bernoulli_naive_bayes_classifier/)

### Multinomial Naive Bayes

The Multinomial Naive Bayes uses the multinomial distribution, which is the generalization of the binomial distribution. In other words, the multinomial distribution models the probability of rolling a k sided die n times.Multinomial Naive Bayes is used frequently in text analytics because it has a bag of words assumption - which is the position of the words doesn't matter. It also has an independence assumption - that the features are all independent.

- [Python Example](https://towardsdatascience.com/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67)

### K Nearest Neighbours (KNN)

K Nearest Neighbors is a the simplest machine learning algorithm. The idea is to memorize the entire dataset and classify a point based on the class of its K nearest neighbors.Figure 3 from Understanding Machine Learning, by Shai Shalev-Shwartz and Shai Ben-David, shows the boundaries in which a label point will be predicted to have the same class as the point already in the boundary. This is a 1 Nearest Neighbor, the class of only 1 nearest neighbor is used.KNN is simple and without any assumptions, but the drawback of the algorithm is that it is slow and can become weak as the number of features increase. It is also difficult to determine the optimal value of K - which is the number of neighbors used.

- Sepsï¼š
  1. Choose the number K of neighbors(default=5)
  2. Take the K nearest neighbors of the new data point, according to the Euclidean distance.
  3. Among these K neghbors, count the number of data points in each category
  4. Assign the new data point to the category where you counted the most neighbors.

```python
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors = 5,
                           metric = 'minkowski',
                           p = 2)
clf.fit(X_train, y_train)
```

- Parameters
  - n_neighborsï¼šè¦ç”¨å¹¾å€‹é»
  - wrightï¼šé€™äº›é»çš„æ¬Šé‡ã€‚å…¨éƒ¨ç­‰æ–¼1 or è·é›¢è¶Šè¿‘è¶Šé‡è¦...

https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/

https://towardsdatascience.com/k-nearest-neighbors-knn-algorithm-bd375d14eec7

ç¼ºé»ï¼šæ¯æ¬¡predictæ™‚éœ€è¦åŠ è¼‰å…¨éƒ¨è³‡æ–™

- [R Example](https://www.datatechnotes.com/2018/10/learning-vector-quantization.html)

### Support Vector Machine (SVM)

An SVM is a classification and regression algorithm. It works by identifying a hyper plane which separates the classes in the data. A hyper plane is a geometric entity which has a dimension of 1 less than it's surrounding (ambient) space.If an SVM is asked to classify a two-dimensional dataset, it will do it with a one-dimensional hyper place (a line), classes in 3D data will be separated by a 2D plane and Nth dimensional data will be separated by a N-1 dimension line.SVM is also called a margin classifier because it draws a margin between classes. The image, shown here, has a class which is linearly separable. However, sometime classes cannot be separated by a straight line in the present dimension. An SVM is capable of mapping the data in higher dimension such that it becomes separable by a margin.Support Vector machines are powerful in situations where the number of features (columns) is more than the number of samples (rows). It is also effective in high dimensions (such as images). It is also memory efficient because it uses a subset of the dataset to learn support vectors.

![Fig. 4: Margin Classifier](https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_71.png)

- [Python Example](https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/)

  ```python
  # SVM
  from sklearn.svm import SVC
  classifier = SVC(kernel = 'rbf', random_state = 0)
  classifier.fit(X_train, y_train)
  ```

  

### Linear Support Vector Classifier (SVC)

A Linear SVC uses a boudary of  one-degree (linear / straight line) to classify data. It has much less complexity than a non-linear classifier and is only appropriate for small datasets. More complex datasets will require a non linear classifier.

- [Python Example](https://pythonprogramming.net/linear-svc-example-scikit-learn-svm-python/)

### NuSVC

NuSVC uses Nu parameters which is for regularization. Nu is the upper bound on the expected classification error. If the value of Nu us 10% then 10% of the data will be misclassified.

- [Python Example](https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)

### Stochastic Gradient Descent (SGD) Classifier 

SGD is a linear classifier which computes the minima of the cost function by computing the gradient at each iteration and updating the model with a decreasing rate. It is an umbrella term for many types of classifiers, such as Logistic Regression or SVM) that use the SGD technique for optimization.

- [R Example](https://rpubs.com/aaronsc32/quadratic-discriminant-analysis)
- [Python Example](https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/)

### Bayesian Network

A Bayesian Network is a graphical model such that there are no cycles in the graph. This algorithm can model events which are consequences of each other. An event that causes another points to it in the graph. The edges of the graph show condition dependence and the nodes are random variables.

- [R Tutorial](https://www.r-bloggers.com/bayesian-network-in-r-introduction/)
- [Graph Source](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/photoz/.g/web/glossary/bayesnet.html)

### Logistic Regression

Logistic regression estimates the relationship between a dependent categorical variable and independent variables. For instance, to predict whether an email is spam (1) or (0) or whether the tumor is malignant (1) or not (0).If we use linear regression for this problem, there is a need to set up a threshold for classification which generates inaccurate results. Besides this, linear regression is unbounded, and hence we dive into the idea of logistic regression. Unlike linear regression, logistic regression is estimated using the Maximum Likelihood Estimation (MLE) approach. MLE is a "likelihood" maximization method, while OLS is a distance-minimizing approximation method. Maximizing the likelihood function determines the mean and variance parameters that are most likely to produce the observed data. Logistic Regression transforms it's output using the sigmoid function in the case of binary logistic regression. As you can see in Fig. 5, if â€˜tâ€™ goes to infinity, Y (predicted) will become 1 and if â€˜tâ€™ goes to negative infinity, Y(predicted) will become 0.The output from the function is the estimated probability. This is used to infer how confident can predicted value be as compared to the actual value when given an input X. There are several types of logistic regression:

![Fig. 5: Sigmoid Function](https://datasciencedojo.com/wp-content/uploads/Logistic-Regression-Sigmoid-function.png)

- Scikit-learn ä¸­çš„ Logistic Regression

  ```python
  from sklearn.linear_model import LogisticRegression
  clf = LogisticRegression(random_state = 0)
  clf.fit(X_train, y_train)
  ```



- [Python Example](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)

### Zero Rule (ZeroR)

ZeroR is a basic classification model which relies on the target and ignores all predictors. It simply predicts the majority category (class). Although there is no predictibility power in ZeroR, it is useful for determining a baseline performance as a benchmark for other classification methods. This is the least accurate classifier that we can have. For instance, if we build a model whose accuracy is less than the ZeroR model then it's useless.The way this algorithm works is that it constructs a frequency table for the target class and select the most frequent value as it's predicted value regardless of the input features.

- [Python Example](https://machinelearningmastery.com/implement-baseline-machine-learning-algorithms-scratch-python/)

### One Rule (OneR)

This algorithm is also based on the frequency table and chooses one predictor that is used for classification.It generates one rule for each predictor in the data set, then selects the rule with the smallest total error as its "One Rule". To create a rule for the predictor, a frequency table is constructed for each predictor against the target.

- [R Example](https://christophm.github.io/interpretable-ml-book/rules.html)

### Linear Discriminant Analysis (LDA)

Linear Discriminant Analysis (LDA) is performed by starting with 2 classes and generalizing to more. The idea is to find a direction, defined by a vector, such that when the two classes are projected on the vector, they are as spread out as possible.

- [Python Example](https://sebastianraschka.com/Articles/2014_python_lda.html)[Vector Source: Ethem Alpaydin](https://www.cmpe.boun.edu.tr/~ethem/i2ml/i2ml-figs.pdf)

### Quadratic Discriminant Analysis (QDA)

QDA is the same concept as LDA, the only difference is that we do not assume the distribution within the classes are normal. Therefore, a different covariance matrix has to be built for each class which increases the computational cost because there are more parameters to estimate, but it fits data better than LDA.

- [R Example](https://rpubs.com/aaronsc32/quadratic-discriminant-analysis)

### Fisher's Linear Discriminant

Fisher's Linear Discriminant improves upon LDA by maximizing the ratio between class variance and the inter class variance. This reduces the loss of information caused by overlapping classes in LDA. 	




## Unsupervised learning
### Dimensionality reduction

With some problems, especially classification, there can be so many variables, or features, that it is difficult to visualize your data. The correlation amongst your features creates redundancies, and that's where dimensionality reduction comes in. Dimensionality Reduction reduces the number of random variables you're working with. 

### Singular Value Decomposition (SVD)

This is a form of matrix analysis that leads to a low-dimensional representation of a high-dimensional matrix. SVD allows an approximate representation of any matrix, and also makes it easy to eliminate the less important parts of that representation to produce an approximate representation with any desired number of dimensions.Suppose we want to represent a very large and complex matrix using some smaller matrix representation then SVD can factorize an m x n matrix, M, of real or complex values into three component matrices, where the factorization has the form USV. The best way to reduce the dimensionality of the three matrices is to set the smallest of the singular values to zero. If we set a particular number of smallest singular values to 0, then we can also eliminate the corresponding columns. The choice of the lowest singular values to drop when we reduce the number of dimensions can be shown to minimize the root-mean-square error between the original matrix M and its approximation. A useful rule of thumb is to retain enough singular values to make up 90% of the energy. That is, the sum of the squares of the retained singular values should be at least 90% of the sum of the squares of all the singular values. It is also possible to reconstruct the approximation of the original matrix M using U, S , and V.SVD is used in the field of predictive analytics. Normally, we would want to remove a number of columns from the data since a greater number of columns increases the time taken to build a model. Eliminating the least important data gives us a smaller representation that closely approximates the original matrix. If some columns are redundant in the information they provide then this means those columns contribute noise to the model and reduce predictive accuracy. Dimensionality reduction can be achieved by simply dropping these extra columns. The resulting transformed data set can be provided to machine learning algorithms to yield much faster and accurate models.

![Fig. 14: SVD](https://datasciencedojo.com/wp-content/uploads/singular-value-decomposition-svd.png)

- [R Example](https://www.displayr.com/singular-value-decomposition-in-r/)

### CA

https://www.princexml.com/doc/troubleshooting/

### Principal Component Analysis (PCA)

- PCA is a projection technique which find a projection of the data in a smaller dimension. The idea is to find an axis in the data with highest variance and to map the data along that axis.In figure 15, the data along vector 1 shows a higher variance than vector 2. Therefore, vector 1 will be preferred and chosen as the first principle component. The axis has been rotated in the direction of highest variance. We have thus reduced the dimensionality from two (X1 and X2) to one (PC 1).PCA is useful in cases where the dimensions are highly correlated. For example, pixels in images have a high correlation with each other, here will will prove a significant gain my reducing the dimension. However, if the features are not correlated to each other than the dimension will be the almost the same in quantity after PCA.Fig. 15: Original vs Principal Component R Tutorial

  ![Fig. 15: Original vs Principal Component ](http://datasciencedojo.com/wp-content/uploads/principle-component-analysis-pca.png)

  - [R Tutorial](https://www.r-bloggers.com/principal-component-analysis-in-r/)

- ç›®çš„

  - Identify patterns in data
  - Detect the correlation between variables
  - Reduce the dimensions of a d-dimensional dataset by projecting into a (k)-dimensional subspace(where k < d) 
  - form the m independent variables of your dataset, PCA extracts p<= m new independent variables that explain the most the variance of the dataset.

- æµç¨‹

  - Standardize the data.
  - Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.
  - Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where $k$ is the number of dimensions of the new feature subspace ($k<=d$).
  - Construct the projection matrix $W$ from the selected $k$ eigenvectors.
  - Transform the original dataset $X$ via $W$ to obtain a $k$-dimensional feature subspace $Y$.

- åƒè€ƒè³‡æ–™

  - [Principal Component Analysis in Python/v3](https://plotly.com/python/v3/ipython-notebooks/principal-component-analysis/)

- èªªæ˜

  - å¯¦å‹™ä¸Šæˆ‘å€‘ç¶“å¸¸é‡åˆ°è³‡æ–™æœ‰éå¸¸å¤šçš„ features, æœ‰äº› features å¯èƒ½â¾¼åº¦ç›¸é—œï¼Œæœ‰ä»€éº¼â½…æ³•èƒ½å¤ æŠŠâ¾¼åº¦ç›¸é—œçš„ features å»é™¤ï¼Ÿ
  - PCA é€éè¨ˆç®— eigen value, eigen vector, å¯ä»¥å°‡åŸæœ¬çš„ features é™ç¶­â¾„ç‰¹å®šçš„ç¶­åº¦
    - åŸæœ¬è³‡æ–™æœ‰ 100 å€‹ featuresï¼Œé€é PCAï¼Œå¯ä»¥å°‡é€™ 100 å€‹ features é™æˆ 2 å€‹features
    - æ–° features ç‚ºèˆŠ features çš„ç·šæ€§çµ„åˆ
    - æ–° features ä¹‹é–“å½¼æ­¤ä¸ç›¸é—œ

- çˆ²ä»€éº¼éœ€è¦é™ä½ç¶­åº¦ ? 

  é™ä½ç¶­åº¦å¯ä»¥å¹«åŠ©æˆ‘å€‘å£“ç¸®åŠä¸Ÿæ£„ç„¡â½¤è³‡è¨Šã€æŠ½è±¡åŒ–åŠçµ„åˆæ–°ç‰¹å¾µã€è¦–è¦ºåŒ–â¾¼ç¶­æ•¸æ“šã€‚å¸¸â½¤çš„ç®—æ³•çˆ²ä¸»æˆåˆ†åˆ†æã€‚

  - å£“ç¸®è³‡æ–™

    - æœ‰åŠ©æ–¼ä½¿â½¤è¼ƒå°‘çš„ RAM æˆ– disk spaceï¼Œä¹Ÿæœ‰åŠ©æ–¼åŠ é€Ÿ learning algorithms

    - å½±åƒå£“ç¸®

      - åŸå§‹å½±åƒç¶­åº¦çˆ² 512, åœ¨é™ä½ç¶­åº¦åˆ° 16 çš„æƒ…æ³ä¸‹ , åœ–ç‰‡é›–ç„¶æœ‰äº›è¨±æ¨¡ç³Š ,ä½†ä¾ç„¶ä¿æœ‰æ˜é¡¯çš„è¼ªå»“å’Œç‰¹å¾µ

        ![](https://lh3.googleusercontent.com/p7w41bPYhLPENoIaH7t_xqsPB1IMINSe918mSI3GELa3uNbzxHBSS66Th8ahXaYIuU9fpEAzsZRyKNuD4hZd9On0axuGqgU3cXimCeJtA_STghhJKa-oZZYYTPah9NqQ5oLj5AuhGPpzmMxA1VmNDSZ5PYAEy5u-GBhFupbLJD5XtcrSTnHm7hTuDj3Fatv8BmCJXUJ3QWeB2L2P4wJduMs7rNt9dI9GE-_v2e5fcay0sBWNa9eCGadZbyemHZZd5FPpaCFpbN-s-NsdUuBCVQ7tN6rgpTgIIiCf0DXyf22oi1gPj3or-dAHXlX4aFHZQC97NvbW3rVYCAIEFZW3tXN5zLdqs5wV_EESqp6AXrGsObv0xbbYp4MlbbabsqcPlbQoRmq9niu9leNi3p2l5bKLE9encAsGTDXE4cm3I57bDlNIjZeTsCtBfL_e0g6WDrdJ_A4NnNy_8LrJpZ0ckX5bAbfTpPxGTvGMK91CcNrMkerRHeBz2tbBD8mpmHrqBYkwUUPFjW2gPlK317vpOb9GHep-TEh6BsZ29ldVvanmbd6zcQtrRiit08cScFQcXcRnQirzfzs5Rn5VRFos7FcIqezZfMPWxpKGXrQCuyWnhX5gQuR00xyUjPNsF-wWVS1pJloFyPJIc7D38vsY-bjbXYWS2xeq_bSMhaMGnDavuimr3dN6qWG5XUXI-zmneS8uTq0Vt7BEvdZGnHFWTygi3oAjaQ6cjYE94jMlOqS3LFA=w646-h514-no)

  - ç‰¹å¾µçµ„åˆåŠæŠ½è±¡åŒ–

    - å£“ç¸®è³‡æ–™å¯é€²â½½çµ„åˆå‡ºæ–°çš„ã€æŠ½è±¡åŒ–çš„ç‰¹å¾µï¼Œæ¸›å°‘å†—é¤˜çš„è³‡è¨Šã€‚

    - å·¦ä¸‹åœ–çš„ x1 å’Œ x2 â¾¼åº¦ç›¸é—œ , å› æ­¤å¯ä»¥åˆä½µæˆ 1 å€‹ç‰¹å¾µ (å³ä¸‹åœ–)ã€‚

      - æŠŠ x(i) æŠ•å½±åˆ°è—â¾Šç·š , å¾ 2 ç¶­é™ä½çˆ² 1 ç¶­ã€‚

      ![](https://lh3.googleusercontent.com/mgqelyYL1QQbGhn9eJhmlb2b0zl72fOr3QzCuK6Kqz0tcva4jR_sBYCgYPtq8VJ0VFTQbgWExqcaVCxHpn9h_dNwCaxx1hIyxFVRk2WP2crTOkqh0l3YT36e_Ckao-_zQSfBBmBPA3spWswzmE_AN5a52iAtH0GTZqx7LrleVS5KFyt2Ih0grm7PNWiBi_9-rHpG5gdyAH0fYYf2sJ04kQyQEEvDLLwaLIHvBbUUTkhV-gNlpdASvhAefrj1LSaGULQSPtn2F1SpQ5D4r9n741OrX9pjuaQvd1I99ZZxGjpCMAlY4IX1K4wQTC9VggxhcqbRmOTzsob7dIexz1u5o8SykSr1AJ7o6VcJFzxogH5h3bKDyZlY6Z2fUs9VwTDgOpGKnw_fjYs5PuBApXCgPiDYbbSD5og9GWu_onWDEB2xWUxbKJJIVukO-w0px7NJZ_uGGQUAw26A2jJWgYbJBKAcsT7vyPitfi287zGMXTyP5ECxoXAJk2ejXmjhxQ-XyoIstOMf4BVGtFJVos3DrhaKN97wv-TI8J63LlbmCtVFu70uOAtxc7QX_miA6JSvCYgwM61eAht292akoFg_xzb7go6IqB4Ev5uRLt5x2TGwQErRxcr7nY-ytEGcAQe7WdrB3aydLaJkG7n7jKjUbeh5OsuKF8eMOfBoi4Yr4oBpgOeI2yLynBGDHVJmZ1RD1PUzLepAi37FZC31CvIFZeZYVuDdvNTf1mdiPH-d4Rt4xKM=w793-h372-no)

  - è³‡æ–™è¦–è¦ºåŒ–

    - ç‰¹å¾µå¤ªå¤šæ™‚ï¼Œå¾ˆé›£ visualize data, ä¸å®¹æ˜“è§€å¯Ÿè³‡æ–™ã€‚
    - æŠŠè³‡æ–™ç¶­åº¦ (ç‰¹å¾µ) é™åˆ° 2 åˆ° 3 å€‹ , å‰‡èƒ½å¤ â½¤â¼€èˆ¬çš„ 2D æˆ– 3D åœ–è¡¨å‘ˆç¾è³‡æ–™

- æ‡‰â½¤

  - çµ„åˆå‡ºä¾†çš„é€™äº›æ–°çš„ features å¯ä»¥é€²â½½â½¤ä¾†åš supervised learning é æ¸¬æ¨¡å‹
  - ä»¥åˆ¤æ–·â¼ˆè‡‰çˆ²ä¾‹ , æœ€é‡è¦çš„ç‰¹å¾µæ˜¯çœ¼ç›ã€â¿â¼¦ã€å˜´å·´ï¼Œè†šâ¾Šå’Œé ­é«®ç­‰éƒ½å¯æ¨æ£„ï¼Œå°‡é€™äº›ä¸å¿…è¦çš„è³‡è¨Šæ¨æ£„é™¤äº†å¯ä»¥åŠ é€Ÿ learning , ä¹Ÿå¯ä»¥é¿å…â¼€é»overfittingã€‚

- å¦‚ä½•æ±ºå®šè¦é¸å¤šå°‘å€‹ä¸»æˆåˆ†?

  - Elbow
  - ç´¯ç©çš„è§£é‡‹è®Šç•°é‡é”85%

- é™ä½ç¶­åº¦å¯ä»¥å¹«åŠ©æˆ‘å€‘å£“ç¸®åŠä¸Ÿæ£„ç„¡â½¤è³‡è¨Šã€æŠ½è±¡åŒ–åŠçµ„åˆæ–°ç‰¹å¾µã€å‘ˆç¾â¾¼ç¶­æ•¸æ“šã€‚å¸¸â½¤çš„ç®—æ³•çˆ²ä¸»æˆåˆ†åˆ†æã€‚

- åœ¨ç¶­åº¦å¤ªâ¼¤ç™¼â½£ overfitting çš„æƒ…æ³ä¸‹ï¼Œå¯ä»¥å˜—è©¦â½¤ PCA çµ„æˆçš„ç‰¹å¾µä¾†åšç›£ç£å¼å­¸ç¿’ï¼Œä½†ä¸å»ºè­°â¼€é–‹å§‹å°±åšã€‚

- æ³¨æ„äº‹é …

  - ä¸å»ºè­°åœ¨æ—©æœŸæ™‚åš , å¦å‰‡å¯èƒ½æœƒä¸Ÿå¤±é‡è¦çš„ features â½½ underfittingã€‚
  - å¯ä»¥åœ¨ optimization éšæ®µæ™‚ , è€ƒæ…® PCA, ä¸¦è§€å¯Ÿé‹â½¤äº† PCA å¾Œå°æº–ç¢ºåº¦çš„å½±éŸ¿
  - PCAæ˜¯é€éè·é›¢ä¾†é€²è¡Œé‹ç®—ï¼Œå› æ­¤åœ¨è·‘PCAä¹‹å‰éœ€è¦å°è³‡æ–™åšæ¨™æº–åŒ–ã€‚é¿å…PCAçš„çµæœå› ç‚ºæ¸¬é‡ç¯„åœçš„ä¸ä¸€è‡´ï¼Œå°è‡´åªåæ˜ å…¶ä¸­ç¯„åœè¼ƒå¤§çš„è®Šé‡ã€‚
  - [https://medium.com/@jimmywu0621/dimension-reduction-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3pca%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95-f0ce2dd28660](https://medium.com/@jimmywu0621/dimension-reduction-å¿«é€Ÿäº†è§£pcaçš„åŸç†åŠä½¿ç”¨æ–¹æ³•-f0ce2dd28660)

```python
from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)
explained_variance = pca.explained_variance_ratio_
```

### Kernel PCA

```python
from sklearn.decomposition import KernelPCA
kpca = KernelPCA(n_components = 2, kernel = 'rbf')
X_train = kpca.fit_transform(X_train)
X_test = kpca.transform(X_test)
```

### Partial Least Squares Regression (PLSR)

Partial least squares regression (PLS regression) is developed from principal components regression. It works in a similar fashion as it finds a linear regression model by projecting the predicted variables and the predictor variables to a new space instead of finding hyperplanes of maximum variance between the target and predictor variables. While, PCR creates components to explain the observed variability in the predictor variables, without considering the target variable at all, PLS Regression, on the other hand,  does take the response variable into account, and therefore often leads to models that are able to fit the target variable with fewer components. However, it depends on the context of the model if using PLS Regression over PCR would offer a more parsimonious model.

- [R Example](https://rpubs.com/omicsdata/pls)

### Latent Dirichlet Analysis (LDA)

Latent Dirichlet Allocation (LDA) is one of the most popular techniques used for topic modelling. Topic modelling is a process to automatically identify topics present in a text object.A latent Dirichlet allocation model discovers underlying topics in a collection of documents and infers word probabilities in topics. LDA treats documents as probabilistic distribution sets of words or topics. These topics are not strongly defined â€“ as they are identified based on the likelihood of co-occurrences of words contained in them.The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place. The goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics.A collection of documents is represented as a document-term matrix. LDA converts this document-term matrix into 2 lower dimensional matrices, where one is a document-topics matrix and the other is a topic-terms matrix. LDA then makes use of sampling techniques in order to improve these matrices. A steady state is achieved where the document topic and topic term distributions are fairly good. As a result, it builds a topic per document model and words per topic model, modeled as Dirichlet distributions.

- [R Example](https://www.tidytextmining.com/topicmodeling.html)

### Regularized Discriminant Analysis (RDA)

The regularized discriminant analysis (RDA) is a generalization of the linear discriminant analysis (LDA) and the quadratic discriminant analysis (QDA). RDA differs from discriminant analysis in a manner that it estimates the covariance in a new way, which combines the covariance of QDA with the covariance of LDA using a tuning parameter. Since RDA is a regularization technique, it is particularly useful when there are many features that are potentially correlated.

- [R Example](https://daviddalpiaz.github.io/r4sl/regularized-discriminant-analysis.html)

### Linear Discriminant Analysis

- Used as a dimensionality reduction technique

- Used in the pre-processing step for pattern classification

- Has the goal to project a dataset onto a lower-dimensional space

- LDA differs because in addition to finding the component axises with LDA we are interested in the axes that maximize the separation between multiple aclsses.

- Breaking it down further:

  The goal of LDA is to project a feature space (a dataset n-dimensional

  samples) onto a small subspace subspace k(where ksn-1) while

  maintaining the class-discriminatory information.

  Both PCA and LDA are linear transformation techniques used for

  dimensional reduction. PCA is described as unsupervised but LDA is

  supervised because of the relation to the dependent variable.

- From the n independent variables of your dataset, LDA extracts p <= n new independent variables that separate the most the classes of the dependent variable.

  - The fact that the DV is considered makes LDA a supervised model.

- Difference with PCA

  - PCA: component axes that maximize the variance.
  - LDA: maximizing the component axes for class-separation.

- Step

  1. Compute the $d$-dimensional mean vectors for the different classes from the dataset.
  2. Compute the scatter matrices (in-between=class and within -class scatter matrix).
  3. Compute the eigenvectors($e_1$, $e_2$,...$e_d$) and corresponging eigenvalues($\lambda_1$, $\lambda_2$, ..., $\lambda_d$) for the scatter matrices.
  4. Sort the eigenvectors by decreasing eigrnvalues and choose $k$ eigenvectors with the largest eigenvalues to form a $d * k$ dimensional matrix $W$ (where every column represents an eigenvector).
  5. Use this $d*k$ eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the matrix multiplication: $Y = X * W$(where $X$ is a $n*d$-dimensional matrix representing the $n$ samples, and $y$ are the transformed $n*k$-dimensional samples in the new subspace).  

  ```python
  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
  lda = LinearDiscriminantAnalysis(n_components = 2)
  X_train = lda.fit_transform(X_train, y_train)
  X_test = lda.transform(X_test)
  ```

  

### t-Distributed Stochastic Neighbor Embedding (t-SNE)

t-SNE is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to lower dimensions which are easy to visualize.This algorithm calculates probability of similarity of points in high-dimensional space and in the low dimensional space. It then tries to optimize these two similarity measures using a cost function. To measure the minimization of the sum of difference of conditional probability, t-SNE minimizes the sum of Kullback-Leibler divergence of data points using a gradient descent method. t-SNE minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the high-dimensional points and a distribution that measures pairwise similarities of the corresponding low-dimensional points. Using this technique, t-SNE can find patterns in the data by identifying clusters based on similarity of data points with multiple features.t-SNE stands out from all the other dimensionality reduction techniques since it is not limited to linear projections so it is suitable for all sorts of datasets. 

- [R and Python Examples](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)



t-Distributed Stochastic Neighbor Embedding

> - ç­è§£ PCA çš„é™åˆ¶
> - t-SNE æ¦‚å¿µç°¡ä»‹ï¼ŒåŠå…¶å„ªåŠ£

- PCA çš„å•é¡Œ

  - æ±‚å…±è®Šç•°æ•¸çŸ©é™£é€²â¾å¥‡ç•°å€¼åˆ†è§£ï¼Œå› æ­¤æœƒè¢«è³‡æ–™çš„å·®ç•°æ€§å½±éŸ¿ï¼Œç„¡æ³•å¾ˆå¥½çš„è¡¨ç¾ç›¸ä¼¼æ€§åŠåˆ†ä½ˆã€‚
  - PCA æ˜¯â¼€ç¨®ç·šæ€§é™ç¶­â½…å¼ï¼Œå› æ­¤è‹¥ç‰¹å¾µé–“æ˜¯éç·šæ€§é—œä¿‚ï¼Œæœƒæœ‰
    underfitting çš„å•é¡Œã€‚

- t-SNE

  - t-SNE ä¹Ÿæ˜¯â¼€ç¨®é™ç¶­â½…å¼ï¼Œä½†å®ƒâ½¤äº†æ›´è¤‡é›œçš„å…¬å¼ä¾†è¡¨é”â¾¼ç¶­å’Œä½ç¶­ä¹‹é–“çš„é—œä¿‚ã€‚
  - ä¸»è¦æ˜¯å°‡â¾¼ç¶­çš„è³‡æ–™â½¤ gaussian distribution çš„æ©Ÿç‡å¯†åº¦å‡½æ•¸è¿‘ä¼¼ï¼Œâ½½ä½ç¶­è³‡æ–™çš„éƒ¨åˆ†â½¤ t åˆ†ä½ˆä¾†è¿‘ä¼¼ï¼Œåœ¨â½¤ KL divergence è¨ˆç®—ç›¸ä¼¼åº¦ï¼Œå†ä»¥æ¢¯åº¦ä¸‹é™ (gradient descent) æ±‚æœ€ä½³è§£ã€‚

- t-SNE å„ªåŠ£

  - å„ªé»
    - ç•¶ç‰¹å¾µæ•¸é‡éå¤šæ™‚ï¼Œä½¿â½¤ PCA å¯èƒ½æœƒé€ æˆé™ç¶­å¾Œçš„ underfittingï¼Œé€™æ™‚å¯ä»¥è€ƒæ…®ä½¿â½¤t-SNE ä¾†é™ç¶­
  - ç¼ºé»
    - t-SNE çš„éœ€è¦æ¯”è¼ƒå¤šçš„æ™‚é–“åŸ·â¾

- è¨ˆç®—é‡å¤ªå¤§äº†ï¼Œé€šå¸¸ä¸æœƒç›´æ¥å°åŸå§‹è³‡æ–™åšTSNE,ä¾‹å¦‚æœ‰100ç¶­çš„è³‡æ–™ï¼Œé€šå¸¸æœƒå…ˆç”¨PCAé™æˆ50ç¶­ï¼Œå†ç”¨TSNEé™æˆ2ç¶­

- å¦‚æœæœ‰æ–°çš„é»åŠ å…¥ï¼Œå¦‚æœç›´æ¥å¥—ç”¨æ—¢æœ‰æ¨¡å‹ã€‚å› æ­¤TSNEä¸æ˜¯ç”¨ä¾†åštraing testingï¼Œè€Œæ˜¯ç”¨ä¾†åšè¦–è¦ºåŒ–

- æµå½¢é‚„åŸ

  - æµå½¢é‚„åŸå°±æ˜¯å°‡â¾¼ç¶­åº¦ä¸Šç›¸è¿‘çš„é»ï¼Œå°æ‡‰åˆ°ä½ç¶­åº¦ä¸Šç›¸è¿‘çš„é»ï¼Œæ²’æœ‰è³‡æ–™é»çš„åœ°â½…ä¸åˆ—å…¥è€ƒé‡ç¯„åœ
  - ç°¡å–®çš„èªªï¼Œå¦‚æœè³‡æ–™çµæ§‹åƒç‘â¼ æ²â¼€æ¨£ï¼Œé‚£éº¼æµå½¢é‚„åŸå°±æ˜¯æŠŠå®ƒæ”¤é–‹é‹ªå¹³ (æµå½¢é‚„åŸè³‡æ–™é›†çš„å…¶ä¸­â¼€ç¨®ï¼Œå°±æ˜¯å«åšç‘â¼ æ²-Swiss Roll)
  - æµå½¢é‚„åŸå°±æ˜¯åœ¨â¾¼ç¶­åº¦åˆ°ä½ç¶­åº¦çš„å°æ‡‰ä¸­ï¼Œç›¡é‡ä¿æŒè³‡æ–™é»ä¹‹é–“çš„é è¿‘é—œä¿‚ï¼Œæ²’æœ‰è³‡æ–™é»çš„åœ°â½…ï¼Œå°±ä¸åˆ—å…¥è€ƒé‡ç¯„åœ
  - é™¤äº† t-sne å¤–ï¼Œè¼ƒå¸¸â¾’çš„æµå½¢é‚„åŸé‚„æœ‰ Isomap èˆ‡ LLE (Locally Linear Embedding) ç­‰â¼¯å…·

- ç‰¹å¾µé–“çˆ²éç·šæ€§é—œä¿‚æ™‚ (e.g. â½‚å­—ã€å½±åƒè³‡æ–™)ï¼ŒPCAå¾ˆå®¹æ˜“ underfittingï¼Œt-SNE å°æ–¼ç‰¹å¾µéç·šæ€§è³‡æ–™æœ‰æ›´å¥½çš„é™ç¶­å‘ˆç¾èƒ½â¼’ã€‚

- Ref
  - [StatsLearning Lect12a](https://www.youtube.com/watch?v=ipyxSYXgzjQ)
  - [StatsLearning Lect12b](https://www.youtube.com/watch?v=dbuSGWCgdzw)
  - [StatsLearning Lect8k](https://www.youtube.com/watch?v=eYxwWGJcOfw)
  - [Principal Component Analysis Algorithm](https://www.youtube.com/watch?v=rng04VJxUt4)
  - [ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰åŸç†æ€»ç»“](https://www.cnblogs.com/pinard/p/6239403.html)

- [Visualizing Data Using t-SNE](https://www.youtube.com/watch?v=RJVL80Gg3lA)
- [ML Lecture 15: Unsupervised Learning - Neighbor Embedding](https://www.youtube.com/watch?v=GBUEjkpoxXc)



### Factor Analysis

Factor Analysis is designed on the premise that there are latent factors which give origin to the available data that are not observed. In PCA, we create new variables with the available ones, here we treat the data as created variables and try to reach the original ones â€“ thus reversing the direction of PCA.If there is a group of variables that are highly correlated, there is an underlying factor that causes that and can be used as a representative variable. Similarly, the other variables can also be grouped and these groups can be represented using such representative variables.Factor analysis can also be used for knowledge extraction, to find the relevant and discriminant piece of information.

- [R Example](https://www.promptcloud.com/blog/exploratory-factor-analysis-in-r/)

### Multidimensional Scaling (MDS)

Multidimensional Scaling (MDS) computes the pairwise distances between data points in the original dimensions of the data. The data points are mapped on the a lower dimension space, like the Euclidean Space, such that the paints with low pairwise distances in higher dimension are also close in the lower dimension and points which are far apart in higher dimension, are also apart in lower dimension.The pitfall of this algorithm can be seen in the analogy of geography. Locations which are far apart in road distance due to mountains or rough terrains, but close by in bird-flight path will be mapped far apart by MDS because of the high value of the pairwise distance.

![Fig 15: Map of Europe drawn by MDS](https://datasciencedojo.com/wp-content/uploads/Diagram-from-the-introduction-to-machine-learning-by-Ethem-Alpaydin.png)

- [R Example](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/122-multidimensional-scaling-essentials-algorithms-and-r-code/)

### AutoEncoder

A tool for dimensionality reduction, an autoencoder has as many outputs as inputs and it is forced to find the best representation of the inputs in the hidden layer. There are fewer perceptrons in the hidden layer, which implies dimensionality reduction. Once training is complete, the first layer from the input layer to the hidden layer acts as an encoder which finds a lower dimension representation of the data. The decoder is from the layer after the hidden layer to the output layer.The encoder can be used to pass data and find a lower dimension representation for dimension reduction.

- [Python Example](https://blog.keras.io/building-autoencoders-in-keras.html)

### Independent Component Analysis (ICA)

ICA solves the cocktail party problem. At a cocktail party, one is able to seperate the voice of any one person from the voices in the background. Computers are not as efficient at separating the noise from signal as the human brain, but ICA can solve this problem if the data is not Gaussian.ICA assumes independence among the variables in the data. It also assumes that the mixing of the noise and signal is linear, and the source singal has a non-gaussian distribution.

- [R Example](https://rpubs.com/skydome20/93614)

### Isomap

Isomap (Isometric Mapping) computes the geodesic distances between data points and maps those distances in a Euclidean space to create a lower dimension mapping of the same data.Isomap offers the advantage of using global patterns by first making a neighborhood graph using euclidean distances and then computes graph distances between the nodes. Thus, it uses local information to find global mappings.

- [Python Example](http://benalexkeen.com/isomap-for-dimensionality-reduction-in-python/)

### Local Linear Embedding (LLE)

LLE reduces the dimension of the data such that neighbourhood information (topology) is intact. Points that are far apart in high dimension should also be far apart in lower dimension. LLE assumes that data is on a smooth surface without abrupt holes and that it is well sampled (dense).LLE works by creating a neighbourhood graph of the dataset and computing a local weight matrix using which it regenerates the data in lower dimension. This local weight matrix allows it to maintain the topology of the data.

- [R Example](http://rstudio-pubs-static.s3.amazonaws.com/94107_913ae6a497fc408a91a2529b6c57f791.html)

### Locality-Sensitive Hashing

This technique uses a hash function to determine the similarity of the data. A hash function provide a lower dimensional unique value for an input and used for indexing in databases. Two similar values will give a similar hash value which is used by this technique to determine which data points are neighbours an which are far apart to produce a lower dimensional version of the input data set.

- [R Example](https://cran.r-project.org/web/packages/textreuse/vignettes/textreuse-minhash.html)

### Sammon Mapping

Sammon Mapping creates a projection of the data such that geometric relations between data points are maintained to the highest extent. It creates a new dataset using the pairwise distances between points. Sammon mapping is frequently used in image recognition tasks.

![Fig. 16: Sammon Mapping vs. PCA Projection](https://datasciencedojo.com/wp-content/uploads/sammon-mapping-vs-pca-projection.png)

- [Paul Henderson](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/AV0910/henderson.pdf)
- [Python Example](https://datawarrior.wordpress.com/2016/10/23/sammon-embedding/)

### Clustering

In supervised learning, we know the labels of the data points and their distribution. However, the labels may not always be known. Clustering is the practice of assigning labels to unlabeled data using the patterns that exist in it. Clustering can either be semi-parametric or probabilistic. 

### K-means

K-Means Clustering is an iterative algorithm which starts of with k random numbers used as mean values to define clusters. Data points belong to the cluster defined by the mean value to which they are closest. This mean value co-ordinate is called the centroid.Iteratively, the mean value of the data points of each cluster is computed and the new mean values are used to restart the process till mean stop changing. The disadvantage of K-Means is that it a local search procedure and could miss global patterns.The k initial centroids can be randomly selected. Another approach of determining k is to compute the mean of the entire dataset and add k random co-ordinates to it to make k initial points. Another approach is to determine the principle component of the data and divide into k equal partitions. The mean of each partition can be used as initial centroids.

- [Python Example](https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/)

- ç•¶å•é¡Œä¸æ¸…æ¥šæˆ–æ˜¯è³‡æ–™æœªæœ‰æ¨™è¨»çš„æƒ…æ³ä¸‹ï¼Œå¯ä»¥å˜—è©¦â½¤åˆ†ç¾¤ç®—æ³•å¹«åŠ©ç­è§£è³‡æ–™çµæ§‹ï¼Œâ½½å…¶ä¸­â¼€å€‹â½…æ³•æ˜¯é‹â½¤ K-means èšé¡ç®—æ³•å¹«åŠ©åˆ†ç¾¤è³‡æ–™

- åˆ†ç¾¤ç®—æ³•éœ€è¦äº‹å…ˆå®šç¾©ç¾¤æ•¸ï¼Œå› æ­¤æ•ˆæœè©•ä¼°åªèƒ½è—‰ç”±â¼ˆçˆ²è§€å¯Ÿã€‚

- æŠŠæ‰€æœ‰è³‡æ–™é»åˆ†æˆ k å€‹ clusterï¼Œä½¿å¾—ç›¸åŒ cluster ä¸­çš„æ‰€æœ‰è³‡æ–™é»å½¼æ­¤å„˜é‡ç›¸ä¼¼ï¼Œâ½½ä¸åŒ cluster çš„è³‡æ–™é»å„˜é‡ä¸åŒã€‚

- è·é›¢æ¸¬é‡ï¼ˆe.g. æ­â½’è·é›¢ï¼‰â½¤æ–¼è¨ˆç®—è³‡æ–™é»çš„ç›¸ä¼¼åº¦å’Œç›¸ç•°åº¦ã€‚æ¯å€‹ clusteræœ‰â¼€å€‹ä¸­â¼¼é»ã€‚ä¸­â¼¼é»å¯ç†è§£ç‚ºæœ€èƒ½ä»£è¡¨ cluster çš„é»ã€‚

- ç®—æ³•æµç¨‹

  1. Choose the number K of cluster

  2. Select at random K points, the centroids

  3. Assign each data point to the colsest centroid.

  4. Compute and place the new centroid of each cluster.

  5. Reassign each data point to the new closest centroid.

     If any reassignment took place, go to Step 4, otherwise go to Finish!

  ![](https://lh3.googleusercontent.com/pw/ACtC-3eZXqeja13nqUyYVNx84Y2fGtUdp5T3kil1x6csdapqivY7eRx2_Ps3_hb_ThFD50Yx2hI8WIBmkfvRbJfPmP_-wLBGBaj2x6llhWQCqgyDrrUA8DMnz3aXMDDA0zcmae-fo_S3vb8JIdBdUpZAxGgG=w637-h619-no?authuser=1)

- æ•´é«”ç›®æ¨™ï¼šK-means â½¬æ¨™æ˜¯ä½¿ç¸½é«”ç¾¤å…§å¹³â½…èª¤å·®æœ€â¼©

$$
\sum^n_{i=0} \min_{\mu \epsilon C}(||X_i -  \mu_j||^2)
$$

- Random initialization Trap

  - initial è¨­å®šçš„ä¸åŒï¼Œæœƒå°è‡´å¾—åˆ°ä¸åŒ clustering çš„çµæœï¼Œå¯èƒ½å°è‡´ local optimaï¼Œâ½½é global optimaã€‚
  - Solution: Kmeans++

- Choosing the right number of cluster

  - å› çˆ²æ²’æœ‰é å…ˆçš„æ¨™è¨˜ï¼Œå°æ–¼ cluster æ•¸é‡å¤šå°‘æ‰æ˜¯æœ€ä½³è§£ï¼Œæ²’æœ‰æ¨™æº–ç­”æ¡ˆï¼Œå¾—é â¼¿å‹•æ¸¬è©¦è§€å¯Ÿã€‚

  - $$
    WCSS = \sum_{P_i inCluster1} distance(Pi,C1)^2 + \sum_{P_i inCluster2} distance(Pi,C2)^2 + \sum_{P_i inCluster3} distance(Pi,C3)^2 + ...
    $$

  - Elbow Method

    è§€å¯Ÿ WCSS çš„æ•¸å€¼çš„é™ä½è¶¨å‹¢ï¼Œç•¶ K+1 çš„ WCSSå€¼æ²’æœ‰æ˜é¡¯é™ä½æ™‚ï¼ŒKå°±æ˜¯åˆé©çš„åˆ†ç¾¤çµ„æ•¸(Optimal number of cluster)

- 

- æ³¨æ„äº‹é …

  3. kmeansæ˜¯é€éè·é›¢ä¾†è©•ä¼°ç›¸ä¼¼åº¦ï¼Œå› æ­¤å°æ–¼é›¢ç¾¤å€¼æœƒéå¸¸æ•æ„Ÿã€‚

- Kmeans in Python

  ```python
  from sklearn.cluster import KMeans
  # Find optimal number of cluster
  wcss = []
  for i in range(1, 11):
      kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
      kmeans.fit(X)
      wcss.append(kmeans.inertia_)
  plt.plot(range(1, 11), wcss)
  plt.title('The Elbow Method')
  plt.xlabel('Number of clusters')
  plt.ylabel('WCSS')
  plt.show()
  
  # Fit and predict
  kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)
  y_kmeans = kmeans.fit_predict(X)
  ```

### K-Medians Clustering

K-Medians uses absolute deviations (Manhattan Distance) to form k clusters in the data. The centroid of the clusters is the median of the data points in the cluster. This technique is the same as K-Means but more robust towards outliers because of the use of median not mean, because K-Means optimizes the squared distances.Consider a list of numbers: 3, 3, 3, 9. It's median is 3 and mean is 4.5. Thus, we see that use of median prevents the effect of outliers.

- [Python Example](https://gist.github.com/mblondel/1451300)

### Mean Shift Clustering

Mean Shift is a hierarchical clustering algorithm. It is a sliding-window-based algorithm that attempts to find dense areas of data points. Mean shift considers the feature space as sampled from the underlying probability density function. For each data point, Mean shift associates it with the nearby peak of the dataset's probability density function. Given a set of data points, the algorithm iteratively assigns each data point towards the closest cluster centroid. A window size is determined and a mean of the data points within the window is calculated. The direction to the closest cluster centroid is determined by where most of the points nearby are at. So after each iteration, each data point will move closer to where the most points are at, which leads to the cluster center.Then, the window is shifted to the newly calculated mean and this process is repeated until convergence. When the algorithm stops, each point is assigned to a cluster.Mean shift can be used as an  image segmentation algorithm. The idea is that similar colors are grouped to use the same color. This can be accomplished by clustering the pixels in the image. This algorithm is really simple since there is only one parameter to control which is the sliding window size. You don't need to know the number of categories (clusters) before applying this algorithm, as opposed to K-Means. The downside to Mean Shift is it's computationally expensiveâ€Šâ€”â€ŠO(nÂ²). The selection of the window size can be non-trivial. Also, it does not scale well with dimension of feature space. 

- [Python Example](https://pythonprogramming.net/mean-shift-from-scratch-python-machine-learning-tutorial/)

### K-Modes Clustering

A lot of data in real world data is categorical, such as gender and profession, and, unlike numeric data, categorical data is discrete and unordered. Therefore, the clustering algorithms for numeric data cannot be used for categorical data. K-Means cannot handle categorical data since mapping the categorical values to 1/0 cannot generate quality clusters for high dimensional data so instead we can land onto K-Modes.The K-Modes approach modifies the standard K-Means process for clustering categorical data by replacing the Euclidean distance function with the simple matching dissimilarity measure, using modes to represent cluster centers and updating modes with the most frequent categorical values in each of iterations of the clustering process. These modifications guarantee that the clustering process converges to a local minimal result. The number of modes will be equal to the number of clusters  required, since they act as centroids. The dissimilarity metric used for K-Modes is the Hamming distance from information theory which can be seen in Fig. 25. Here, x and y are the values of attribute j in object X and Y. The larger the number of mismatches of categorical values between X and Y is, the more dissimilar the two objects. In case of categorical dataset, the mode of an attribute is either â€œ1â€ or â€œ0,â€ whichever is more common in the cluster. The mode vector of a cluster minimizes the sum of the distances between each object in the cluster and the cluster centerThe K-Modes clustering process consists of the following steps:

![Fig. 18: Hamming Distance](https://datasciencedojo.com/wp-content/uploads/k-mode-hammer-distance.png)

[Python Example](https://pypi.org/project/kmodes/)

### Fuzzy K-Modes

The Fuzzy K-Modes clustering algorithm is an extension to K-Modes. Instead of assigning each object to one cluster, the Fuzzy K-Modes clustering algorithm calculates a cluster membership degree value for each object to each cluster. Similar to the Fuzzy K-Means, this is achieved by introducing the fuzziness factor in the objective function.The Fuzzy K-Modes clustering algorithm has found new applications in bioinformatics. It can improve the clustering result whenever the inherent clusters overlap in a data set.

- [Python Example](https://github.com/medhini/Genetic-Algorithm-Fuzzy-K-Modes)

### Fuzzy C-Means

Fuzzy C-Means is a probabilistic version of K-Means clustering. It associates all data points to all clusters such that the sum of all the associations is 1. The impact is that all clusters have a continuous (as opposed to discrete as in K-Means) association to each cluster relative to each other cluster.The algorithm iteratively assigns and computes the centroids of the clusters the same as K-Means till either criterion function is optimized of the convergence falls below a predetermined threshold value.The advantages of this algorithm are that it is not stringent like K-Means in assigning and works well for over lapping datasets. However it has the same disadvantage as K-Means of having a prior assumption of the number of clusters. Also, a low threshold value gives better results but is more computationally costly.

- [Python Example](https://pythonhosted.org/scikit-fuzzy/auto_examples/plot_cmeans.html)

### Mini Batch K-Means Clustering

Mini Batch K-Means uses a random subset of the entire data set to perform the K-Means algorithm. The provides the benefit of saving computational power and memory requirements are reduced, thus saving hardware costs or time (or a combination of both).There is, however, a loss in overall quality, but an extensive study as shows that the loss in quality is not substantial.

![Fig. 19: Difference between K-Means and Mini-Batch Graph](https://media.geeksforgeeks.org/wp-content/uploads/20190510070216/8fe10fb0-438d-4706-8fb0-f3ae95f35652.png)

- [Python Example](https://www.geeksforgeeks.org/ml-mini-batch-k-means-clustering-algorithm/)



### Hierarchical Clustering

Hierarchical Clustering uses the approach of finding groups in the data such that the instances are more similar to each other than to instances in other groups. This measure of similarity is generally a Euclidean distance between the data points, but Citi-block and Geodesic distances can also be used.The data is broken down into clusters in a hierarchical fashion. The number of clusters is 0 at the top and maximum at the bottom. The optimum number of clusters is selected from this hierarchy.

- â¼€ç¨®æ§‹å»º cluster çš„å±¤æ¬¡çµæ§‹çš„ç®—æ³•ã€‚è©²ç®—æ³•å¾åˆ†é…çµ¦â¾ƒâ¼° cluster çš„æ‰€æœ‰è³‡æ–™é»é–‹å§‹ã€‚ç„¶å¾Œï¼Œå…©å€‹è·é›¢æœ€è¿‘çš„ cluster åˆä½µç‚ºåŒâ¼€å€‹ clusterã€‚æœ€å¾Œï¼Œç•¶åªå‰©ä¸‹â¼€å€‹ cluster æ™‚ï¼Œè©²ç®—æ³•çµæŸã€‚
- K-means vs. éšå±¤åˆ†ç¾¤
  - K-mean è¦é å…ˆå®šç¾©ç¾¤æ•¸(n of clusters)
  - éšå±¤åˆ†ç¾¤å¯æ ¹æ“šå®šç¾©è·é›¢ä¾†åˆ†ç¾¤(bottom-up)ï¼Œä¹Ÿå¯ä»¥æ±ºå®šç¾£æ•¸åšåˆ†ç¾¤ (top-down)
- ç®—æ³•æµç¨‹

  1. Make each data point a single point cluster

2. Take the two closest data points and make them one cluster
  3. Take the two closest clusters and make them one cluster
4. Repeat STEP3 until there is only one cluster

- è·é›¢è¨ˆç®—æ–¹å¼

  - Single-linkï¼šä¸åŒç¾¤èšä¸­æœ€æ¥è¿‘å…©é»é–“çš„è·é›¢ã€‚
  - Complete-linkï¼šä¸åŒç¾¤èšä¸­æœ€é å…©é»é–“çš„è·é›¢ï¼Œé€™æ¨£å¯ä»¥ä¿è­‰é€™å…©å€‹é›†åˆåˆä½µå¾Œ, ä»»ä½•â¼€å°çš„è·é›¢ä¸æœƒâ¼¤æ–¼ dã€‚
  - Average-linkï¼šä¸åŒç¾¤èšé–“å„é»èˆ‡å„é»é–“è·é›¢ç¸½å’Œçš„å¹³å‡ã€‚
  - Centroidï¼šè¨ˆç®—ä¸åŒç¾¤ä¸­å¿ƒé»çš„è·é›¢
- æœ€ä½³çµ„æ•¸çš„é¸æ“‡æ–¹å¼
  - Dendrogramsï¼šå…ˆå°‡ç·šçš„é•·åº¦åˆ†å‰²æˆä¸å¯åˆ†å‰²çš„æœ€å°è·é›¢ï¼Œå†å¾ä¸­å–æœ€å¤§è·é›¢çš„åˆ‡åˆ†é»ä½œç‚ºæœ€ä½³åˆ†å‰²çµ„æ•¸
- éšå±¤åˆ†ç¾¤å„ªåŠ£åˆ†æ
- å„ªé»ï¼š
  1. æ¦‚å¿µç°¡å–®ï¼Œæ˜“æ–¼å‘ˆç¾
  2. ä¸éœ€æŒ‡å®šç¾¤æ•¸

- ç¼ºé»ï¼š
  1. åªé©â½¤æ–¼å°‘é‡è³‡æ–™ï¼Œâ¼¤é‡è³‡æ–™æœƒå¾ˆé›£è™•ç†

- åƒè€ƒè³‡æ–™
  - [Hierarchical Clustering / Dendrograms](https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Hierarchical_Clustering-Dendrograms.pdf)
  - [R Example](https://uc-r.github.io/hc_clustering)

### Expectation Maximization

Expectation Maximization uses a Maximum Likelihood Estimate system and is a three step procedure. The first step is Estimation - to conjecture parameters and a probability distribution for the data. The next step is to feed data into the model. The 3rd step is Maximization - to tweak the parameters of the model to include the new data. These three steps are repeated iteratively to improve the model.

- [R Example](http://rstudio-pubs-static.s3.amazonaws.com/154174_78c021bc71ab42f8add0b2966938a3b8.html)

### DBSCAN

DBSCAN stands for Density-based spatial clustering of applications with noise. Points that are a x distance from each other are a dense region and form a set of core points. Points that are x distance from each other, both core and non-core, form a cluster. Points that are not reachable from any core points are noise points.Density-Based Spatial Clustering of Applications with Noise is a density based clustering algorithm which identifies dense regions in the data as clusters. Dense regions are defined as areas in which points are reachable by each other. The algorithm uses two parameters, epsilon, and minimum points.Two data points are within reach of each other if their distance is less than epsilon. A cluster also needs to have a minimum number of points to be considered a cluster. Points which have the minimum number of points within epsilon distance are called core points.Points that are not reachable by any cluster are Noise points.DBSCAN's density based design makes it robust to outliers. However, it does not work well when working with clusters of varying density.

- [Python Example](https://medium.com/@elutins/dbscan-what-is-it-when-to-use-it-how-to-use-it-8bd506293818)

### Minimum Spanning Trees

The minimum spanning tree clustering algorithm is capable of detecting clusters with irregular boundaries. The MST based clustering method can identify clusters of arbitrary shape by removing inconsistent edges. The clustering algorithm constructs MST using Kruskal algorithm and then sets a threshold value and step size. It then removes those edges  from the MST, whose lengths are greater than the threshold value. A ratio between the intra-cluster distance and inter-cluster distance is calculated. Then, the threshold value is updated by incrementing the step size. At each new threshold value, the steps are repeated. The algorithm stops when no more edges can be removed from the tree. At this point, the minimum  value of the ratio can be checked and the clusters can be formed corresponding to the threshold value.MST searches for that optimum value of the threshold for which the Intra and Inter distance ratio is minimum. Generally, MST comparatively performs better than the k-Means algorithm for clustering.

- [Python Tutorial](https://slicematrix.github.io/mst_stock_market.html)

### Quality Threshold

Quality Threshold uses a minimum distance a point has to be away from a cluster to be a member and a minimum number of points for each cluster. Points are assigned clusters till the point and the cluster qualify these two criteria. Thus the first cluster is made and the process is repeated on the points which were not within distance and beyond the minimum number to form another cluster.The advantage of this algorithm is that quality of clusters is guaranteed and unlike K-Means the number of clusters does not have to be fixed apriori. The approach is also exhaustive and candidate clusters for all data points are considered.The exhaustive approach has the disadvantage of being computationally intense and time consuming. There is also the requirement of selecting the distance and minimum number apriori.

- [Python Example](https://github.com/melvrl13/python-quality-threshold/blob/master/QT.py)

### Gaussian Mixture Model (GMM)

A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. In this approach we describe each cluster by its centroid (mean), covariance , and the size of the cluster(Weight). All the instances generated from a single Gaussian distribution form a cluster where each cluster can have a different shape, size, density and orientation.GMMs have been used for feature extraction from speech data and have also been used extensively in object tracking of multiple objects. The parameters for Gaussian mixture models are derived either from maximum a posteriori estimation or an iterative expectation-maximization algorithm from a prior model which is well trained.

- [R Tutorial](http://tinyheero.github.io/2015/10/13/mixture-model.html)

### Spectral Clustering

Spectral clustering has become a promising alternative to traditional clustering algorithms due to its simple implementation and promising performance in many graph-based clustering. The goal of spectral clustering is to cluster data that is connected but not necessarily compact or clustered within convex boundaries. This algorithm relies on the power of graphs and the proximity between the data points in order to cluster them. This makes it possible to avoid the sphere shape cluster that the K-Means algorithm forces us to assume. As a result, spectral clustering usually outperforms K-Means algorithm.In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance, when clusters are nested circles on the 2D plane.Spectral Clustering requires the number of clusters to be specified. It works well for a small number of clusters but is not advised when using many clusters.

- [Python Tutorial](https://medium.com/@tomernahshon/spectral-clustering-from-scratch-38c68968eae0)

## Ensemble learning

Ensemble learning methods are meta-algorithms that combine several machine learning methods into a single predictive model to increase the overall performance. 

### Random Forest

A random forest is comprised of a set of decision trees, each of which is trained on a random subset of the training data. These trees predictions can then be aggregated to provide a single prediction from a series of predictions.To build a random forest, you need to choose the total number of trees and the number of samples for each individual tree. Later, for each tree, the set number of samples with replacement and features are selected to train the decision tree using this data.The outputs from all the seperate models are aggregated into a single prediction as part of the final model. In terms of regression, the output is simply the average of predicted outcome values. In terms of classification, the category with the highest frequency output is chosen.The bootstrapping and feature bagging process outputs varieties of different decision trees rather than just a single tree applied to all of the data.Using this approach, the models that were trained without some features will be able to make predictions in aggregated models even with missing data. Moreover, each model trained with different subsets of data will be able to make decisions based on different structure of the underlysing data/population. Hence, in aggregated model they will be able to make prediction even when the training data doesnâ€™t look exactly like what weâ€™re trying to predict. 

- æ±ºç­–æ¨¹çš„ç¼ºé»

  - è‹¥ä¸å°æ±ºç­–æ¨¹é€²â¾é™åˆ¶ (æ¨¹æ·±åº¦ã€è‘‰â¼¦ä¸Šâ¾„å°‘è¦æœ‰å¤šå°‘æ¨£æœ¬ç­‰)ï¼Œæ±ºç­–æ¨¹éå¸¸å®¹æ˜“ Overfitting
  - ç‚ºäº†è§£æ±ºæ±ºç­–æ¨¹çš„ç¼ºé»ï¼Œå¾ŒçºŒç™¼å±•å‡ºäº†éš¨æ©Ÿæ£®æ—çš„æ¦‚å¿µï¼Œä»¥æ±ºç­–æ¨¹ç‚ºåŸºåº•å»¶ä¼¸å‡ºçš„æ¨¡å‹

- é›†æˆæ¨¡å‹

  - é›†æˆ (Ensemble) æ˜¯å°‡å¤šå€‹æ¨¡å‹çš„çµæœçµ„åˆåœ¨â¼€èµ·ï¼Œé€éæŠ•ç¥¨æˆ–æ˜¯åŠ æ¬Šçš„â½…å¼å¾—åˆ°æœ€çµ‚çµæœ
  - é€éå¤šæ£µè¤‡é›œçš„æ±ºç­–æ¨¹ä¾†æŠ•ç¥¨å¾—åˆ°çµæœï¼Œç·©è§£åŸæœ¬æ±ºç­–æ¨¹å®¹æ˜“éæ“¬å’Œçš„å•é¡Œï¼Œå¯¦å‹™ä¸Šçš„çµæœé€šå¸¸éƒ½æœƒæ¯”æ±ºç­–æ¨¹ä¾†å¾—å¥½

- éš¨æ©Ÿæ£®æ— (Random Forest), éš¨æ©Ÿåœ¨å“ªï¼Ÿ

  - è¨“ç·´æ¨£æœ¬é¸æ“‡æ–¹é¢çš„ Bootstrapæ–¹æ³•éš¨æ©Ÿé¸æ“‡å­æ¨£æœ¬
  - ç‰¹å¾µé¸æ“‡æ–¹é¢éš¨æ©Ÿé¸æ“‡ k å€‹å±¬æ€§ï¼Œæ¯å€‹æ¨¹ç¯€é»åˆ†è£‚æ™‚ï¼Œå¾é€™éš¨æ©Ÿçš„ k å€‹å±¬æ€§ï¼Œé¸æ“‡æœ€å„ªçš„ã€‚
  - éš¨æ©Ÿæ£®æ—æ˜¯å€‹é›†æˆæ¨¡å‹ï¼Œé€éå¤šæ£µè¤‡é›œçš„æ±ºç­–æ¨¹ä¾†æŠ•ç¥¨å¾—åˆ°çµæœï¼Œç·©è§£åŸæœ¬æ±ºç­–æ¨¹å®¹æ˜“éæ“¬å’Œçš„å•é¡Œã€‚

- è¨“ç·´æµç¨‹

  1. å¾åŸå§‹è¨“ç·´é›†ä¸­ä½¿ç”¨bootstrapæ–¹æ³•éš¨æ©Ÿæœ‰æ”¾å›æ¡æ¨£é¸å‡º m å€‹æ¨£æœ¬ï¼Œèˆ‡m2 å€‹ columnï¼Œå…±é€²è¡Œ n_tree æ¬¡æ¡æ¨£ï¼Œç”Ÿæˆ n_tree å€‹è¨“ç·´é›†

  2. å°æ–¼ n_tree å€‹è¨“ç·´é›†ï¼Œæˆ‘å€‘åˆ†åˆ¥è¨“ç·´ n_tree å€‹æ±ºç­–æ¨¹æ¨¡å‹

  3. å°æ–¼å–®å€‹æ±ºç­–æ¨¹æ¨¡å‹ï¼Œå‡è¨­è¨“ç·´æ¨£æœ¬ç‰¹å¾µçš„å€‹æ•¸ç‚º n_treeï¼Œé‚£éº¼æ¯æ¬¡åˆ†è£‚æ™‚æ ¹æ“šè³‡è¨Šå¢ç›Š/è³‡è¨Šå¢ç›Šæ¯”/åŸºå°¼æŒ‡æ•¸é¸æ“‡æœ€å¥½çš„ç‰¹å¾µé€²è¡Œåˆ†è£‚

  4. æ¯æ£µæ¨¹éƒ½ä¸€ç›´é€™æ¨£åˆ†è£‚ä¸‹å»ï¼Œç›´åˆ°è©²ç¯€é»çš„æ‰€æœ‰è¨“ç·´æ¨£ä¾‹éƒ½å±¬æ–¼åŒä¸€é¡ã€‚åœ¨æ±ºç­–æ¨¹çš„åˆ†è£‚éç¨‹ä¸­ä¸éœ€è¦å‰ªæ

  5. å°‡ç”Ÿæˆçš„å¤šæ£µæ±ºç­–æ¨¹çµ„æˆéš¨æ©Ÿæ£®æ—ã€‚

     - å°æ–¼åˆ†é¡å•é¡Œï¼ŒæŒ‰å¤šæ£µæ¨¹åˆ†é¡å™¨æŠ•ç¥¨æ±ºå®šæœ€çµ‚åˆ†é¡çµæœ
     - å°æ–¼å›æ­¸å•é¡Œï¼Œç”±å¤šæ£µæ¨¹é æ¸¬å€¼çš„å‡å€¼æ±ºå®šæœ€çµ‚é æ¸¬çµæœ

- ä½¿â½¤ Sklearn ä¸­çš„éš¨æ©Ÿæ£®æ—

  ```python
  from sklearn.ensemble import RandomForestRegressor
  reg = RandomForestRegressor()
  from sklearn.ensemble import RandomForestClassifier
  clf = RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 0)
  clf.fit(X_train, y_train)
  ```

  - n_estimators:æ±ºç­–æ¨¹çš„æ•¸é‡
    - max_features:å¦‚ä½•é¸å– features

- Ref:

  - [éš¨æ©Ÿæ£®æ—ï¼ˆRandom forest,RFï¼‰çš„ç”Ÿæˆæ–¹æ³•ä»¥åŠå„ªç¼ºé»](https://www.itread01.com/content/1547100921.html)

### Bagging (Bootstrap Aggregation)

Bagging (Bootstrap Aggregation) is used when we want to reduce the variance (over fitting) of a decision tree. Bagging comprises of the following steps:Bootstrap SamplingSeveral subsets of data can be obtained from the training data chosen randomly with replacement. This collection of data will be used to train decision trees. Bagging will construct n decision trees using bootstrap sampling of the training data. As a result, we will have an ensemble of different models at the end.AggregationThe outputs from all the seperate models are aggregated into a single prediction as part of the final model. In terms of regression, the output is simply the average of predicted outcome values. In terms of classification, the category with the highest frequency output is chosen. Unlike boosting, bagging involves the training a bunch of individual models in a parallel way. The advantage of using Bootstrap aggregation is that it allows the variance of the model to be reduced by averaging multiple estimates that are measured from random samples of a population data.

- [R Example](http://rpubs.com/kangrinboqe/268745)

### AdaBoost

AdaBoost is an iterative ensemble method. It builds a strong classifier by combining multiple weak performing classifiers.The final classifier is the weighted combination of several weak classifiers. It fits a sequence of weak learners on different weighted training data. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy. You can see this process represented in the AdaBoost Figure.Initially, AdaBoost selects a training subset randomly and gives equal weight to each observation. If prediction is incorrect using the first learner then it gives higher weight to observation which have been predicted incorrectly. The model is iteratively training by selecting the training set based on the accurate prediction of the last training. Being an iterative process, the model continues to add multiple learners until a limit is reached in the number of models or accuracy.It is possible to use any base classifier with AdaBoost. This algorithm is not prone to overfitting. AdaBoost is easy to implement. One of the downsides of AdaBoost is that it is highly affected by outliers because it tries to fit each point perfectly. It is computationally slower as compared to XGBoost. You can use it both for classification and regression problem. 

![Fig. 17: AdaBoost](https://datasciencedojo.com/wp-content/uploads/adaboost.png)

- [R Tutorial](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)

### Gradient Boosting

Gradient boosting is a method in which we re-imagine the boosting problem as an optimisation problem, where we take up a loss function and try to optimise it.Gradient boosting involves 3 core elements: a weak learner to make predictions, a loss function to be optimized, and an additive model to add to the weak learners to minimize the loss function.This algorithm trains various models sequentially. Decision trees are used as the base weak learner in gradient boosting. Trees are added one at a time, and existing trees in the model are not changed. Each new tree helps to correct errors made by previously trained tree. A gradient descent procedure is used to minimize the loss when adding trees. After calculating error or loss, the parameters of the tree are modified to minimize that error. Gradient Boosting often provides predictive accuracy that cannot be surpassed. These machines can optimize different loss functions depending on the problem type which makes it felxible. There is no data pre-processing required as it also handles missing data.One of the applications of Gradient Boosting Machine is anomaly detection in supervised learning settings where data is often highly unbalanced such as DNA sequences, credit card transactions or cyber security. One of the drawbacks of GBMs is that they are more sensitive to overfitting if the data is noisy and are also computationally expensive which can be time and memory exhaustive.

- éš¨æ©Ÿæ£®æ—ä½¿â½¤çš„é›†æˆâ½…æ³•ç¨±ç‚º Bagging (Bootstrap aggregating)ï¼Œâ½¤æŠ½æ¨£çš„è³‡æ–™èˆ‡ features â½£æˆæ¯â¼€æ£µæ¨¹ï¼Œæœ€å¾Œå†å–å¹³å‡

- è¨“ç·´æµç¨‹

  1. å°‡è¨“ç·´è³‡æ–™é›†ä¸­çš„æ¯å€‹æ¨£æœ¬è³¦äºˆä¸€å€‹æ¬Šå€¼ï¼Œé–‹å§‹çš„æ™‚å€™ï¼Œæ¬Šé‡éƒ½åˆå§‹åŒ–ç‚ºç›¸ç­‰å€¼
  2. åœ¨æ•´å€‹è³‡æ–™é›†ä¸Šè¨“ç·´ä¸€å€‹å¼±åˆ†é¡å™¨ï¼Œä¸¦è¨ˆç®—éŒ¯èª¤ç‡
  3. åœ¨åŒä¸€å€‹è³‡æ–™é›†ä¸Šå†æ¬¡è¨“ç·´ä¸€å€‹å¼±åˆ†é¡å™¨ï¼Œåœ¨è¨“ç·´çš„éç¨‹ä¸­ï¼Œæ¬Šå€¼é‡æ–°èª¿æ•´ï¼Œå…¶ä¸­åœ¨ä¸Šä¸€æ¬¡åˆ†é¡ä¸­åˆ†å°çš„æ¨£æœ¬æ¬Šå€¼å°‡æœƒé™ä½ï¼Œåˆ†éŒ¯çš„æ¨£æœ¬æ¬Šå€¼å°‡æœƒæé«˜
  4. é‡è¤‡ä¸Šè¿°éç¨‹ï¼Œä¸²åˆ—çš„ç”Ÿæˆå¤šå€‹åˆ†é¡å™¨ï¼Œç‚ºäº†å¾æ‰€æœ‰å¼±åˆ†é¡å™¨ä¸­å¾—åˆ°å¤šå€‹åˆ†é¡çµæœ
  5. åè¦†é‹ç®—å®Œæˆå¾Œï¼Œæœ€å¾Œçš„åˆ†é¡å™¨æ˜¯ç”±åè¦†é‹ç®—éç¨‹ä¸­é¸æ“‡çš„å¼±åˆ†é¡å™¨ç·šæ€§åŠ æ¬Šå¾—åˆ°çš„

- Boosting å‰‡æ˜¯å¦â¼€ç¨®é›†æˆâ½…æ³•ï¼Œå¸Œæœ›èƒ½å¤ ç”±å¾Œâ¾¯â½£æˆçš„æ¨¹ï¼Œä¾†ä¿®æ­£å‰â¾¯æ¨¹å­¸ä¸å¥½çš„åœ°â½…

- è¦æ€éº¼ä¿®æ­£å‰â¾¯å­¸éŒ¯çš„åœ°â½…å‘¢ï¼Ÿè¨ˆç®— Gradient!

- æ¯æ¬¡â½£æˆæ¨¹éƒ½æ˜¯è¦ä¿®æ­£å‰â¾¯æ¨¹é æ¸¬çš„éŒ¯èª¤ï¼Œä¸¦ä¹˜ä¸Š learning rate è®“å¾Œâ¾¯çš„æ¨¹èƒ½æœ‰æ›´å¤šå­¸ç¿’çš„ç©ºé–“ï¼Œç·©è§£åŸæœ¬æ±ºç­–æ¨¹å®¹æ˜“éæ“¬å’Œçš„å•é¡Œï¼Œå¯¦å‹™ä¸Šçš„çµæœé€šå¸¸ä¹Ÿæœƒæ¯”æ±ºç­–æ¨¹ä¾†å¾—å¥½

- Bagging èˆ‡ Boosting çš„å·®åˆ¥

  - æ¨£æœ¬é¸æ“‡ä¸Š

    - Baggingï¼šè¨“ç·´é›†æ˜¯åœ¨åŸå§‹é›†ä¸­æœ‰æ”¾å›é¸å–çš„ï¼Œå¾åŸå§‹é›†ä¸­é¸å‡ºçš„å„è¼ªè¨“ç·´é›†ä¹‹é–“æ˜¯ç¨ç«‹çš„ã€‚
    - Boostingï¼šæ¯ä¸€è¼ªçš„è¨“ç·´é›†ä¸è®Šï¼Œåªæ˜¯è¨“ç·´é›†ä¸­æ¯å€‹æ¨£ä¾‹åœ¨åˆ†é¡å™¨ä¸­çš„æ¬Šé‡ç™¼ç”Ÿè®ŠåŒ–ã€‚è€Œæ¬Šå€¼æ˜¯æ ¹æ“šä¸Šä¸€è¼ªçš„åˆ†é¡çµæœé€²è¡Œèª¿æ•´ã€‚

  - æ¨£ä¾‹æ¬Šé‡

    - Baggingï¼šä½¿ç”¨å‡å‹»å–æ¨£ï¼Œæ¯å€‹æ¨£ä¾‹çš„æ¬Šé‡ç›¸ç­‰ã€‚ 
    - Boostingï¼šæ ¹æ“šéŒ¯èª¤ç‡ä¸æ–·èª¿æ•´æ¨£ä¾‹çš„æ¬Šå€¼ï¼ŒéŒ¯èª¤ç‡è¶Šå¤§å‰‡æ¬Šé‡è¶Šå¤§ã€‚

  - é æ¸¬å‡½æ•¸

    - Baggingï¼šæ‰€æœ‰é æ¸¬å‡½æ•¸çš„æ¬Šé‡ç›¸ç­‰ã€‚
    - Boostingï¼šæ¯å€‹å¼±åˆ†é¡å™¨éƒ½æœ‰ç›¸æ‡‰çš„æ¬Šé‡ï¼Œå°æ–¼åˆ†é¡èª¤å·®å°çš„åˆ†é¡å™¨æœƒæœ‰æ›´å¤§çš„æ¬Šé‡ã€‚

  - ä½¿ç”¨æ™‚æ©Ÿ

    - Baggingï¼šæ¨¡å‹æœ¬èº«å·²ç¶“å¾ˆè¤‡é›œï¼Œä¸€ä¸‹å°±Overfitäº†ï¼Œéœ€è¦é™ä½è¤‡é›œåº¦æ™‚
    - Boosting:æ¨¡å‹ç„¡æ³•fitè³‡æ–™æ™‚ï¼Œé€éBoostingä¾†å¢åŠ æ¨¡å‹çš„è¤‡é›œåº¦

  - ä¸»è¦ç›®æ¨™ï¼š

    - Baggingï¼šé™ä½Variance
    - Boostingï¼šé™ä½bias

  - å¹³è¡Œè¨ˆç®—ï¼š Baggingï¼šå„å€‹é æ¸¬å‡½æ•¸å¯ä»¥ä¸¦è¡Œç”Ÿæˆã€‚ Boostingï¼šå„å€‹é æ¸¬å‡½æ•¸åªèƒ½é †åºç”Ÿæˆï¼Œå› ç‚ºå¾Œä¸€å€‹æ¨¡å‹åƒæ•¸éœ€è¦å‰ä¸€è¼ªæ¨¡å‹çš„çµæœã€‚

  - ä½¿â½¤ Sklearn ä¸­çš„æ¢¯åº¦æå‡æ©Ÿ

    ```python
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.ensemble import GradientBoostingRegressor
    clf = GradientBoostingClassifier()
    ```

  - å¯æ±ºå®šè¦â½£æˆæ•¸çš„æ•¸é‡ï¼Œè¶Šå¤šè¶Šä¸å®¹æ˜“éæ“¬å’Œï¼Œä½†æ˜¯é‹ç®—æ™‚é–“æœƒè®Šé•·

  - Loss çš„é¸æ“‡ï¼Œè‹¥æ”¹ç‚º exponential å‰‡æœƒè®ŠæˆAdaboosting æ¼”ç®—æ³•ï¼Œæ¦‚å¿µç›¸åŒä½†å¯¦ä½œç¨å¾®ä¸åŒ

  - learning_rateæ˜¯æ¯æ£µæ¨¹å°æœ€çµ‚çµæœçš„å½±éŸ¿ï¼Œæ‡‰èˆ‡ï¼Œn_estimators æˆåæ¯”

  - n_estimators: æ±ºç­–æ¨¹çš„æ•¸é‡

[ç´”ä¹¾è²¨ï½œæ©Ÿå™¨å­¸ç¿’ä¸­æ¢¯åº¦ä¸‹é™æ³•çš„åˆ†é¡åŠå°æ¯”åˆ†æï¼ˆé™„æºç¢¼ï¼‰](https://kknews.cc/tech/mmr8kag.html)

- åƒè€ƒè³‡æ–™
  - [æ©Ÿå™¨/æ·±åº¦å­¸ç¿’-åŸºç¤æ•¸å­¸(äºŒ):æ¢¯åº¦ä¸‹é™æ³•(gradient descent)](https://medium.com/@chih.sheng.huang821/æ©Ÿå™¨å­¸ç¿’-åŸºç¤æ•¸å­¸-äºŒ-æ¢¯åº¦ä¸‹é™æ³•-gradient-descent-406e1fd001f)
  - [GBDTè°ƒä¼˜è¯¦è§£](https://7125messi.github.io/post/gbdt%E8%B0%83%E4%BC%98%E8%AF%A6%E8%A7%A3/)
  - [R Tutorial](https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab)

### Gradient Boosted Regression Trees

Gradient Boosted Regression Trees (GBRT) are a flexible, non-parametric learning technique for classification and regression, and are one of the most effective machine learning models for predictive analytics. Boosted regression trees combine the strengths of two algorithms which include regression trees and boosting methods. Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. 

- [Python Example](https://scikit-learn.org/stable/modules/ensemble.html)

### XGBoost(Extreme Gradient Boosting)

"XGBoost is similar to gradient boosting framework but it improves upon the base GBM architechture by using system optimization and algorithmic improvements.System optimizations:
Parallelization: It executes the sequential tree building using parallelized implementation. 
Hardware: It uses the hardware resources efficiently by allocating internal buffers in each thread to store gradient statistics.Tree Pruning: XGBoost uses â€˜max_depthâ€™ parameter instead of criterion first, and starts pruning trees backward. This â€˜depth-firstâ€™ approach improves computational performance significantly.Algorithmic Improvements:
Regularization: It penalizes more complex models through both LASSO (L1) and Ridge (L2) regularization to prevent overfitting.Sparsity Awareness: Handles different types of sparsity patterns in the data more efficiently.Cross-validation: The algorithm comes with built-in cross-validation method at each iteration, taking away the need to explicitly program this search and to specify the exact number of boosting iterations required in a single run.Due to it's computational complexity and ease of implementation, XGBoost is used widely over Gradient Boosting."

https://zhuanlan.zhihu.com/p/31182879

- ç°¡ä»‹

  - XGBçš„å»ºç«‹åœ¨GBDTçš„åŸºç¤ä¸Š,ç¶“éç›®æ¨™å‡½æ•¸ã€æ¨¡å‹æ¼”ç®—æ³•ã€é‹ç®—æ¶æ§‹ç­‰ç­‰çš„å„ªåŒ–,ä½¿XGBæˆç‚ºé€Ÿåº¦å¿«ã€æ•ˆæœå¥½çš„Boostingæ¨¡å‹

    - ç›®æ¨™å‡½æ•¸çš„å„ªåŒ–:

      æ¨¡å‹çš„é€šå‰‡æ˜¯è¿½æ±‚ç›®æ¨™å‡½æ•¸çš„ã€Œæ¥µå°åŒ–ã€,å…¶ä¸­æå¤±å‡½æ•¸æœƒéš¨æ¨¡å‹è¤‡é›œåº¦å¢åŠ è€Œæ¸›å°‘,è€ŒXGBå°‡æ¨¡å‹çš„ç›®æ¨™å‡½æ•¸åŠ å…¥æ­£å‰‡åŒ–é …,å…¶å°‡éš¨æ¨¡å‹è¤‡é›œåº¦å¢åŠ è€Œå¢åŠ ,æ•…XGBæœƒåœ¨æ¨¡å‹æº–ç¢ºåº¦å’Œæ¨¡å‹è¤‡é›œåº¦é–“å–æ¨(trade-off),é¿å…ç‚ºäº†è¿½æ±‚æº–ç¢ºåº¦å°è‡´æ¨¡å‹éæ–¼è¤‡é›œ,é€ æˆoverfitting

- è¨“ç·´æµç¨‹

  ```python
  from xgboost import XGBClassifier
  classifier = XGBClassifier()
  classifier.fit(X_train, y_train)
  ```

  

- èª¿åƒé †åº

  1. è¨­ç½®ä¸€äº›åˆå§‹å€¼ã€‚

     ```python
     - learning_rate: 0.1
     - n_estimators: 500
     - max_depth: 5
     - min_child_weight: 1
     - subsample: 0.8
     - colsample_bytree:0.8
     - gamma: 0
     - reg_alpha: 0
     - reg_lambda: 1
     ```

  2. estimdators

  3. min_child_weight åŠ max_depth

  4. gamma

  5. subsample åŠ colsample_bytree

  6. reg_alpha åŠ reg_lambda

  7. learning_rateï¼Œ é€™æ™‚å€™è¦èª¿å°æ¸¬è©¦

- Ref
  - [R Tutorial](https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/)

### Voting Classifier

A voting classifier combines the results of several classifiers to predict the class labels. It is one of the simplest ensemble methods. The voting classifier usually achieves better results than the best classifier in the ensemble. A hard-voting classifier uses the majority vote to predict the class labels. Whereas, a soft-voting classifier will use the average predicted probabilities to predict the labels, however, this can only be possible if all individual classifiers can predict class probabilities.The voting classifier can balance out the individual weakness of each classifier used. It will be beneficial to include diverse classifiers so that models which fall prey to similar types of errors do not aggregate the errors. As an example, one can train a logistic regression, a random forest classifier a naÃ¯ve bayes classifier and a support vector classifier. To predict the label, the class that receives the highest number of votes from all of the 4 classifiers will be the predicted class of the ensemble (Voting classifier).

- [Python Tutorial](http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/)

### Extremely Randomized Trees

Extremely Randomized Trees (also known as Extra-Trees) increases the randomness of Random Forest algorithms and moves a step further. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminating thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule.This trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. One can use it for both regression and classification.

- [R Example](https://daviddalpiaz.github.io/stat432sp18/lab/enslab/enslab.html)

### Boosted Decision Tree

Boosted Decision Trees are a collection of weak decision trees which are used in congregation to make a strong learner. The other decision trees are called weak because they have lesser ability than the full model and use a simpler model. Each weak decision tree is trained to address the error of the previous tree to finally come up with a robust model.

- [R Example](https://www.r-bloggers.com/gradient-boosting-in-r/)



### lightgbm

https://zhuanlan.zhihu.com/p/52583923

The LightGBM boosting algorithm is becoming more popular by the day due to its speed and efficiency. LightGBM is able to handle huge amounts of data with ease. But keep in mind that this algorithm does not perform well with a small number of data points.

### Category Boosting (CatBoost)

CatBoost is a fast, scalable, high performance algorithm for gradient boosting on decision trees. It can work with diverse data types to help solve a wide range of problems that businesses face today. Catboost achieves the best results on the benchmark.Catboost is built with a similar approach and attributes as with Gradient Boost Decision Tree models. The feature that separates CatBoost algorithm from rest is its unbiased boosting with categorical variables. Its power lies in its categorical features preprocessing, prediction time and model analysis.Catboost introduces two critical algorithmic advances - the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features.CatBoost handles data very efficiently, few tweaks can be made to increase efficiency like choosing the mode according to data. However, Catboostâ€™s training and optimization times is considerably high.

- [R Tutorial](https://www.kaggle.com/slickwilly/simple-catboost-in-r)


As the name suggests, CatBoost is a boosting algorithm that can handle categorical variables in the data. Most [machine learning algorithms](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?utm_source=blog&utm_medium=4-boosting-algorithms-machine-learning) cannot work with strings or categories in the data. Thus, converting categorical variables into numerical values is an essential preprocessing step.

CatBoost can internally handle categorical variables in the data. These variables are transformed to numerical ones using various statistics on combinations of features.

If you want to understand the math behind how these categories are converted into numbers, you can go through this article:

- [Transforming categorical features to numerical features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html#algorithm-main-stages_cat-to-numberic)

## Evaluation Method

https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/

æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ä¸­çš„â½¬æ¨™å‡½æ•¸

- æ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„â½¬æ¨™å‡½æ•¸ä¸­æœ‰å…©å€‹éå¸¸é‡è¦çš„å…ƒç´ 

  - æå¤±å‡½æ•¸ (Loss function)

    æå¤±å‡½æ•¸è¡¡é‡é æ¸¬å€¼èˆ‡å¯¦éš›å€¼çš„å·®ç•°ï¼Œè®“æ¨¡å‹èƒ½å¾€æ­£ç¢ºçš„â½…å‘å­¸ç¿’

  - æ­£å‰‡åŒ– (Regularization)

    - æ­£å‰‡åŒ–æ˜¯ç‚ºäº†è§£æ±ºéæ“¬åˆå•é¡Œï¼Œåˆ†ç‚º L1 å’Œ L2 æ­£å‰‡åŒ–ã€‚ä¸»è¦é€šéä¿®æ­£æå¤±å‡½æ•¸ï¼ŒåŠ å…¥æ¨¡å‹è¤‡é›œæ€§è©•ä¼°

    - æ­£å‰‡åŒ–æ˜¯ç¬¦åˆ**å¥§å¡å§†å‰ƒåˆ€åŸç†**ï¼šåœ¨æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹ä¸­ï¼Œèƒ½å¤ å¾ˆå¥½çš„è§£é‡‹å·²çŸ¥æ•¸æ“šä¸¦ä¸”ååˆ†ç°¡å–®çš„æ‰æ˜¯æœ€å¥½çš„æ¨¡å‹ã€‚

å®šç¾©â¼€å€‹â½¬æ¨™å‡½æ•¸ (Objective function) ä¹Ÿå¯ç¨±ä½œæå¤±å‡½æ•¸ (Loss function)ï¼Œä¾†è¡¡é‡æ¨¡å‹çš„å¥½å£ï¼ŒLoss è¶Šâ¼¤ï¼Œä»£è¡¨é€™çµ„åƒæ•¸çš„æ¨¡å‹é æ¸¬å‡ºçš„ Å· è¶Šä¸æº–ï¼Œä¹Ÿä»£è¡¨ä¸æ‡‰è©²é¸é€™çµ„åƒæ•¸çš„æ¨¡å‹

- **åˆ†é¡æ¨¡å‹**ï¼šè§€å¯Ÿã€Œé æ¸¬å€¼ã€ (prediction) èˆ‡ã€Œå¯¦éš›å€¼ã€ (Ground truth) çš„æ­£ç¢ºç¨‹åº¦

  - Accuracy
   - AUC, Area Under Curve

  - Precision: æ¨¡å‹åˆ¤å®šç‘•ç–µï¼Œæ¨£æœ¬ç¢ºå¯¦ç‚ºç‘•ç–µçš„æ¯”ä¾‹

  - Recall: æ¨¡å‹åˆ¤å®šçš„ç‘•ç–µï¼Œä½”æ¨£æœ¬æ‰€æœ‰ç‘•ç–µçš„æ¯”ä¾‹
  - F1 - Score (Precision, Recall), ç¯„åœ: [0, 1]

### å›æ­¸æ¨¡å‹

- è§€å¯Ÿã€Œé æ¸¬å€¼ã€ (Prediction) èˆ‡ã€Œå¯¦éš›å€¼ã€ (Ground truth) çš„å·®è·

  - MAE, Mean Absolute Error, ç¯„åœ: [-âˆ, âˆ]
    $$
    \frac{1}{m}\sum_{i=1}^m\vert (y_i-\hat y)\vert
    $$
    MSE, Mean Square Error, ç¯„åœ: [-âˆ, âˆ]
    $$
    \frac{1}{m}\sum_{i=1}^m(y_i-\hat y)^2
    $$

  - R-square, ç¯„åœ: [0, 1]

    

  - Adjust R-square

    - R^2æœƒéš¨è‘—è®Šæ•¸æ•¸é‡çš„å¢åŠ è€Œæå‡ï¼Œé€²è€Œå®¹æ˜“æœ‰Overfitçš„å•é¡Œï¼Œè€Œadjust R^2 å‰‡æœƒé‡å°è®Šæ•¸æ•¸é‡é€²è¡Œæ‡²ç½°ï¼Œå¯ä»¥å¹«åŠ©æˆ‘å€‘æ‰¾å‡ºæœ€åˆé©çš„è®Šæ•¸æ•¸é‡

    $$
    AdjR^2 = 1 - (1-R^2)\frac{n-1}{n-p-1}
    $$

    - p: number of independent variable
    - n: sample size

### åˆ†é¡æ¨¡å‹

- è§€å¯Ÿã€Œé æ¸¬å€¼ã€ (prediction) èˆ‡ã€Œå¯¦éš›å€¼ã€ (Ground truth) çš„æ­£ç¢ºç¨‹åº¦

  - æœƒé€éæ··æ·†çŸ©é™£ (Confusion Matrix)ä¾†è¡¡é‡æ¨¡å‹çš„æ•ˆåº¦

  - å› æ‡‰é æ¸¬èˆ‡å¯¦éš›çµæœçš„ä¸ä¸€è‡´ï¼Œæœƒç”¢ç”ŸTPï¼ŒTNï¼ŒFPï¼ŒFNç­‰4ç¨®æƒ…æ³

    (è‹±æ–‡çš„å‘½åå¯ä»¥å¾é æ¸¬çš„è§’åº¦ä¾†ç†è§£)

    ![](C:/Users/TLYu0419/Documents/Github/DataScience/images/confusion_matrix_1.png)

  

- è©•ä¼°æŒ‡æ¨™

  - Accuracyï¼š

    - Accuracy in classification problems is the **number of correct predictions** made by the model divided by the **total number of predictions.**

      $\frac{(TP + TN)}{Total Sample}$

    - Accuracy Paradox

      - æ¨£æœ¬æ¥µåº¦ä¸å¹³è¡¡æ™‚ï¼Œç›´æ¥å°‡æ‰€æœ‰æ¨£æœ¬é æ¸¬æˆå¤šæ•¸çš„é¡åˆ¥å³å¯ç²å¾—é«˜ Accuracy rate
      - å„ªé»ï¼šç›´è§€ï¼›ç¼ºé»ï¼šæ²’æœ‰è€ƒé‡ä¸åŒé¡å‹çŠ¯éŒ¯çš„æˆæœ¬å·®ç•°

  - Precisionï¼š å‰‡æ˜¯é‡å°æŸé¡åˆ¥é€²â¾è©•ä¼°

    - Ability of a classification model to identify **only** the relevant data points.
    - Precision is defined as the number of **true positives divided by the number of true positives plus the number of false positives.** 
    - Precision: æ¨¡å‹åˆ¤å®šç‘•ç–µï¼Œæ¨£æœ¬ç¢ºå¯¦ç‚ºç‘•ç–µçš„æ¯”ä¾‹

  - Recall

    - Ability of a model to find **all** the relevant cases within a dataset. 
    - The precise definition of recall is the **number of true positives divided by the number of true positives plus the number of false negatives.** 
    - Recall: æ¨¡å‹åˆ¤å®šçš„ç‘•ç–µï¼Œä½”æ¨£æœ¬æ‰€æœ‰ç‘•ç–µçš„æ¯”ä¾‹
      (ä»¥ç‘•ç–µæª¢æ¸¬ç‚ºä¾‹ï¼Œè‹¥ç‚º recall=1 å‰‡ä»£è¡¨æ‰€æœ‰ç‘•ç–µéƒ½è¢«æ‰¾åˆ°)
    - Often you have a trade-off between Recall and Precision.
    - While recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant actually were relevant.

  - F1 - Score (Precision, Recall), ç¯„åœ: [0, 1]

    - In cases where we want to find an optimal blend of precision and recall we can combine the two metrics using what is called the F1 score.

    - The F1 score is the harmonic mean of precision and recall taking both metrics into account in the following equation:

      $F_1=2*\frac{precision*recall}{precision+recall}$

    - We use the harmonic mean instead of a simple average because it punishes extreme values. 

    - A classifier with a precision of 1.0 and a recall of 0.0 has a simple average of 0.5 but an F1 score of 0. 

    - Precision and Recall typically make more sense in the context of a confusion matrix.

    - F1 æ˜¯ Precision, Recall çš„èª¿å’Œå¹³å‡æ•¸

    - åˆ†é¡å•é¡Œä¸­ï¼Œæˆ‘å€‘æœ‰æ™‚æœƒå°æŸâ¼€é¡åˆ¥çš„æº–ç¢ºç‡ç‰¹åˆ¥æœ‰èˆˆè¶£ã€‚ä¾‹å¦‚ç‘•ç–µ/æ­£å¸¸æ¨£æœ¬åˆ†é¡ï¼Œæˆ‘å€‘å¸Œæœ›ä»»ä½•ç‘•ç–µæ¨£æœ¬éƒ½ä¸èƒ½è¢«æ¼æ‰ã€‚

    - è¡ç”ŸæŒ‡æ¨™

      - F1-Scoreæ˜¯æŒ‡å‡†ç¡®ç‡å’Œå¬å›ç‡ä¸€æ ·é‡è¦ï¼›

      - F2-Scoreæ˜¯æŒ‡å¬å›ç‡æ¯”å‡†ç¡®ç‡é‡è¦ä¸€å€ï¼›

      - F0.5-Scoreæ˜¯æŒ‡å‡†ç¡®ç‡æ¯”å¬å›ç‡é‡è¦ä¸€å€ã€‚

  - AUC, Area Under Curve, ç¯„åœ: [0, 1]

    - AUC æŒ‡æ‘½æ˜¯åˆ†é¡å•é¡Œå¸¸â½¤çš„æŒ‡æ¨™ï¼Œé€šå¸¸åˆ†é¡å•é¡Œéƒ½éœ€è¦å®šâ¼€å€‹é–¾å€¼(threshold) ä¾†æ±ºå®šåˆ†é¡çš„é¡åˆ¥ (é€šå¸¸ç‚ºæ©Ÿç‡ > 0.5 åˆ¤å®šç‚º 1, æ©Ÿç‡ < 0.5 åˆ¤å®šç‚º 0)
    - AUC æ˜¯è¡¡é‡æ›²ç·šä¸‹çš„â¾¯ç©ï¼Œå› æ­¤å¯è€ƒé‡æ‰€æœ‰é–¾å€¼ä¸‹çš„æº–ç¢ºæ€§ï¼Œå› æ­¤ AUC ä¹Ÿå»£æ³›åœ°åœ¨åˆ†é¡å•é¡Œçš„æ¯”è³½ä¸­ä½¿â½¤

  - CAP(Cumulative Accuracy Profile)

    - è¡¡é‡æ¨¡å‹æ•´é«”åœ¨æŠ“å¤šå°‘çš„äººæ™‚(Xè»¸)ï¼Œèƒ½æŠ“åˆ°å¤šå°‘ç›®æ¨™å®¢æˆ¶(Y)
    - éš¨æ©ŸæŠ“æ™‚æŠ“å¤šå°‘%çš„å®¢æˆ¶å°±æœƒæ‰¾åˆ°å¤šå°‘%çš„ç›®æ¨™å®¢æˆ¶ï¼Œå¦‚æœæ›²ç·šè¶Šæ¥è¿‘å·¦ä¸Šè¡¨ç¤ºæ¨¡å‹çš„æ•ˆæœè¶Šå¥½

    - Xè»¸æ”¾æ¨£æœ¬çš„é æ¸¬æ©Ÿç‡*-1

    - yè»¸æ”¾ç´¯ç©æŠ“åˆ°çš„äººæ•¸

    - è¡¡é‡æŒ‡æ¨™(æŠ“50%çš„æ¨£æœ¬æ™‚ï¼Œæ‰¾åˆ°çš„ç›®æ¨™ç™¾åˆ†æ¯”)

      - Rubbishï¼š< 60%
      - Poor ï¼š 60% ~ 70%
      - Goodï¼š70% ~ 80%
      - Very Goodï¼š80% ~ 90%
      - Too Goodï¼š 90% ~ 100%

  - ROC(Receiver Operating Characteristic)

  - [MAP](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)

  - å¤šåˆ†é¡å•é¡Œï¼Œå‰‡å¯ä½¿â½¤ top-k accuracyï¼Œk ä»£è¡¨æ¨¡å‹é æ¸¬å‰ k å€‹é¡åˆ¥æœ‰åŒ…å«æ­£ç¢ºé¡åˆ¥å³ç‚ºæ­£ç¢º (ImageNet ç«¶è³½é€šå¸¸éƒ½æ˜¯æ¯” Top-5 Accuracy)

    - Type I error: False Positive
    - Type II error: false negative

- https://gombru.github.io/2018/05/23/cross_entropy_loss/

### Cluster

- è¼ªå»“åˆ†æ(Silhouette analysis)

  - æ­·å²

    - æœ€æ—©ç”± Peter J. Rousseeuw æ–¼ 1986 æå‡ºã€‚å®ƒåŒæ™‚è€ƒæ…®äº†ç¾¤å…§ä»¥åŠç›¸é„°ç¾¤çš„è·é›¢ï¼Œé™¤äº†å¯ä»¥è©•ä¼°è³‡æ–™é»åˆ†ç¾¤æ˜¯å¦å¾—ç•¶ï¼Œä¹Ÿå¯ä»¥â½¤ä¾†è©•ä¼°ä¸åŒåˆ†ç¾¤â½…å¼å°æ–¼è³‡æ–™çš„åˆ†ç¾¤æ•ˆæœ

  - è¨­è¨ˆç²¾ç¥

    - åŒâ¼€ç¾¤çš„è³‡æ–™é»æ‡‰è©²å¾ˆè¿‘ï¼Œä¸åŒç¾¤çš„è³‡æ–™é»æ‡‰è©²å¾ˆé ï¼Œæ‰€ä»¥è¨­è¨ˆâ¼€ç¨®ç•¶ åŒç¾¤è³‡æ–™é»è¶Šè¿‘ / ä¸åŒç¾¤è³‡æ–™é»è¶Šé  æ™‚è¶Šâ¼¤çš„åˆ†æ•¸
    - ç•¶è³‡æ–™é»åœ¨å…©ç¾¤äº¤ç•Œé™„è¿‘ï¼Œå¸Œæœ›åˆ†æ•¸æ¥è¿‘ 0

  - åˆ†ç¾¤æ¨¡å‹çš„è©•ä¼°

    - èˆ‡ç›£ç£æ¨¡å‹ä¸åŒï¼Œéç›£ç£å› ç‚ºæ²’æœ‰â½¬æ¨™å€¼ï¼Œå› æ­¤ç„¡æ³•ä½¿â½¤â½¬æ¨™å€¼çš„é ä¼°èˆ‡å¯¦éš›å·®è·ï¼Œä¾†è©•ä¼°æ¨¡å‹çš„å„ªåŠ£

  - è¼ªå»“åˆ†æ

    - è¼ªå»“åˆ†æ•¸æ˜¯â¼€ç¨®åŒç¾¤è³‡æ–™é»è¶Šè¿‘ / ä¸åŒç¾¤è³‡æ–™é»è¶Šé æ™‚æœƒè¶Šâ¼¤çš„åˆ†æ•¸ï¼Œé™¤äº†å¯ä»¥è©•ä¼°è³‡æ–™é»åˆ†ç¾¤æ˜¯å¦å¾—ç•¶ï¼Œä¹Ÿå¯ä»¥â½¤ä¾†è©•ä¼°åˆ†ç¾¤æ•ˆæœ
    - è¦ä»¥è¼ªå»“åˆ†æè§€å¯Ÿ K -meanï¼Œé™¤äº†å¯ä»¥å°‡æ¯å€‹è³‡æ–™é»åˆ†çµ„è§€å¯Ÿä»¥è©•ä¼°è³‡æ–™é»åˆ†ç¾¤æ˜¯å¦å¾—ç•¶ï¼Œä¹Ÿå¯â½¤å¹³å‡å€¼è§€å¯Ÿè©•ä¼°ä¸åŒ K å€¼çš„åˆ†ç¾¤æ•ˆæœ

  - è©•ä¼°â½…å¼é¡å‹

    - æœ‰â½¬æ¨™å€¼çš„åˆ†ç¾¤

      - å¦‚æœè³‡æ–™æœ‰â½¬æ¨™å€¼ï¼Œåªæ˜¯å…ˆå¿½ç•¥â½¬æ¨™å€¼åšéç›£ç£å­¸ç¿’ï¼Œå‰‡åªè¦å¾®èª¿å¾Œï¼Œå°±å¯ä»¥ä½¿â½¤åŸæœ¬ç›£ç£çš„æ¸¬é‡å‡½æ•¸è©•ä¼°æº–ç¢ºæ€§
    - ç„¡â½¬æ¨™å€¼çš„åˆ†ç¾¤

    - ä½†é€šå¸¸æ²’æœ‰â½¬æ¨™å€¼/â½¬æ¨™å€¼éå¸¸å°‘æ‰æœƒâ½¤éç›£ç£æ¨¡å‹ï¼Œé€™ç¨®æƒ…æ³ä¸‹ï¼Œåªèƒ½ä½¿â½¤è³‡æ–™æœ¬â¾çš„åˆ†å¸ƒè³‡è¨Šï¼Œä¾†åšæ¨¡å‹çš„è©•ä¼°

    - å–®é»è¼ªå»“å€¼

      - å°ä»»æ„å–®â¼€è³‡æ–™é» iï¼Œã€Œèˆ‡ i åŒâ¼€ç¾¤ã€ çš„è³‡æ–™é»ï¼Œè·é›¢ i çš„å¹³å‡ç¨±ç‚º ai
      - ã€Œèˆ‡ i ä¸åŒç¾¤ã€ çš„è³‡æ–™é»ä¸­ï¼Œä¸åŒç¾¤è·é›¢ i å¹³å‡ä¸­ï¼Œæœ€â¼¤çš„ç¨±ç‚ºbi ( å…¶å¯¦å°±æ˜¯è¦å–ç¬¬â¼†é è¿‘ i çš„é‚£â¼€ç¾¤å¹³å‡ï¼Œæ»¿â¾œäº¤ç•Œä¸Šåˆ†æ•¸ç‚º0 çš„è¨­è¨ˆ)

  - i é»çš„è¼ªå»“åˆ†æ•¸ si : (bi-ai) / max{bi, ai}

    - å…¶å¯¦åªè¦ä¸æ˜¯åˆ»æ„åˆ†éŒ¯ï¼Œbi é€šå¸¸æœƒâ¼¤æ–¼ç­‰æ–¼ aiï¼Œæ‰€ä»¥ä¸Šè¿°å…¬å¼åœ¨æ­¤æ¢ä»¶ä¸‹å¯ä»¥åŒ–ç°¡ç‚º 1 - ai / bi 

    - æ•´é«”çš„è¼ªå»“åˆ†æ

      - åˆ†çµ„è§€å¯Ÿ å¦‚ä¸‹åœ–ï¼Œå·¦åœ–ä¾ç…§ä¸åŒçš„é¡åˆ¥ï¼Œå°‡åŒé¡åˆ¥çš„è¼ªå»“åˆ†æ•¸æ’åºå¾Œé¡¯â½°ï¼Œå¯ä»¥ç™¼ç¾é»ƒç¶ å…©çµ„çš„è¼ªå»“å€¼â¼¤å¤šåœ¨å¹³å‡ä»¥ä¸‹ï¼Œä¸”æ¯”ä¾‹ä¸Šæ¥è¿‘ 0çš„é»ä¹Ÿæ¯”è¼ƒå¤šï¼Œé€™äº›æƒ…æ³éƒ½è¡¨â½°é€™å…©çµ„ä¼¼ä¹æ²’åˆ†å¾—é‚£éº¼é–‹ (å¯å°ç…§ä¸‹åœ–)

        ![](https://lh3.googleusercontent.com/gjSeS-QxeX7aQgk6qeimUFxEGdbgRik64dDZttLGBmZf06fjfAfwxG1rS0nZIYO-pUVcVFj_0jGSEOWERzUqc-iL_qcCjLwyggqHeVroC2V4HmknH1N9l_8BEadADJ9s27t1txj81mLitKe59iGX89qTOQepLAazDMGSR64LTNKBqVLFDFsXpI1zegCA8SOT6y7mrKSM8xd6UfnnI0TIDT8Wt2Y-41vxCe1vG3BTYVFcg6XGqOXqqhjTXuhHytSuSASZisaJG9NlqX1wsfCWYEc8fTDCdeve0zxESyEpPBqsHLPsFXKtiT0M0BDxpwNuIaJJZZa5lBIv-vTx3H7YoYGoaSE_pxVNgFvT57H3yrditWvqbnQhs7ta2oJvAn7NFi4K2d1MC5awNweBXDldfhSBQA3uEhYY694ayyXPYzo00f2Nad0Jz6NGCfi9QRpJjs31cdAaSu4_4FplN8O32q2FalgQWF4gRRVKBSsAep860lL3gCiijqU3ZrpZSzBnqF6OHVOVpdeWKXggHFn-JcVSxl0f7MAO5TAury_bnwa7K2hL93-nnvsc6869Ev5JPKJFrtQsYITFSXI0D0Byj7Hpc4s6CpVdDngEcXGij0Vyqd9u3RHgw5Ev8PDze93qrDaTO6ch21j-QQb5nmD04ytzftOgGd-VnfsxSL30zoOp9DC8eHSS80EAvWGYekRcx0HP_yEPj0LvmLo1tg76-B81AWLhe78ykgPz62lsA-eW7nY=w996-h414-no)

      - å¹³å‡å€¼è§€å¯Ÿ è¨ˆç®—åˆ†ç¾¤çš„è¼ªå»“åˆ†æ•¸ç¸½å¹³å‡ï¼Œåˆ†çš„ç¾¤æ•¸è¶Šå¤šæ‡‰è©²åˆ†æ•¸è¶Šâ¼©ï¼Œå¦‚æœç¸½å¹³å‡å€¼æ²’æœ‰éš¨è‘—åˆ†ç¾¤æ•¸å¢åŠ â½½è®Šâ¼©ï¼Œå°±èªªæ˜äº†é‚£äº›åˆ†çµ„æ•¸è¼ƒä¸æ´½ç•¶

### Dimension Reduction

- KMOçƒå‹æª¢å®š
- Compenent Loading



### æ¨¡å‹é©—è­‰(Validation)

- å‡ºæ–¼ç†è§£çš„è€ƒé‡ï¼Œæˆ‘æŠŠæ¨¡å‹é©—è­‰çš„é †åºæ”¾åœ¨é¸æ¨¡å‹èˆ‡è¡¡é‡æŒ‡æ¨™çš„å¾Œé¢ï¼Œå¯¦éš›åœ¨å»ºç«‹æ¨¡å‹æ™‚è¦å…ˆåšé€™å€‹æ­¥é©Ÿæ‰é–‹å§‹å»ºæ¨¡ã€‚

- æ©Ÿå™¨å­¸ç¿’æ¨¡å‹éœ€è¦è³‡æ–™æ‰èƒ½è¨“ç·´ï¼Œè‹¥å°‡â¼¿ä¸Šæ‰€æœ‰è³‡æ–™éƒ½é€é€²æ¨¡å‹è¨“ç·´ï¼Œé€™æ¨£å°±æ²’æœ‰é¡å¤–è³‡æ–™ä¾†è©•ä¼°æ¨¡å‹è¨“ç·´æƒ…å½¢ï¼

- æ©Ÿå™¨å­¸ç¿’æ¨¡å‹å¯èƒ½æœƒæœ‰éæ“¬åˆ (Over-fitting) çš„æƒ…å½¢ç™¼â½£ï¼Œéœ€é€éé©—è­‰/æ¸¬è©¦é›†è©•ä¼°æ¨¡å‹æ˜¯å¦éæ“¬åˆ

- æœ‰äº›è³‡æ–™è¦ç‰¹åˆ¥æ³¨æ„!

  - æ™‚é–“åºåˆ—è³‡æ–™
  - åŒâ¼€â¼ˆæœ‰å¤šç­†è³‡æ–™

- è‹¥åƒ…åšâ¼€æ¬¡è¨“ç·´/æ¸¬è©¦é›†åˆ‡åˆ†ï¼Œæœ‰äº›è³‡æ–™æœƒæ²’æœ‰è¢«æ‹¿ä¾†è¨“ç·´éï¼Œå› æ­¤å¾ŒçºŒå°±æœ‰ cross-validation çš„â½…æ³•ï¼Œå¯ä»¥è®“çµæœæ›´ç‚ºç©©å®šï¼Œï¼«ç‚º fold æ•¸é‡

- æ¯ç­†è³‡æ–™éƒ½æ›¾ç¶“ç•¶éâ¼€æ¬¡é©—è­‰é›†ï¼Œå†å–å¹³å‡å¾—åˆ°æœ€çµ‚çµæœã€‚

- åœ¨Test Dataçš„æ¨™ç±¤æœªçŸ¥çš„æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘éœ€è¦è‡ªå·±æ§‹é€ æ¸¬è©¦è³‡æ–™ä¾†é©—è­‰æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå› æ­¤æŠŠTrain Dataåˆ†å‰²æˆTrain Setå’ŒValid Setå…©éƒ¨åˆ†ï¼ŒTrain Setç”¨æ–¼è¨“ç·´ï¼ŒValid Setç”¨æ–¼é©—è­‰ã€‚

  - ç°¡å–®åˆ‡åˆ†

    - å°‡Train DataæŒ‰ä¸€å®šæ–¹æ³•åˆ†æˆå…©ä»½ï¼Œæ¯”å¦‚éš¨æ©Ÿå–å…¶ä¸­70%çš„è³‡æ–™ä½œç‚ºTrain Setï¼Œå‰©ä¸‹30%ä½œç‚ºValid Setï¼Œæ¯æ¬¡éƒ½å›ºå®šåœ°ç”¨é€™å…©ä»½è³‡æ–™åˆ†åˆ¥è¨“ç·´æ¨¡å‹å’Œé©—è­‰æ¨¡å‹ã€‚é€™ç¨®åšæ³•çš„ç¼ºé»å¾ˆæ˜é¡¯ï¼Œå®ƒæ²’æœ‰ç”¨åˆ°æ•´å€‹è¨“ç·´è³‡æ–™ï¼Œæ‰€ä»¥é©—è­‰æ•ˆæœæœƒæœ‰åå·®ã€‚é€šå¸¸åªæœƒåœ¨è¨“ç·´è³‡æ–™å¾ˆå¤šï¼Œæ¨¡å‹è¨“ç·´é€Ÿåº¦è¼ƒæ…¢çš„æ™‚å€™ä½¿ç”¨ã€‚

    ```python
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
    ```

  - Cross-validation

    - äº¤å‰é©—è­‰æ˜¯å°‡æ•´å€‹è¨“ç·´è³‡æ–™éš¨æ©Ÿåˆ†æˆKä»½ï¼Œè¨“ç·´Kå€‹æ¨¡å‹ï¼Œæ¯æ¬¡å–å…¶ä¸­çš„K-1ä»½ä½œç‚ºTrain Setï¼Œç•™å‡º1ä»½ä½œç‚ºValid Setï¼Œå› æ­¤ä¹Ÿå«åš**K-fold**ã€‚è‡³æ–¼é€™å€‹Kï¼Œä½ æƒ³å–å¤šå°‘éƒ½å¯ä»¥ï¼Œä½†ä¸€èˆ¬é¸åœ¨3ï½10ä¹‹é–“ã€‚æˆ‘å€‘å¯ä»¥ç”¨Kå€‹æ¨¡å‹å¾—åˆ†çš„meanå’Œstdï¼Œä¾†è©•åˆ¤æ¨¡å‹å¾—å¥½å£ï¼ˆmeané«”ç¾æ¨¡å‹çš„èƒ½åŠ›ï¼Œstdé«”ç¾æ¨¡å‹æ˜¯å¦å®¹æ˜“éæ“¬åˆï¼‰ï¼Œä¸¦ä¸”ç”¨K-foldçš„é©—è­‰çµæœé€šå¸¸æœƒæ¯”è¼ƒå¯é ã€‚

      å¦‚æœè³‡æ–™å‡ºç¾Labelä¸å‡è¡¡æƒ…æ³ï¼Œå¯ä»¥ä½¿ç”¨Stratified K-foldï¼Œé€™æ¨£å¾—åˆ°çš„Train Setå’ŒTest Setçš„Labelæ¯”ä¾‹æ˜¯å¤§è‡´ç›¸åŒã€‚

       

    - æ ¹æ“šåˆ‡åˆ†çš„æ–¹æ³•ä¸åŒï¼Œäº¤å‰é©—è­‰åˆ†ç‚ºä¸‹é¢ä¸‰ç¨®ï¼šã€€ã€€ã€€

      - ç°¡å–®äº¤å‰é©—è­‰ï¼Œæ‰€è¬‚çš„ç°¡å–®ï¼Œæ˜¯å’Œå…¶ä»–äº¤å‰é©—è­‰æ–¹æ³•ç›¸å°è€Œè¨€çš„ã€‚é¦–å…ˆï¼Œæˆ‘å€‘éš¨æ©Ÿçš„å°‡æ¨£æœ¬è³‡æ–™åˆ†ç‚ºå…©éƒ¨åˆ†ï¼ˆæ¯”å¦‚ï¼š 70%çš„è¨“ç·´é›†ï¼Œ30%çš„æ¸¬è©¦é›†ï¼‰ï¼Œç„¶å¾Œç”¨è¨“ç·´é›†ä¾†è¨“ç·´æ¨¡å‹ï¼Œåœ¨æ¸¬è©¦é›†ä¸Šé©—è­‰æ¨¡å‹åŠåƒæ•¸ã€‚æ¥è‘—ï¼Œæˆ‘å€‘å†æŠŠæ¨£æœ¬æ‰“äº‚ï¼Œé‡æ–°é¸æ“‡è¨“ç·´é›†å’Œæ¸¬è©¦é›†ï¼Œç¹¼çºŒè¨“ç·´è³‡æ–™å’Œæª¢é©—æ¨¡å‹ã€‚æœ€å¾Œæˆ‘å€‘é¸æ“‡æå¤±å‡½æ•¸è©•ä¼°æœ€å„ªçš„æ¨¡å‹å’Œåƒæ•¸ã€‚ã€€

      - ç¬¬äºŒç¨®æ˜¯ S æŠ˜äº¤å‰é©—è­‰ï¼ˆ S-Folder Cross Validationï¼‰ï¼Œå’Œç¬¬ä¸€ç¨®æ–¹æ³•ä¸åŒï¼Œ S æŠ˜äº¤å‰é©—è­‰å…ˆå°‡è³‡æ–™é›† D éš¨æ©ŸåŠƒåˆ†ç‚º S å€‹å¤§å°ç›¸åŒçš„äº’æ–¥å­é›†ï¼Œå³

        $$D=D_1\cup D_2\cup ...\cup D_S,D_i\cap D_j=\varnothing(i\ne j)$$

        æ¯æ¬¡éš¨æ©Ÿçš„é¸æ“‡ ä»½ä½œç‚ºè¨“ç·´é›†ï¼Œå‰©ä¸‹çš„1ä»½åšæ¸¬è©¦é›†ã€‚ç•¶é€™ä¸€è¼ªå®Œæˆå¾Œï¼Œé‡æ–°éš¨æ©Ÿé¸æ“‡ ä»½ä¾†è¨“ç·´è³‡æ–™ã€‚è‹¥å¹²è¼ªï¼ˆå°æ–¼ ï¼‰ä¹‹å¾Œï¼Œé¸æ“‡æå¤±å‡½æ•¸è©•ä¼°æœ€å„ªçš„æ¨¡å‹å’Œåƒæ•¸ã€‚æ³¨æ„ï¼Œäº¤å‰é©—è­‰æ³•è©•ä¼°çµæœçš„ç©©å®šæ€§å’Œä¿çœŸæ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–æ±ºæ–¼ å–å€¼ã€‚

      - ç¬¬ä¸‰ç¨®æ˜¯ç•™ä¸€äº¤å‰é©—è­‰ï¼ˆLeave-one-out Cross Validationï¼‰ï¼Œå®ƒæ˜¯ç¬¬äºŒç¨®æƒ…æ³çš„ç‰¹ä¾‹ï¼Œæ­¤æ™‚ S ç­‰æ–¼æ¨£æœ¬æ•¸ N ï¼Œé€™æ¨£å°æ–¼ N å€‹æ¨£æœ¬ï¼Œæ¯æ¬¡é¸æ“‡ N-1 å€‹æ¨£æœ¬ä¾†è¨“ç·´è³‡æ–™ï¼Œç•™ä¸€å€‹æ¨£æœ¬ä¾†é©—è­‰æ¨¡å‹é æ¸¬çš„å¥½å£ã€‚æ­¤æ–¹æ³•ä¸»è¦ç”¨æ–¼æ¨£æœ¬é‡éå¸¸å°‘çš„æƒ…æ³ï¼Œæ¯”å¦‚å°æ–¼é€šé©ä¸­å•é¡Œï¼Œ N å°æ–¼ 50 æ™‚ï¼Œä¸€èˆ¬æ¡ç”¨ç•™ä¸€äº¤å‰é©—è­‰ã€‚

      ![](https://lh3.googleusercontent.com/Q8wUvU5LNtUC-KfgXi6onDlAYzhwzrMtJLqAETx9lxiICpwMQ6avrzQZeZuTbk4jLfy8yLzQE8GtQVPhvwQLLgBCwHahR80HYHnhk9HFYw2XFXojQJyN1aCx4xGwIKHXws0zaCJhfP2fvpcaRcjyX6qpeyTANWU6x8PgTaG7QZibxwBa0HhRGkZvFGJvgpEg8cQRENu7O3tVghzmIrTMDl_DT1R71SLi5cuC8nRWwfgy2mC7k5QZQemELATPskGnC9m8ocq6j526DKheHdUzg_H-RNnsXW4VSZ0SAmtrxM2wYv4Yr-giyt2aKau593Ed7IV052HnELmbfAK02ytqJ4STKzgQODjgydWn686EgWfb2XsEjg-_pppEbeNL5PGbHxGdSrrGVLSH_njIWlA6AGnT5Zl5N6EaCYvqqOmz_d3bF2I1uXyHEBdW9DLk-Biw-I7wfoe-1VYG7PVzQuNNYktqS59V3jq71PbMB0JlwnoYq0NeFEBHiAr4LlSCNLkRUnNLIx36BM7yWvCANBz7ueVNnSrdp6wXachkE5i9CGqkZHodJTs1L05ztMF3e-quBPhd87tfa_zwRO74sE44PofvkH38qvFE0--rQJnXHWZZ9n88ilp12CYyxrhRLWEoCMpDA3ZQPlTk9yARiH-Em5EfHu8xppfFGz5gdf6zvROpAxFtbrVKMmHKkchUIG9x79xLl7ZYzNesryK6qLirr41EH-Dd2S29eGEBkEMFHLiQ8fQ=w665-h303-no)

      ```python
      sklearn.model_selection.KFold()
      ```

  - é©—è­‰é›† (validation set) èˆ‡æ¸¬è©¦é›† (testing set)æœ‰ç”šéº¼å·®ç•°ï¼Ÿ

    - é©—è­‰é›†å¸¸â½¤ä¾†è©•ä¼°ä¸åŒè¶…åƒæ•¸æˆ–ä¸åŒæ¨¡å‹çš„çµæœã€‚â½½æ¸¬è©¦é›†å‰‡æ˜¯åœ¨æ©Ÿå™¨å­¸ç¿’å°ˆæ¡ˆé–‹å§‹å‰å…ˆä¿ç•™â¼€â¼©éƒ¨åˆ†è³‡æ–™ï¼Œå°ˆæ¡ˆé€²â¾ä¸­éƒ½ä¸èƒ½ä½¿â½¤ï¼Œæœ€çµ‚å†æ‹¿ä¾†åšæ¸¬è©¦ã€‚

  


### åƒè€ƒè³‡æ–™

- [All Models Are Wrong: Concepts of Statistical Learning](https://allmodelsarewrong.github.io/index.html)
- [What is a good r square value in regression analysis?](https://www.researchgate.net/post/what_is_a_good_r_square_value_in_regression_analysis)
- [æå®æ¯…â½¼å¸«ç·šä¸Šèª²ç¨‹/ç°¡å ±](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html])
- [Google åŸºç¤æ©Ÿå™¨å­¸ç¿’ç·šä¸Šèª²](https://developers.google.com/machine-learning/crash-course/)
- [AWS Machine Learning ç·šä¸Šèª²](https://aws.amazon.com/tw/training/learn-about/machine-learning/)



## Anomaly detection

Also known as outlier detection, anomaly detection is used to find rare occurrences or suspicious events in your data. The outliers typically point to a problem or rare event.

### Isolation Forest

Isolation Forests build a Random Forest in which each Decision Tree is grown randomly. At each node, it picks a feature randomly, then it picks a random threshold value (between the min and max value) to split the dataset in two. The dataset gradually gets chopped into pieces this way, until all instances end up isolated from the other instances.Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produces shorter path lengths for particular samples, they are highly likely to be anomalies.Python TutorialFig 12: IsolationForest Example

- [Python Tutorial](https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e)

  ![Fig 12: IsolationForest Example](https://scikit-learn.org/stable/_images/sphx_glr_plot_isolation_forest_001.png)



### Once Class SVM

One Class SVM is an anomaly detection technique which trains a Support Vector Machine on data with only one class. The SVM learns a boundary around the data and identifies which data points are far away from it. Data points which are abnormally far away from the data are considered outliers.

![Fig. 13: One-Class SVM](https://scikit-learn.org/stable/_images/sphx_glr_plot_oneclass_001.png)

- [R Tutorial](https://tsmatz.wordpress.com/2017/04/03/r-anomaly-detection-one-class-support-vector-machine-with-microsoftml-rxoneclasssvm/)
- [Python Tutorial](https://www.kaggle.com/amarnayak/once-class-svm-to-detect-anomaly)

### PCA-Based Anomaly Detection

PCA-Based Anomaly Detection uses distance metrics to differentiate between normal and anomalous behavior. Data points which are far apart from most of the data are classified as anomalous.

- [Python Example](https://www.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/ch04.html)

### Fast-MCD

The Fast-MCD (minimum covariance determinant) algorithm is used for outlier detection. It assumes that the normal instances, also called inliers, are generated from a single Gaussian distribution and not a mixture, but it also assumes that the dataset is contaminated with outliers that were not generated from this Gaussian distribution. When it estimates the parameters of the Gaussian distribution, it is careful to ignore the instances that are most likely outliers. This makes it better at identifying the outliers. 

- [R Example](https://www.rdocumentation.org/packages/robustbase/versions/0.93-5/topics/covMcd)

### Local Outlier Factor (LOF)

LOF is an unsupervised algorithm and is used for anomaly/outlier detection which is based on the local density estimation. It compares the density of instances around a given instance to the density around its neighbors. An anomaly is often more isolated than its k nearest neighbors. If the density of a point is much smaller than the densities of its neighbors (LOF â‰«1), the point is far from dense areas and, hence, an outlier. In short, we can say that the density around an outlier object is significantly different from the density around its neighbors.It can be summarized in three steps:

- [Python Tutorial](https://medium.com/@mtngt/local-outlier-factor-simple-python-example-8925dad97fe6)





### Stacked Generalization (Stacking)

Stacking is an ensemble method where a new model is trained to combine the predictions from two or more models already trained on a dataset. It is based on a simple idea: instead of using trivial ensemble functions to aggregate the predictions of all predictors in an ensemble, stacking would train a model to perform this aggregation. The idea is that you can attack a learning problem with different types of models which are capable to learn some part of the problem, but not the whole space of the problem.The procedure starts with splitting the training set into two disjoint sets. Following this we would train several base learners on the first part and test the base learners on the second part. Using these predictions as the inputs, and the correct responses as the outputs, weâ€™ll train a higher-level learner.For example, for a classification problem, we can choose as weak learners a KNN classifier, a logistic regression and an SVM, and decide to learn a neural network as meta-model. Then, the neural network will take as inputs the outputs of our three weak learners and will learn to return final predictions based on it.It is typical to use a simple linear method to combine the predictions for sub models such as simple averaging or voting, to a weighted sum using linear regression or logistic regression. It is important that sub-models produce different predictions, so-called uncorrelated predictions. Stacking is one of the most efficient techniques used in winning data science competitions.

- [Python Tutorial](https://machinelearningmastery.com/implementing-stacked-scratch-python)



### Time Series



## Association Rule Learning

Association rule analysis is a technique to uncover how items are associated with each other. 

### Apriori

This algorithm is one of the basic approaches used in mining frequent patterns from datasets. It is one of the fundamental algorithms used in the market-basket analysis. You can use this to find the groups of items that occur together frequently in a shopping dataset which can help buisnesses find ways to promote their products. Apriori works by finding the counts of all the items in the dataset and filtering out items that do not occur frequently. The counts can be extended to a pair of items and later on to the count of itemsets till size N and then filtering out the infrequent itemsets. One thing to note here is that the frequent pairs are those where both items in the pair are frequent items.The advantage of this algorithm is that it saves a lot of time by cutting down on the number of itemsets that it builds and counts.

- [R Example](http://r-statistics.co/Association-Mining-With-R.html)

### Eclat

The ECLAT algorithm stands for Equivalence Class Clustering and bottom-up Lattice Traversal. While the Apriori algorithm works in a horizontal sense imitating the Breadth-First Search of a graph, the ECLAT algorithm works in a vertical manner just like the Depth-First Search of a graph. Due to this vertical approach, ECLAT is faster and scalable than the Apriori algorithm.ECLAT is superior over Apriori because of memory (Since the ECLAT algorithm uses a Depth-First Search approach, it uses less memory than Apriori algorithm) and computations (The ECLAT algorithm does not involve the repeated scanning of the data to compute the support values).

- Simplified Version of Aprior Rules, only care about support value
- ä½¿ç”¨æµç¨‹
  1. Set a minimum support
  2. Take all the subsets in transactions having higher support than minimum support
  3. Sort these subsets by decreasing supprt

- [R Example](http://r-statistics.co/Association-Mining-With-R.html)

## regularization

Regularization is used to prevent overfitting. Overfitting means a machine learning algorithm has fit the data set too strongly such that it has high accuracy in it but does not perform well on unseen data. 

### LASSO Regularization (Least Absolute Shrinkage and Selection Operator)

LASSO regularization adds the sum of the absolute values of the coefficients of the model to the cost function. This also acts as a form of feature selection as the coefficients may become 0 and only the coefficients of the variables which are discriminative stay. LASSO works well if few features affect the predictor variable (with a high coefficient) and others are close to zero.

- [Lasso and Ridge in Python](https://www.kaggle.com/jmataya/regularization-with-lasso-and-ridge)

### Ridge Regularization

Ridge regularization works by adding the square of the coefficients of the model to the cost function. The coefficients of correlated features becomes similar. Ridge works well if there are many large coefficients of the same value, that is, many features have a strong impact on the predicted variable.

- [Lasso and Ridge in Python](https://www.kaggle.com/jmataya/regularization-with-lasso-and-ridge)

### Elastic Net Regularization

Elastic Net is a combination of the penalties of LASSO and Ridge. It picks out a group of independent variables which are correlated and if there is a strong predicting power in them all of them will be used.

- [R Tutorial](https://www.r-bloggers.com/variable-selection-with-elastic-net/)







http://www.cc.ntu.edu.tw/chinese/epaper/0036/20160321_3606.html

- æ‡‰ç”¨

  - è¶…å¸‚ï¼šæª¢è¦–å“ªäº›å•†å“æœƒä¸€èµ·è³¼è²·
  - å®¢æœï¼šæª¢è¦–å“ªäº›å®¢æˆ¶æœå‹™æœƒä¸€èµ·å‡ºç¾
    - é€šéé—œè¯æœå‹™èˆ‡é—œè¯çš„é—œè¯æœå‹™ï¼Œè¨­è¨ˆæœå‹™æµç¨‹

- è©•ä¼°æŒ‡æ¨™

  - æ”¯æŒåº¦(support)ï¼š
    - åœ¨æ‰€æœ‰çš„äº‹å‹™ä¸­åŒæ™‚å‡ºç¾Aå’ŒBçš„æ©Ÿç‡ã€‚æ”¯æŒåº¦è¡¨ç¤ºäº†Aå’ŒBåŒæ™‚å‡ºç¾çš„é »ç‡ï¼Œå¦‚æœAå’ŒBä¸€èµ·å‡ºç¾çš„é »ç‡éå¸¸å°ï¼Œé‚£éº¼å°±èªªæ˜äº†Aå’ŒBä¹‹é–“çš„è¯ç¹«ä¸¦ä¸å¤§;å¦‚æœä¸€èµ·å‡ºç¾çš„é »ç‡éå¸¸é »ç¹
    - ã€Œè¦å‰‡ã€åœ¨è³‡æ–™å…§å…·æœ‰æ™®éæ€§ï¼Œä¹Ÿå°±æ˜¯é€™äº› A è·Ÿ B åŒæ™‚å‡ºç¾çš„æ©Ÿç‡å¤šå°‘ã€‚

  $$
  Support = \frac{freq(A,B)}{N}
  $$

  - ä¿¡è³´åº¦(confidence)ï¼š
    - ä¿¡è³´åº¦è¡¨ç¤ºäº†é€™æ¢è¦å‰‡æœ‰å¤šå¤§ç¨‹åº¦ä¸Šå€¼å¾—å¯ä¿¡ã€‚è¡¨ç¤ºé—œè¯è¦å‰‡A-->Bä¸­ï¼Œç™¼ç”ŸAçš„å‰æä¸‹ä¹Ÿå‡ºç¾äº†Bï¼Œå…¶å¯¦å°±æ˜¯ä¸€ç¨®æ¢ä»¶æ©Ÿç‡
    - ç½®ä¿¡åº¦æ­ç¤ºäº†Bå‡ºç¾æ™‚ï¼ŒAæ˜¯å¦ä¸€å®šæœƒå‡ºç¾ï¼Œå¦‚æœå‡ºç¾å‰‡å…¶å¤§æ¦‚æœ‰å¤šå¤§çš„å¯èƒ½å‡ºç¾ã€‚å¦‚æœç½®ä¿¡åº¦ç‚º100%, å‰‡èªªæ˜äº†Bå‡ºç¾æ™‚ï¼ŒAä¸€å®šå‡ºç¾ã€‚é‚£éº¼ï¼Œå°é€™ç¨®æƒ…æ³è€Œè¨€ï¼Œå‡è¨­Aå’ŒBæ˜¯å¸‚å ´ä¸Šçš„å…©ç¨®å•†å“ï¼Œå°±æ²’æœ‰ç†ç”±ä¸é€²è¡Œæ†ç¶éŠ·å”®äº†ã€‚å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œé‚£éº¼å°±æœƒç”¢ç”Ÿé€™æ¨£çš„ç–‘å•ï¼ŒAå’ŒBé—œä¿‚ä¸¦ä¸å¤§ï¼Œä¹Ÿè¨±èˆ‡Bé—œè¯çš„ä¸¦ä¸æ˜¯Aã€‚
    - ã€Œè¦å‰‡ã€è¦æœ‰ä¸€å®šçš„ä¿¡å¿ƒæ°´æº–ï¼Œä¹Ÿå°±æ˜¯ç•¶è³¼è²· A ç‹€æ…‹ä¸‹ï¼Œä¹Ÿæœƒè³¼è²· B çš„æ¢ä»¶æ©Ÿç‡ã€‚

  $$
  Confidence(B|A)=\frac{Freq(A,B)}{Freq(A)}
  $$

  - å¢ç›Š(Lift)ï¼š
    - è¡¨ç¤ºå‡ºç¾Açš„æ¢ä»¶ä¸‹åŒæ™‚å‡ºç¾Bçš„å¯èƒ½æ€§èˆ‡æ²’æœ‰ä»»ä½•æ¢ä»¶ä¸‹å‡ºç¾Bçš„å¯èƒ½æ€§ä¹‹æ¯”
    - é€™å€‹æŒ‡æ¨™æ˜¯ç½®ä¿¡åº¦çš„è£œå……ï¼Œç”¨ä¾†åˆ¤æ–·Aèˆ‡Bä¹‹é–“æ˜¯ä¸æ˜¯ç¨ç«‹ï¼Œä¸ç¨ç«‹çš„è©±é—œè¯æ€§æœ‰å¤šå¼·ã€‚ä¸€èˆ¬æå‡åº¦ç­‰æ–¼1æ™‚ï¼Œè¡¨ç¤ºAèˆ‡Bä¹‹é–“æ˜¯ç¨ç«‹çš„ï¼Œå³Açš„å‡ºç¾å°Bçš„å‡ºç¾æ²’æœ‰æ´¾ä¸Šä»ç„¶ä½œç”¨ï¼›æå‡åº¦å¤§æ–¼1ï¼Œä¸”å€¼è¶Šå¤§èªªæ˜Aå°Bçš„å½±éŸ¿è¶Šå¤§ï¼Œé—œè¯æ€§ä¹Ÿå°±è¶Šå¼·ã€‚
    - è¶Šæ¥è¿‘1è¡¨ç¤ºXèˆ‡Yäº’ç›¸ç¨ç«‹ï¼Œè¶Šé«˜è¡¨ç¤ºé—œè¯æ€§è¶Šå¼·

  $$
  Lift(A->B) = \frac{support(A,B)}{support(A)*support(B)}
  $$

- ä½¿ç”¨æµç¨‹

  1. Set a minimum support and confidence.
  2. Take all the subsets in transactions having higher support than minimum support.
  3. Take all the rules of these subsets having higher confidence than minimum confidence.
  4. Sort the rules by decreasing lift.

- Python Code

  ```python
  # Training the Apriori model on the dataset
  from apyori import apriori
  rules = apriori(transactions = transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2, max_length = 2)
  
  # Visualising the results
  results = list(rules)
  def inspect(results):
      lhs         = [tuple(result[2][0][0])[0] for result in results]
      rhs         = [tuple(result[2][0][1])[0] for result in results]
      supports    = [result[1] for result in results]
      confidences = [result[2][0][2] for result in results]
      lifts       = [result[2][0][3] for result in results]
      return list(zip(lhs, rhs, supports, confidences, lifts))
  resultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])
  ```

  - é‡è¦åƒæ•¸
    - min_support
    - min_confidence
    - min_lift
    - min_length
    - max_length

- æ³¨æ„

- è¦å°‡è³‡æ–™è™•ç†æˆä»¥è¨‚å–®ç·¨è™Ÿç‚ºbaseçš„DataFrameï¼Œcolumnæ˜¯æ¯å€‹å•†å“ï¼Œvalueæ˜¯1,0

- åƒè€ƒè³‡æ–™

  - https://www.itread01.com/content/1547127395.html
  - https://ithelp.ithome.com.tw/articles/10217912
  - https://kknews.cc/news/pvy9ke2.html 
  - https://www.twblogs.net/a/5c9a2e99bd9eee4250080ad1



### Reinforcement Learning

- Train the dog walk

- The Multi-Armed Bandit Problem

  - We have $d$ arms. For example, arms are ads we display to users each time they connect to a web page.
  - Each time a user connects to this web page, that makes a round.
  - At each round $n$, we choose one ad to display to the user.
  - At each round $n$, ad $i$ gives reward $r_i(n) \in \{0,1\}$
    - $r_i(n)=1$ if the user clicked on the ad
    - $r_i(n)=0$ if the user didn't.
  - Our goal is to maximize the total reward we get over many rounds.
  - å¤šè‡‚å¼åƒè§’å­è€è™æ©Ÿçš„å•é¡Œå…¶å¯¦æ˜¯åœ¨è€ƒé‡ï¼Œç›®å‰æœ‰å€‹åƒè§’å­è€è™æ©Ÿï¼Œä¸Šé¢æœ‰![K](https://s0.wp.com/latex.php?latex=K&bg=ffffff&fg=000&s=0) å€‹æ‰‹è‡‚ï¼Œä¸€æ¬¡åªèƒ½æ‹‰ä¸€å€‹è‡‚ï¼Œæ‹‰å®Œä¹‹å¾Œæœƒè§€å¯Ÿåˆ°ä¸€å€‹æ–°çš„å ±é…¬ï¼Œè¦æ¡ç”¨ä»€éº¼æ¨£å­çš„ç­–ç•¥ï¼Œèƒ½å¤ ç²å¾—æœ€å¤§çš„æœŸæœ›å ±é…¬ï¼Ÿç‚ºäº†å›ç­”é€™å€‹å•é¡Œï¼Œã€Œå¦‚ä½•æ±ºå®šè¦å»æ‹‰å“ªä¸€å€‹æ‰‹è‡‚ã€ï¼Œä»¥åŠã€Œ![R_a(s) ](https://s0.wp.com/latex.php?latex=R_a%28s%29+&bg=ffffff&fg=000&s=0) è©²è¢«å¦‚ä½•åˆ»åŠƒã€ï¼Œå°‡æ˜¯å¤šè‡‚å¼åƒè§’å­è€è™æ©Ÿçš„é‡è¦å…ƒç´ ã€‚
  - æˆ‘å€‘ç•¶ç„¶èƒ½å¤ é€é A/B Testçš„æ–¹å¼æ‰¾å‡ºç­”æ¡ˆï¼Œä½†A/B Testçš„æˆæœ¬(æ™‚é–“ï¼Œé‡‘éŒ¢)å¤ªé«˜äº†ï¼Œè€Œä¸”ç•¶æœ‰å¤šå€‹ campain æ™‚ï¼ŒA/B Test æœƒéœ€è¦å¤šè¼ªçš„æ¸¬è©¦ã€‚

- Upper Confidence Bound Algorithm

  1. At each round $n$, we consider two numbers for each ad $i$:

     - $N_i(n)$ - the number of times the ad $i$ was selected up to round $n_i$
     - $R_i(n)$ - the sum of rewards of the ad $i$ up to eound n.

  2. From these two numbers we compute:

     - the average reward of ad $i$ up to round $n$
       $$
       \bar r_i(n) = \frac{R_i(n)}{N_i(n)}
       $$

- the confidence interval[$\bar r_i(n) - \Delta_i (n), \bar r_i(n) + \Delta_i(n)$] at round $n$ with 
      $$
      \Delta_i(n) = \sqrt \frac{3log(n)}{2N_i(n)}
      $$
      

3. We select the ad $i$ that has the maximum UCB $\bar r_i(n) + \Delta_i(n)$.

- æœ€çµ‚æœƒè‡ªå‹•æ‰¾å‡ºConversion Rateæœ€é«˜çš„campainï¼Œä¸¦æ¨è–¦è©²campain

- Thompson Sampling Algorithm

  - At each round $n$, we consider two numbers for each ad $i$:

    - $N_i^1(n)$ - the number of times the ad $i$ got reward 1 up to round $n$
    - $N_i^0(n)$ - the number of times the ad $i$ got reward 0 up to round n.

  - For each ad $i$, we take a random draw from the distribution below:
    $$
    \theta_i(n) = \beta(N_i^1(n)+1, N_i^0(n)+1)
    $$

  - We select the ad that has the highest $\theta_i(n)$.

- Compare UCB and Thompson Sampling

  - UCB
    - Deterministic
    - Requires update at every round
  - Thompson Sampling
    - Probabilistic
    - Can accommodate delayed feedback
    - Better empirical evidence

- åƒè€ƒè³‡æ–™

  - [Multi-Armed Bandit: epsilon-greedy](https://zhuanlan.zhihu.com/p/32335683)
  - [Multi-Armed Bandit: UCB (Upper Bound Confidence)](https://zhuanlan.zhihu.com/p/32356077)
  - [äººå·¥æ™ºæ…§èˆ‡å¢å¼·å­¸ç¿’-2ï¼šå¤šè‡‚åƒè§’å­è€è™æ©Ÿç†è«–](https://taweihuang.hpd.io/2016/10/17/äººå·¥æ™ºæ…§èˆ‡å¢å¼·å­¸ç¿’-2ï¼šå¤šè‡‚å¼åƒè§’å­è€è™æ©Ÿç†è«–/)
  - [ä»€ä¹ˆæ˜¯æ±¤æ™®æ£®é‡‡æ ·ï¼ˆThompson samplingï¼‰ï¼Ÿ](https://www.zhihu.com/question/37212823)

- 

### æ¨¡å‹é›†æˆ(Ensemble)

æ›¾ç¶“è½éä¸€å¥è©±ï¼Œâ€Featureç‚ºä¸»ï¼ŒEnsembleç‚ºå¾Œâ€ã€‚Featureæ±ºå®šäº†æ¨¡å‹æ•ˆæœçš„ä¸Šé™ï¼Œè€ŒEnsembleå°±æ˜¯è®“ä½ æ›´æ¥è¿‘é€™å€‹ä¸Šé™ã€‚Ensembleè¬›ç©¶â€œå¥½è€Œä¸åŒâ€ï¼Œä¸åŒæ˜¯æŒ‡æ¨¡å‹çš„å­¸ç¿’åˆ°çš„å´é‡é¢ä¸ä¸€æ¨£ã€‚

å¸¸è¦‹çš„Ensembleæ–¹æ³•æœ‰Baggingã€Boostingã€Stackingã€Blendingã€‚

- Bagging

  - Baggingæ˜¯å°‡å¤šå€‹æ¨¡å‹ï¼ˆ**åŸºå­¸ç¿’å™¨**ï¼‰çš„é æ¸¬çµæœç°¡å–®åœ°**åŠ æ¬Šå¹³å‡æˆ–è€…æŠ•ç¥¨**ã€‚Baggingçš„å¥½è™•åœ¨æ–¼å¯ä»¥ä¸¦è¡Œåœ°è¨“ç·´åŸºå­¸ç¿’å™¨ï¼Œå…¶ä¸­Random Forestå°±ç”¨åˆ°äº†Baggingçš„æ€æƒ³ã€‚
  - Baggingé€šå¸¸æ˜¯æ²’æœ‰ä¸€å€‹æ˜ç¢ºçš„å„ªåŒ–ç›®æ¨™çš„ï¼Œä½†æ˜¯æœ‰ä¸€ç¨®å«[Bagging Ensemble Selection](http://link.zhihu.com/?target=http%3A//www.cs.cornell.edu/~alexn/papers/shotgun.icml04.revised.rev2.pdf)çš„æ–¹æ³•ï¼Œå®ƒé€šéè²ªå©ªæ¼”ç®—æ³•ä¾†Baggingå¤šå€‹æ¨¡å‹ä¾†å„ªåŒ–ç›®æ¨™å€¼ã€‚

- Boosting

  - Boostingçš„æ€æƒ³æœ‰é»åƒ**çŸ¥éŒ¯èƒ½æ”¹**ï¼Œæ¯è¨“ç·´ä¸€å€‹åŸºå­¸ç¿’å™¨ï¼Œæ˜¯ç‚ºäº†å½Œè£œä¸Šä¸€å€‹åŸºå­¸ç¿’å™¨æ‰€çŠ¯çš„éŒ¯èª¤ã€‚å…¶ä¸­è‘—åçš„æ¼”ç®—æ³•æœ‰AdaBoostï¼ŒGradient Boostã€‚Gradient Boost Treeå°±ç”¨åˆ°äº†é€™ç¨®æ€æƒ³ã€‚

    åœ¨å‰é¢çš„éŒ¯èª¤åˆ†æä¸­æåˆ°Boostingï¼ŒéŒ¯èª¤åˆ†æ->æŠ½å–ç‰¹å¾µ->è¨“ç·´æ¨¡å‹->éŒ¯èª¤åˆ†æï¼Œé€™å€‹éç¨‹å°±è·ŸBoostingå¾ˆç›¸ä¼¼ã€‚

- Stacking

  - Stackingæ˜¯ç”¨æ–°çš„æ¨¡å‹ï¼ˆ**æ¬¡å­¸ç¿’å™¨**ï¼‰å»**å­¸ç¿’æ€éº¼çµ„åˆ**é‚£äº›**åŸºå­¸ç¿’å™¨**ï¼Œå®ƒçš„æ€æƒ³æºè‡ªæ–¼[Stacked Generalization](http://link.zhihu.com/?target=http%3A//www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf)é€™ç¯‡è«–æ–‡ã€‚å¦‚æœæŠŠBaggingçœ‹ä½œæ˜¯å¤šå€‹åŸºåˆ†é¡å™¨çš„ç·šæ€§çµ„åˆï¼Œé‚£éº¼Stackingå°±æ˜¯å¤šå€‹åŸºåˆ†é¡å™¨çš„éç·šæ€§çµ„åˆã€‚Stackingå¯ä»¥å¾ˆéˆæ´»ï¼Œå®ƒå¯ä»¥å°‡å­¸ç¿’å™¨ä¸€å±¤ä¸€å±¤åœ°å †ç Œèµ·ä¾†ï¼Œå½¢æˆä¸€å€‹ç¶²ç‹€çš„çµæ§‹

- Blending

  - Blendingèˆ‡Stackingå¾ˆé¡ä¼¼ï¼Œå®ƒå€‘çš„å€åˆ¥å¯ä»¥åƒè€ƒ[é€™è£¡](http://link.zhihu.com/?target=https%3A//mlwave.com/kaggle-ensembling-guide/)

### å¾Œè™•ç†

æœ‰äº›æ™‚å€™åœ¨ç¢ºèªæ²’æœ‰éæ“¬åˆçš„æƒ…æ³ä¸‹ï¼Œé©—è­‰é›†ä¸Šåšæ ¡é©—æ™‚æ•ˆæœæŒºå¥½ï¼Œä½†æ˜¯å°‡æ¸¬è©¦çµæœæäº¤å¾Œçš„åˆ†æ•¸å»ä¸å¦‚äººæ„ï¼Œé€™æ™‚å€™å°±æœ‰å¯èƒ½æ˜¯è¨“ç·´é›†çš„åˆ†ä½ˆèˆ‡æ¸¬è©¦é›†çš„åˆ†ä½ˆä¸ä¸€æ¨£è€Œå°è‡´çš„ã€‚é€™æ™‚å€™ç‚ºäº†æé«˜LeaderBoardçš„åˆ†æ•¸ï¼Œé‚„éœ€è¦å°æ¸¬è©¦çµæœé€²è¡Œåˆ†ä½ˆèª¿æ•´ã€‚

æ¯”å¦‚é€™æ¬¡æ¯”è³½ï¼Œè¨“ç·´è³‡æ–™ä¸­æ­£é¡çš„å æ¯”ç‚º0.37ï¼Œé‚£éº¼é æ¸¬çµæœä¸­æ­£é¡çš„æ¯”ä¾‹ä¹Ÿåœ¨0.37å·¦å³ï¼Œç„¶å¾ŒKernelä¸Šæœ‰äººé€šéæ¸¬è©¦çŸ¥é“äº†æ¸¬è©¦è³‡æ–™ä¸­æ­£é¡çš„å æ¯”ç‚º0.165ï¼Œæ‰€ä»¥æˆ‘å€‘ä¹Ÿå°é æ¸¬çµæœé€²è¡Œäº†èª¿æ•´ï¼Œå¾—åˆ°äº†æ›´å¥½çš„åˆ†æ•¸ã€‚å…·é«”å¯ä»¥çœ‹[é€™è£¡](http://link.zhihu.com/?target=https%3A//www.kaggle.com/davidthaler/how-many-1-s-are-in-the-public-lb)ã€‚

### åƒè€ƒè³‡æ–™

- åˆ‡åˆ†è¨“ç·´/æ¸¬è©¦è³‡æ–™

  - [How (dis)similar are my train and test data?](https://towardsdatascience.com/how-dis-similar-are-my-train-and-test-data-56af3923de9b)

- é¸å®šè©•ä¼°æŒ‡æ¨™

  - [ç²¾åº¦è¯„å®šä¸­çš„å‡†ç¡®ç‡ï¼ˆPrecisionï¼‰å’Œå¬å›ç‡ï¼ˆRecallï¼‰](https://www.jianshu.com/p/a4d3c393f9b5)
  - [ROC curves and Area Under the Curve explained (video)](https://www.dataschool.io/roc-curves-and-auc-explained/)
  - [æœºå™¨å­¦ä¹ æ¨¡å‹è¯„ä¼°](https://zhuanlan.zhihu.com/p/30721429)

- æ¨¡å‹é¸æ“‡

  - [Logistic Regression â€” Detailed Overview](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)
  - [ç·šæ€§è¿´æ­¸çš„é‹ä½œåŸç†](https://brohrer.mcknote.com/zh-Hant/how_machine_learning_works/how_linear_regression_works.html)
  - [é‚è¼¯æ–¯å›æ­¸(Logistic Regression) ä»‹ç´¹]([https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-3%E8%AC%9B-%E7%B7%9A%E6%80%A7%E5%88%86%E9%A1%9E-%E9%82%8F%E8%BC%AF%E6%96%AF%E5%9B%9E%E6%AD%B8-logistic-regression-%E4%BB%8B%E7%B4%B9-a1a5f47017e5](https://medium.com/jameslearningnote/è³‡æ–™åˆ†æ-æ©Ÿå™¨å­¸ç¿’-ç¬¬3-3è¬›-ç·šæ€§åˆ†é¡-é‚è¼¯æ–¯å›æ­¸-logistic-regression-ä»‹ç´¹-a1a5f47017e5))
  - [ä½ å¯èƒ½ä¸çŸ¥é“çš„é‚è¼¯è¿´æ­¸ (Logistic Regression)](https://taweihuang.hpd.io/2017/12/22/logreg101/)
  - [Linear regression with one variable](https://www.coursera.org/lecture/machine-learning/model-representation-db3jS)
  - [é€»è¾‘å›å½’å¸¸è§é¢è¯•é¢˜æ€»ç»“](https://www.cnblogs.com/ModifyRong/p/7739955.html)
  - [Homemade Machine Learning](https://github.com/trekhleb/homemade-machine-learning)
  - [2 WAYS TO IMPLEMENT MULTINOMIAL LOGISTIC REGRESSION IN PYTHON](http://dataaspirant.com/2017/05/15/implement-multinomial-logistic-regression-python/)
  - [è„Šå›å½’ï¼ˆRidge Regressionï¼‰](https://blog.csdn.net/daunxx/article/details/51578787)
  - [Linear least squares, Lasso,ridge regressionæœ‰ä½•æœ¬è´¨åŒºåˆ«ï¼Ÿ](https://www.zhihu.com/question/38121173)
  - https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b
  - [æ±ºç­–æ¨¹(Decision Tree)ä»¥åŠéš¨æ©Ÿæ£®æ—(Random Forest)ä»‹ç´¹](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)
  - [Letâ€™s Write a Decision Tree Classifier from Scratch - Machine Learning Recipes](https://www.youtube.com/watch?v=LDRbO9a6XPU)
  - [HOW DECISION TREE ALGORITHM WORKS](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)
  - [Creating and Visualizing Decision Trees with Python](https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176)
  - [[ML] Random Forest](http://hhtucode.blogspot.com/2013/06/ml-random-forest.html)
  - [How Random Forest Algorithm Works in Machine Learning](https://medium.com/@Synced/how-random-forest-algorithm-works-in-machine-learning-3c0fe15b6674)
  - [Random Forests - The Math of Intelligence (Week 6)](https://www.youtube.com/watch?v=QHOazyP-YlM)
  - [A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)
  - [ML Lecture 22: Ensemble](https://www.youtube.com/watch?v=tH9FH1DH5n0)
  - [How to explain gradient boosting](https://explained.ai/gradient-boosting/index.html)
  - [GBDTï¸°æ¢¯åº¦æå‡æ±ºç­–æ¨¹](https://ifun01.com/84A3FW7.html)
  - [Kaggle Winning Solution Xgboost Algorithm - Learn from Its Author, Tong He](https://www.youtube.com/watch?v=ufHo8vbk6g4)
  - [Introduction to Boosted Trees](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)
  - [Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)
  - [bootstrapè‡ªé‡‡æ ·å†ç†è§£](https://blog.csdn.net/iterate7/article/details/79740136)
  - [Boosting ç®—æ³•ä»‹ç»](https://zhuanlan.zhihu.com/p/75330932)
  - [xgboostå‚æ•°è°ƒèŠ‚](https://zhuanlan.zhihu.com/p/28672955)
  - [ä¸€æ–‡è¯»æ‡‚æœºå™¨å­¦ä¹ å¤§æ€å™¨XGBooståŸç†](https://zhuanlan.zhihu.com/p/40129825)
  - [Complete Guide to Parameter Tuning in XGBoost with codes in Python](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
  - [XGboostæ•°æ®æ¯”èµ›å®æˆ˜ä¹‹è°ƒå‚ç¯‡(å®Œæ•´æµç¨‹)](https://segmentfault.com/a/1190000014040317)
  - [UnSupervised Learning by Andrew Ng](https://www.youtube.com/watch?v=hhvL-U9_bLQ)
  - [Unsupervised learningï¼šPCA ](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2017/Lecture/PCA.mp4)

  - [Scikit-learn unsupervised learning](http://scikit-learn.org/stable/unsupervised_learning.html)

- **è¶…åƒæ•¸èª¿æ•´èˆ‡æ­¥é©Ÿ**
- [Scanning hyperspace: how to tune machine learning models](https://cambridgecoding.wordpress.com/2016/04/03/scanning-hyperspace-how-to-tune-machine-learning-models/)
  - [Hyperparameter Tuning the Random Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)

- [æ©Ÿå™¨å­¸ç¿’è€ä¸­é†«ï¼šåˆ©ç”¨å­¸ç¿’æ›²ç·šè¨ºæ–·æ¨¡å‹çš„åå·®å’Œæ–¹å·®](http://www.sohu.com/a/218382300_465975)

- [è°ˆè°ˆ Bias-Variance Tradeoff](https://liam.page/2017/03/25/bias-variance-tradeoff/)

- [ML Lecture 1: Regression - Case Study](https://www.youtube.com/watch?v=fegAeph9UaA)

- [ML Lecture 2: Where does the error come from?](https://www.youtube.com/watch?v=D_S6y0Jm6dQ&feature=youtu.be)

- [Awesome Gradient Boosting Papers](https://awesomeopensource.com/project/benedekrozemberczki/awesome-gradient-boosting-papers)



## å»ºæ¨¡æµç¨‹

- è¨­å®šè©•ä¼°æŒ‡æ¨™
- åˆ‡åˆ†è¨“ç·´æ¸¬è©¦è³‡æ–™
- 

## Model tunning

- è¶…åƒæ•¸èª¿æ•´æ–¹æ³•

  - çª®èˆ‰æ³• (Grid Search)ï¼šç›´æ¥æŒ‡å®šè¶…åƒæ•¸çš„çµ„åˆç¯„åœï¼Œæ¯â¼€çµ„åƒæ•¸éƒ½è¨“ç·´å®Œæˆï¼Œå†æ ¹æ“šé©—è­‰é›† (validation) çš„çµæœé¸æ“‡æœ€ä½³åƒæ•¸
  - éš¨æ©Ÿæœå°‹ (Random Search)ï¼šæŒ‡å®šè¶…åƒæ•¸çš„ç¯„åœï¼Œç”¨å‡å‹»åˆ†å¸ƒé€²â¾åƒæ•¸æŠ½æ¨£ï¼Œç”¨æŠ½åˆ°çš„åƒæ•¸é€²â¾è¨“ç·´ï¼Œå†æ ¹æ“šé©—è­‰é›†çš„çµæœé¸æ“‡æœ€ä½³åƒæ•¸
  - éš¨æ©Ÿæœå°‹é€šå¸¸éƒ½èƒ½ç²å¾—æ›´ä½³çš„çµæœï¼Œè©³â¾’[Smarter Parameter Sweeps (or Why Grid Search Is Plain Stupid)](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881)

- è¶…åƒæ•¸èª¿æ•´æ­¥é©Ÿ

  - è‹¥æŒçºŒä½¿â½¤åŒâ¼€ä»½é©—è­‰é›† (validation) ä¾†èª¿åƒï¼Œå¯èƒ½è®“æ¨¡å‹çš„åƒæ•¸éæ–¼æ“¬åˆè©²é©—è­‰é›†ï¼Œæ­£ç¢ºçš„æ­¥é©Ÿæ˜¯ä½¿â½¤ Cross-validation ç¢ºä¿æ¨¡å‹æ³›åŒ–æ€§

    - å…ˆå°‡è³‡æ–™åˆ‡åˆ†ç‚ºè¨“ç·´/æ¸¬è©¦é›†ï¼Œæ¸¬è©¦é›†ä¿ç•™ä¸ä½¿â½¤

    - å°‡å‰›åˆ‡åˆ†å¥½çš„è¨“ç·´é›†ï¼Œå†ä½¿â½¤Cross-validation åˆ‡åˆ† K ä»½è¨“ç·´/é©—è­‰é›†

      ,è—‰ç”± grid/random search çš„è¶…åƒæ•¸é€²â¾è¨“ç·´èˆ‡è©•ä¼°

    - é¸å‡ºæœ€ä½³çš„åƒæ•¸ï¼Œâ½¤è©²åƒæ•¸èˆ‡å…¨éƒ¨è¨“ç·´é›†å»ºæ¨¡

    - æœ€å¾Œä½¿â½¤æ¸¬è©¦é›†è©•ä¼°çµæœ

    ![Cross Validation](https://lh3.googleusercontent.com/AyYCe_Yfd-SzpV38jADYV0DMNCHgCPOCkrglQXOpO8D8JeyuUfrEmhdIIiB6uCLIeg48H9Ypu59tguI2MsunnUUJv5L3yU0v1pc9tKMDwt-nW4hVM7I5UdHC0VyfXPUOljwpX0RD6wRGeoOzHhsAzu49mMTq2a_Tj1lnnLW_834qo38hJzZMhbHnc_9N9XYP2BzcwPAgyxjWxCmcawSqWxSRHvOzqX__9oFSgEQRUYBX4OD0NazCnYYmIxVHYQ-7FNMe5-MuYIyw7NtKdFnoJJNdis_YyjXTz7R9XXYfwgpJMbPIzbH_IykYlfriDdATNGm5uTkXnKs3H-bgymX4I4_Y8fs9B5IOYkkUPHS-JlHdKElZijbQjjWN5riQsWRg2hu0rpQnQv6oowJlyWSmz2uwceUc-EuOIgVIaGqIocSArSD1HTgomxqlPVzNMFQtvaauaQ26mexDy-layK-yUK_ACs-XGI3H7QNgKkJSe1vSqA6qlR2qphXACOOxMaa6EJ4CjxJ8ifta7PK4DhrsujI4r4eVlQ1XRim3JULteCOriC17byiUxOB-8s97x1ZuCbNreF-fIRjg92ZzNyEgBZymbFIGz0Zi9_uNgYZp2QLexZO-Bi2tWFpGvy6HFU53r933bHtGCwHBj7mwSRvmiaVb-5graFdw-eyRKVOsF6V-RDv22xv-AZbYlwptin2ZXurtnbgHvRnyguAePWJ6wW42=w958-h583-no)

```python
from sklearn.model_selection import GridSearchCV
parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},
              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]
grid_search = GridSearchCV(estimator = classifier,
                           param_grid = parameters,
                           scoring = 'accuracy',
                           cv = 10,
                           n_jobs = -1)
grid_search = grid_search.fit(X_train, y_train)
best_accuracy = grid_search.best_score_
best_parameters = grid_search.best_params_
print("Best Accuracy: {:.2f} %".format(best_accuracy*100))
print("Best Parameters:", best_parameters)
```



- å¸¸è¦‹å•é¡Œ
  - å°è‡´ Overfitting çš„å•é¡Œä¸»è¦æœ‰å…©ç¨®ï¼Œä¸€ç¨®æ˜¯æ¨¡å‹ï¼Œä¸€ç¨®æ˜¯åƒæ•¸æ•¸é‡
    - é›–ç„¶ç·šæ€§æ¨¡å‹ç›¸è¼ƒéç·šæ€§æ¨¡å‹è¼ƒä¸å®¹æ˜“æœ‰overfittingçš„å•é¡Œï¼Œä½†ç•¶åƒæ•¸ä¸€å¤šæ™‚ä»ç„¶æœƒæœ‰é€™å€‹ç‹€æ³ç™¼ç”Ÿ!
  - é€™äº›æ¨¡å‹çš„æ•¸å­¸å¼â¼¦éƒ½å¾ˆå¤šï¼Œâ¼€å®šè¦å®Œå…¨çœ‹æ‡‚æ‰ç¹¼çºŒå¾€ä¸‹å—? ä¸æœƒæ¨å°å¯ä»¥å—?
    - å›æ­¸æ¨¡å‹æ˜¯æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ä¸­çš„åŸºç¤ï¼Œé›–ç„¶å¯¦å‹™ä¸Šæ‡‰â½¤çš„æ©Ÿæœƒä¸å¤š (å› ç‚ºæ¨¡å‹éæ–¼ç°¡å–®)ï¼Œä½†æ˜¯ä¹‹å¾Œæ›´è¤‡é›œçš„æ¨¡å‹éƒ½æ˜¯åŸºæ–¼å›æ­¸æ¨¡å‹åšåŠ å¼·ï¼Œæ‰€ä»¥å°åŸºæœ¬åŸç†æœ‰â¼€å®šçš„äº†è§£æœƒæ¯”è¼ƒå¥½ã€‚ç•¢ç«Ÿ Python ä½¿â½¤ç·šæ€§å›æ­¸åªè¦â¼€â¾ç¨‹å¼ç¢¼ï¼Œä½†æ˜¯ä¸äº†è§£åŸç†ï¼Œå°±æœƒé™·å…¥ç•¶é‡åˆ°éŒ¯èª¤ä¸çŸ¥å¦‚ä½•ä¿®æ­£çš„æƒ…æ³ã€‚
  - Lasso è·Ÿ Ridge éƒ½æ˜¯å›æ­¸å•é¡Œçš„æ¨¡å‹ï¼Œé‚£éº¼åœ¨ä½¿â½¤æ™‚æ‡‰è©²å…ˆâ½¤å“ªå€‹æ¨¡å‹è·‘å‘¢ï¼Ÿ
    - å¾æ¨¡å‹çš„ç‰¹æ€§ä¾†çœ‹ï¼ŒLasso ä½¿â½¤çš„æ˜¯ L1 regularizationï¼Œé€™å€‹æ­£å‰‡åŒ–çš„ç‰¹æ€§æœƒè®“æ¨¡å‹è®Šå¾—è¼ƒç‚ºç¨€ç–ï¼Œé™¤äº†èƒ½åšç‰¹å¾µé¸å–å¤–ï¼Œä¹Ÿæœƒè®“æ¨¡å‹è®Šå¾—æ›´è¼•é‡ï¼Œé€Ÿåº¦è¼ƒå¿«ã€‚
    - å¯¦å‹™ä¸Šå› ç‚ºè¨“ç·´å›æ­¸æ¨¡å‹éå¸¸å®¹æ˜“ï¼Œå¯ä»¥å…©è€…éƒ½è·‘è·‘çœ‹ï¼Œåœ¨æ¯”è¼ƒæº–ç¢ºç‡ï¼Œæ‡‰è©²ä¸æœƒæœ‰å¤ªâ¼¤çš„å·®ç•°ï¼


https://www.analyticsvidhya.com/blog/2020/02/underfitting-overfitting-best-fitting-machine-learning/

åœ¨è¨“ç·´æ¨¡å‹å‰ï¼Œæˆ‘å€‘éœ€è¦é è¨­ä¸€äº›åƒæ•¸ä¾†ç¢ºå®š**æ¨¡å‹çµæ§‹**ï¼ˆæ¯”å¦‚æ¨¹çš„æ·±åº¦ï¼‰å’Œ**å„ªåŒ–éç¨‹**ï¼ˆæ¯”å¦‚å­¸ç¿’ç‡ï¼‰ï¼Œé€™ç¨®åƒæ•¸è¢«ç¨±ç‚ºè¶…åƒï¼ˆHyper-parameterï¼‰ï¼Œä¸åŒçš„åƒæ•¸æœƒå¾—åˆ°çš„æ¨¡å‹æ•ˆæœä¹Ÿæœƒä¸åŒã€‚ç¸½æ˜¯èªªèª¿åƒå°±åƒæ˜¯åœ¨â€œç…‰ä¸¹â€ï¼Œåƒä¸€é–€â€œç„å­¸â€ï¼Œä½†æ˜¯æ ¹æ“šç¶“é©—ï¼Œé‚„æ˜¯å¯ä»¥æ‰¾åˆ°ä¸€äº›ç« æ³•çš„ï¼š

1. æ ¹æ“šç¶“é©—ï¼Œé¸å‡ºå°æ¨¡å‹æ•ˆæœ**å½±éŸ¿è¼ƒå¤§çš„è¶…åƒ**ã€‚

2. æŒ‰ç…§ç¶“é©—è¨­ç½®è¶…åƒçš„**æœç´¢ç©ºé–“**ï¼Œæ¯”å¦‚å­¸ç¿’ç‡çš„æœç´¢ç©ºé–“ç‚º[0.001ï¼Œ0.1]ã€‚

3. é¸æ“‡**æœç´¢æ¼”ç®—æ³•**ï¼Œæ¯”å¦‚Random Searchã€Grid Searchå’Œä¸€äº›å•Ÿç™¼å¼æœç´¢çš„æ–¹æ³•ã€‚

4. **é©—è­‰æ¨¡å‹**çš„æ³›åŒ–èƒ½åŠ›



## Model Selection

- Type of parameter
  - parameterï¼šThe model learns that is the parameters that were changed and found optimal values by running the model 
  - Hyperparameter: We chose ourselves,nor learned by the modelWe need to figure out by gridsearch or randomsearch. For example the kernel parameter in the kernel as we model and these parameters.
- There is still room to improve the model because we can still choose some optimal values for these hyperparameters.

```python
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))
```



## å»ºç«‹ pipeline

```python
pipr_lr = Pipeline(steps=[('cv', CountVectorizer()), ('lr', LogisticRegression())])

from sklearn import set_config
set_config(display='diagram')

#Accuracy
pipr_lr.score(x_test, y_test)
```



## æ¨¡å‹è¨ºæ–·

- æˆ‘å€‘é€šå¸¸æœƒç‚ºäº†æå‡æ¨¡å‹çš„æº–ç¢ºåº¦ï¼Œæœƒç›¡å¯èƒ½çš„å¢åŠ æ¨¡å‹çš„è¤‡é›œåº¦ã€‚ä½†æ˜¯ç•¶æ¨¡å‹çš„è¤‡é›œåº¦æå‡æ™‚ä¼´éš¨è€Œä¾†çš„å°±æ˜¯å¯è§£é‡‹æ€§å°±éš¨ä¹‹é™ä½ã€‚
- ç•¶æ¨¡å‹å¾ˆæº–ç¢ºä¸”ç„¡éœ€è§£é‡‹æ™‚ï¼Œå›ºç„¶å¯ä»¥ç›´æ¥ä½¿ç”¨è¤‡é›œçš„æ¨¡å‹ï¼Œä½†åœ¨å•†æ¥­ç’°å¢ƒä¸­å‰‡å¾€å¾€æœƒéœ€è¦è§£é‡‹æ¨¡å‹ç™¼ç¾äº†ä»€éº¼ï¼Œå› ç‚ºé™¤äº†é æ¸¬çš„æº–ç¢ºåº¦ä¹‹å¤–ï¼Œå…¬å¸ä¹Ÿæœƒå¸Œæœ›æ¨¡å‹å”åŠ©ç®¡ç†ï¼Œå¦‚ç‡Ÿé‹ç¸¾æ•ˆã€æœå‹™æµç¨‹ç­‰ç­‰ã€‚
- åˆæˆ–è€…ç•¶æ¨¡å‹çš„æº–ç¢ºåº¦ä¸å¤ æ™‚æœƒéœ€è¦å°æ¨¡å‹é€²è¡Œè¨ºæ–·ï¼Œæ‰¾å‡ºæ¨¡å‹çŠ¯éŒ¯çš„åŸå› ï¼Œä¾†æ”¹å–„æ¨¡å‹çš„é æ¸¬çµæœã€‚ 
- æ¬ æ“¬åˆ
  - ç‰¹å¾µèƒå–
- éæ“¬åˆ
  - ç‰¹å¾µé¸æ“‡
  
  ![](./images/88104188_1453591268135516_255250267178532864_o.png)

- Ref
  - [æœºå™¨å­¦ä¹ è€ä¸­åŒ»ï¼šåˆ©ç”¨å­¦ä¹ æ›²çº¿è¯Šæ–­æ¨¡å‹çš„åå·®å’Œæ–¹å·® ](https://www.sohu.com/a/218382300_465975)

### Bias and Variance
- [åå·®ï¼ˆBiasï¼‰ä¸æ–¹å·®ï¼ˆVariance)](https://cloud.tencent.com/developer/article/1012465)
- [ä¸ºä»€ä¹ˆè¯´baggingæ˜¯å‡å°‘varianceï¼Œè€Œboostingæ˜¯å‡å°‘bias?](https://www.zhihu.com/question/26760839)
- [æœºå™¨å­¦ä¹ è€ä¸­åŒ»ï¼šåˆ©ç”¨å­¦ä¹ æ›²çº¿è¯Šæ–­æ¨¡å‹çš„åå·®å’Œæ–¹å·®](https://cloud.tencent.com/developer/article/1119597?areaSource=106002.15)
- [ã€æ©Ÿå™¨å­¸ç¿’ã€‘åå·®èˆ‡æ–¹å·®ä¹‹æ¬Šè¡¡ Bias-Variance Tradeoff](https://jason-chen-1992.weebly.com/home/-bias-variance-tradeoff)

### OverFit

- éæ“¬åˆ (Over-fitting)

  - æ¨¡å‹çš„è¨“ç·´â½¬æ¨™æ˜¯å°‡æå¤±å‡½æ•¸çš„æå¤±é™â¾„æœ€ä½

  - éæ“¬åˆä»£è¡¨æ¨¡å‹å¯èƒ½å­¸ç¿’åˆ°è³‡æ–™ä¸­çš„å™ªâ¾³ï¼Œå°è‡´åœ¨å¯¦éš›æ‡‰â½¤æ™‚é æ¸¬å¤±æº–

- å¦‚ä½•çŸ¥é“æ¨¡å‹å·²ç¶“éæ“¬åˆäº†?

  - ä¿ç•™â¼€äº›æ¸¬è©¦è³‡æ–™ï¼Œè§€å¯Ÿæ¨¡å‹å°æ–¼è¨“ç·´è³‡æ–™çš„èª¤å·®èˆ‡æ¸¬è©¦è³‡æ–™çš„èª¤å·®ï¼Œæ˜¯å¦æœ‰æ”¹è®Šçš„è¶¨å‹¢(å­¸ç¿’æ›²ç·š Learning curve)

- å¦‚ä½•è§£æ±ºéæ“¬åˆæˆ–æ¬ æ“¬åˆ**

  - éæ“¬åˆ
  - å¢åŠ è³‡æ–™é‡
    - é™ä½æ¨¡å‹è¤‡é›œåº¦
  - ä½¿â½¤æ­£è¦åŒ– (Regularization)
  - â½‹æ“¬åˆ
  - å¢åŠ æ¨¡å‹è¤‡é›œåº¦
    - æ¸›è¼•æˆ–ä¸ä½¿â½¤æ­£è¦åŒ–

  ![](https://lh3.googleusercontent.com/LX_68rjUR9qhcmgY6IKZaBFmoEG_xsOiHx8scVquqB7nrwHHSvlB8JJ74OpZxlPOS4Vyv04LRc2bTChyXOVx5eZQl2v6s2DGyhdCHy_UFD7QzZOlsPNFhZ-Ogxi0uP0RevdIe0qQs0YMu4XiOYpoR8KY1rPH9oci-z0W0-lx2JLeopj2gAZUpbvol2uwUqS0aR29-5DnfWka5Bp6ua5Urkb9ai0BWMejvG3ZiJDgAANypm0qrBbQvWFTQCS79qyxalNL3HoQvZlrimGf_IviHUADpDOMnyxNUrXOzAthzdht3CqpDZ6UgL2TDQtXs9W6xXYdhp4cZPKZhAOHKOT7KDhQfrHVrCAmFCFy7rbubY6VTAreKknnK--GAHct3UDoOWVA7aFmNFkwqYUjPLaq4IzRhDqfvP2HSeoTij0GtfvpNIbQP7RSr08Qmf1P-lkdxQnP_JBydYLvwufPi0OKle5sFXIlgn6ugR1yzg9HxAxAsOf7iVZi17ZLprA5VVEEWds__ZEBBYfp3dxuBi5rj4cYZRSc0OgYob4MYPcNkP1J9a54mAups7xNxwyQdySBBYmMgsMetfd056fIS88iPPbMQhqUT15NaxOBNNS1X8T44MixoiI4maFwxU5PWZFJwZuUq6R_YWPoAI5QC2lZ_m2Nj-VtU5ZTHkhlurasDP3JlEFj6x-vnXs1a35qlmkzaqlBaJbMPoJY3bWpPMXBKjUD=w958-h333-no)

- åˆ‡åˆ†å®Œè¨“ç·´æ¸¬è©¦è³‡æ–™éœ€è¦æ¯”è¼ƒå…©è€…æ˜¯å¦æœ‰å·®ç•°

  - å°‡åˆ‡å‡ºçš„è¨“ç·´/æ¸¬è©¦è³‡æ–™ä½œç‚º Y æ¨™ç±¤ä¾†å»ºç½® RandomForestæ¨¡å‹ï¼Œçœ‹æ¨¡å‹èƒ½ä¸èƒ½æº–ç¢ºå€åˆ†å‡ºå…©è€…
  - å¦‚æœèƒ½å°±å°‡è©²æ¨¡å‹çš„é‡è¦è®Šæ•¸ä¸Ÿæ‰ï¼Œä¸¦åœ¨å¾ŒçºŒçš„å»ºæ¨¡æµç¨‹ä¸­æ’é™¤



#### Feature Selection

- åœ¨åšç‰¹å¾µæŠ½å–çš„æ™‚å€™ï¼Œæˆ‘å€‘æ˜¯ç›¡å¯èƒ½åœ°æŠ½å–æ›´å¤šçš„Featureï¼Œä½†éå¤šçš„Featureæœƒé€ æˆå†—é¤˜ï¼Œé›œè¨Šï¼Œå®¹æ˜“éæ“¬åˆç­‰å•é¡Œï¼Œå› æ­¤æˆ‘å€‘éœ€è¦é€²è¡Œç‰¹å¾µç¯©é¸ã€‚ç‰¹å¾µé¸æ“‡èƒ½å‰”é™¤ä¸ç›¸é—œ(irrelevant)æˆ–å†—é¤˜(redundant)çš„ç‰¹å¾µï¼Œå¾è€Œé”åˆ°æ¸›å°‘ç‰¹å¾µå€‹æ•¸ï¼Œæé«˜æ¨¡å‹ç²¾ç¢ºåº¦ï¼Œæ¸›å°‘åŸ·è¡Œæ™‚é–“çš„ç›®çš„ã€‚

- å¦ä¸€æ–¹é¢ï¼Œé¸å–å‡ºçœŸæ­£ç›¸é—œçš„ç‰¹å¾µç°¡åŒ–æ¨¡å‹ï¼Œå”åŠ©ç†è§£è³‡æ–™ç”¢ç”Ÿçš„éç¨‹ã€‚

  - Garbage In Garbage Out
  - [å¥¥å¡å§†å‰ƒåˆ€åŸç†](https://zhuanlan.zhihu.com/p/45321953)

- è®“å¾ŒçºŒè§£é‡‹æ›´ç°¡å–®

- All-in

  åŸºæ–¼ Domain Knowledgeï¼Œæˆ–è¨­è¨ˆéçš„èª¿æŸ¥è³‡æ–™æ™‚æœƒä½¿ç”¨æ­¤æ–¹æ³•ã€‚æ­¤å¤–é€šå¸¸åœ¨å»ºç½®æ¨¡å‹æ˜¯ä¹Ÿæœƒå‡ºæ–¼èˆ‡å…¶ä»–æ¨¡å‹åšæ¯”è¼ƒçš„è€ƒé‡ï¼Œå»ºç½®ä¸€å€‹é€™é¡çš„æ¨¡å‹ï¼Œè—‰ä»¥æª¢è¦–å¾ŒçºŒå…¶ä»–æ¨¡å‹çš„æ•ˆåº¦ã€‚

- ç‰¹å¾µé¸æ“‡æœ‰ä¸‰â¼¤é¡â½…æ³•

  - éæ¿¾æ³• (Filter) : é¸å®šçµ±è¨ˆæ•¸å€¼èˆ‡è¨­å®šâ¾¨æª»ï¼Œåˆªé™¤ä½æ–¼â¾¨æª»çš„ç‰¹å¾µ
  - åŒ…è£æ³• (Wrapper) : æ ¹æ“šâ½¬æ¨™å‡½æ•¸ï¼Œé€æ­¥åŠ å…¥ç‰¹å¾µæˆ–åˆªé™¤ç‰¹å¾µ

- åµŒå…¥æ³• (Embedded) : ä½¿â½¤æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼Œæ ¹æ“šæ“¬åˆå¾Œçš„ä¿‚æ•¸ï¼Œåˆªé™¤ä¿‚æ•¸ä½æ–¼â¾¨æª»çš„ç‰¹å¾µ

  



##### éæ¿¾æ³•(Filter)

æŒ‰ç…§ç™¼æ•£æ€§æˆ–è€…ç›¸é—œæ€§å°å„å€‹ç‰¹å¾µé€²è¡Œè©•åˆ†ï¼Œè¨­å®šé–¾å€¼æˆ–è€…å¾…é¸æ“‡é–¾å€¼çš„å€‹æ•¸é¸æ“‡ç‰¹å¾µã€‚

- æ–¹å·®é¸æ“‡
  å…ˆè¦è¨ˆç®—å„å€‹ç‰¹å¾µçš„æ–¹å·®ï¼Œç„¶å¾Œæ ¹æ“šé–¾å€¼ï¼Œé¸æ“‡æ–¹å·®å¤§æ–¼é–¾å€¼çš„ç‰¹å¾µ

  ```python
  from sklearn.feature_selection import VarianceThreshold
  
  #æ–¹å·®é€‰æ‹©æ³•ï¼Œè¿”å›å€¼ä¸ºç‰¹å¾é€‰æ‹©åçš„æ•°æ®
  #å‚æ•°thresholdä¸ºæ–¹å·®çš„é˜ˆå€¼
  VarianceThreshold(threshold=3).fit_transform(iris.data)
  ```

- ç›¸é—œä¿‚æ•¸
  - çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸æ˜¯ä¸€ç¨®æœ€ç°¡å–®çš„ï¼Œèƒ½èª¬æ˜ç†è§£ç‰¹å¾µå’Œå›æ‡‰è®Šæ•¸ä¹‹é–“é—œä¿‚çš„æ–¹æ³•ï¼Œè©²æ–¹æ³•è¡¡é‡çš„æ˜¯è®Šæ•¸ä¹‹é–“çš„ç·šæ€§ç›¸é—œæ€§ï¼Œçµæœçš„å–å€¼å€é–“ç‚º $-1$ è‡³ $1$  ï¼Œ $-1$ è¡¨ç¤ºå®Œå…¨çš„è² ç›¸é—œ(é€™å€‹è®Šæ•¸ä¸‹é™ï¼Œé‚£å€‹å°±æœƒä¸Šå‡)ï¼Œ$+1$ è¡¨ç¤ºå®Œå…¨çš„æ­£ç›¸é—œï¼Œ$0$ è¡¨ç¤ºæ²’æœ‰ç·šæ€§ç›¸é—œã€‚

  - Pearsonç›¸é—œä¿‚æ•¸çš„ä¸€å€‹æ˜é¡¯ç¼ºé™·æ˜¯ï¼Œä½œç‚ºç‰¹å¾µæ’åºæ©Ÿåˆ¶ï¼Œä»–åªå°ç·šæ€§é—œä¿‚æ•æ„Ÿã€‚å¦‚æœé—œä¿‚æ˜¯éç·šæ€§çš„ï¼Œå³ä¾¿å…©å€‹è®Šæ•¸å…·æœ‰ä¸€ä¸€å°æ‡‰çš„é—œä¿‚ï¼ŒPearsonç›¸é—œæ€§ä¹Ÿå¯èƒ½æœƒæ¥è¿‘ $0$

  ```python
  from sklearn.feature_selection import SelectKBest
  from scipy.stats import pearsonr
  
  #é€‰æ‹©Kä¸ªæœ€å¥½çš„ç‰¹å¾ï¼Œè¿”å›é€‰æ‹©ç‰¹å¾åçš„æ•°æ®
  #ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºè®¡ç®—è¯„ä¼°ç‰¹å¾æ˜¯å¦å¥½çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°è¾“å…¥ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å‘é‡ï¼Œè¾“å‡ºäºŒå…ƒç»„ï¼ˆè¯„åˆ†ï¼ŒPå€¼ï¼‰çš„æ•°ç»„ï¼Œæ•°ç»„ç¬¬ié¡¹ä¸ºç¬¬iä¸ªç‰¹å¾çš„è¯„åˆ†å’ŒPå€¼ã€‚åœ¨æ­¤å®šä¹‰ä¸ºè®¡ç®—ç›¸å…³ç³»æ•°
  #å‚æ•°kä¸ºé€‰æ‹©çš„ç‰¹å¾ä¸ªæ•°
  SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
  ```

  

- å¡æ–¹æª¢é©—(K-Best)
  - å‚³çµ±çš„å¡æ–¹æª¢é©—æ˜¯æª¢é©—é¡åˆ¥è®Šæ•¸å°é¡åˆ¥ç›®æ¨™è®Šæ•¸çš„ç›¸é—œæ€§ã€‚å‡è¨­è‡ªè®Šæ•¸æœ‰ $N$ ç¨®å–å€¼ï¼Œç›®æ¨™è®Šæ•¸æœ‰ $M$ ç¨®å–å€¼ï¼Œè€ƒæ…®è‡ªè®Šæ•¸ç­‰æ–¼ $i$ ä¸”ç›®æ¨™è®Šæ•¸ç­‰æ–¼ $j$ çš„æ¨£æœ¬é »æ•¸çš„è§€å¯Ÿå€¼èˆ‡æœŸæœ›çš„å·®è·ï¼Œæ§‹å»ºçµ±è¨ˆé‡ï¼š

  $$
  \chi^2 = \sum \frac{(A-E)^2}{E}
  $$

  ```python
  from sklearn.datasets import load_iris
  from sklearn.feature_selection import SelectKBest
  from sklearn.feature_selection import chi2
  iris = load_iris()
  X, y = iris.data, iris.target
  X.shape
  # (150, 4)
  # è¿´æ­¸ï¼šf_regression
  # åˆ†é¡ï¼šchi2, f_classif
  X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
  X_new.shape
  # (150, 2)
  ```

  



##### åŒ…è£…æ³•(Wrapper)

åŒ…è£¹å‹æ˜¯æŒ‡æŠŠç‰¹å¾µé¸æ“‡çœ‹åšä¸€å€‹ç‰¹å¾µå­é›†æœç´¢å•é¡Œï¼Œæ ¹æ“šç›®æ¨™å‡½æ•¸ï¼ˆé€šå¸¸æ˜¯é æ¸¬æ•ˆæœè©•åˆ†ï¼‰ï¼Œæ¯æ¬¡é¸æ“‡/åˆªé™¤è‹¥å¹²ç‰¹å¾µï¼Œè—‰ä»¥è©•ä¼°æ•ˆæœã€‚

- Forward Selection(å‘å‰æœç´¢)
  1. å…ˆè¨­å®šä¸€å€‹é¡¯è‘—æ°´æº–/é‡è¦æ€§
  2. é€ä¸€é‡å°ï¼Œæ¯å€‹ X è®Šæ•¸å° Y å»ºç«‹æ¨¡å‹ï¼Œå¾ä¸­ä¿ç•™æœ€é¡¯è‘— / é‡è¦æ€§æœ€é«˜çš„è®Šæ•¸
  3. é€ä¸€é‡å°å…¶é¤˜çš„ Xè®Šæ•¸ä¸¦åŠ å…¥ä¿ç•™ä¸‹ä¾†çš„è®Šæ•¸ï¼Œå°Yå»ºç«‹æ¨¡å‹ï¼Œå¾ä¸­ä¿ç•™æœ€é¡¯è‘—/æœ€é‡è¦çš„è®Šæ•¸
  4. è§€æ¸¬æ¨¡å‹çš„æ•ˆæœæ˜¯å¦æœ‰æå‡ï¼Œè‹¥æœ‰å‰‡é‡è¤‡ç¬¬3å€‹æ­¥é©Ÿ
  5. æœ€å¾Œåªæœƒç•™ä¸‹æœ€ä½³è§£é‡‹æ•ˆæœçš„è®Šæ•¸ï¼Œä¸¦è—‰ä»¥å»ºå‡ºçš„æ¨¡å‹

- Backwark Elimination(å‘å¾Œæœç´¢)

  æœ€å¿«é€Ÿï¼Œè€Œä¸”çœ‹å¾—åˆ°éç¨‹

  1. å…ˆè¨­å®šä¸€å€‹é¡¯è‘—æ°´æº–/é‡è¦æ€§
  2. å°‡å…¨éƒ¨è®Šæ•¸æŠ•å…¥åˆ°æ¨¡å‹ä¸­
  3. æ‰¾å‡ºæœ€ä¸é¡¯è‘—(på€¼æœ€é«˜)/é‡è¦æ€§æœ€ä½çš„è®Šæ•¸ï¼Œä¸¦ç§»é™¤è©²è®Šæ•¸ï¼Œä¸¦é‡æ–°å»ºæ¨¡
  4. è§€æ¸¬ç§»é™¤å¾Œçš„æ¨¡å‹è¡¨ç¾æ˜¯å¦æœ‰è¼ƒç§»é™¤å‰æå‡ï¼Œè‹¥æœ‰ï¼Œå›åˆ°ç¬¬3å€‹æ­¥é©Ÿç¹¼çºŒåŸ·è¡Œ
  5. æœ€å¾Œåªæœƒç•™ä¸‹æœ€ä½³è§£é‡‹æ•ˆæœçš„è®Šæ•¸ï¼Œä¸¦è—‰ä»¥å»ºå‡ºçš„æ¨¡å‹

- Bidirectional Elimination(éæ­¸ç‰¹å¾µæ¶ˆé™¤æ³•)

  çµåˆå‰å…©ç¨®æ–¹æ³•ï¼Œç”±æ–¼æ¨¡å‹åœ¨æ–°å¢è®Šæ•¸æ™‚æœƒå½±éŸ¿åˆ°å…¶ä»–è®Šæ•¸çš„é¡¯è‘—æ€§/é‡è¦æ€§ï¼Œå› æ­¤åœ¨æ–°å¢è®Šæ•¸å¾ŒåŒæ­¥ç¢ºèªæ™‚å€™æœ‰è®Šæ•¸è®Šå¾—ä¸é¡¯è‘—ï¼Œæ­¤æ™‚éœ€è¦å°‡é€™é¡è®Šæ•¸å¾æ¨¡å‹ä¸­ç§»é™¤ã€‚å¾Œåªæœƒç•™ä¸‹æœ€ä½³è§£é‡‹æ•ˆæœçš„è®Šæ•¸ï¼Œä¸¦è—‰ä»¥å»ºå‡ºçš„æ¨¡å‹ã€‚

  1. è¨­å®šè¦é¸å…¥è®Šæ•¸/ç§»é™¤è®Šæ•¸çš„é–¾å€¼(é¡¯è‘—æ°´æº–/é‡è¦æ€§)
  2. åŸ·è¡Œ Forward Selection æ‰¾å‡ºæœ€é‡è¦çš„è®Šæ•¸åŠ å…¥è‡³æ¨¡å‹ä¸­
  3. é‡å°ç›®å‰é¸å…¥çš„è®Šæ•¸åŸ·è¡Œ Backwark Eliminationï¼Œç¢ºèªæ‰€æœ‰è®Šæ•¸çš„é–¾å€¼éƒ½æœ‰ç¬¦åˆè¨­å®šçš„æ¢ä»¶ï¼ŒåŸ·è¡Œå®Œå¾Œå›åˆ°æ­¥é©Ÿ2ï¼Œç¹¼çºŒæ‰¾æ–°è®Šæ•¸ã€‚
  4. ç›´åˆ°æ²’æœ‰è®Šæ•¸å¯ä»¥æ–°å¢/ç§»é™¤å¾Œæ‰çµæŸ

  ```python
  # RFE
  from sklearn.feature_selection import RFE
  # EFCV
  from sklearn.feature_selection import RFECV
  ```

  

##### åµŒå…¥æ³•(Embedded)

å…ˆä½¿ç”¨æŸäº›æ©Ÿå™¨å­¸ç¿’çš„æ¼”ç®—æ³•å’Œæ¨¡å‹é€²è¡Œè¨“ç·´ï¼Œå¾—åˆ°å„å€‹ç‰¹å¾µçš„æ¬Šå€¼ä¿‚æ•¸ï¼Œæ ¹æ“šä¿‚æ•¸å¾å¤§åˆ°å°é¸æ“‡ç‰¹å¾µã€‚é¡ä¼¼æ–¼Filteræ–¹æ³•ï¼Œä½†æ˜¯æ˜¯é€šéè¨“ç·´ä¾†ç¢ºå®šç‰¹å¾µçš„å„ªåŠ£ã€‚



- åŸºæ–¼æ‡²ç½°é …çš„ç‰¹å¾µé¸æ“‡æ³•(Lasso)
  - é€šéL1æ­£å‰‡é …ä¾†é¸æ“‡ç‰¹å¾µï¼šL1æ­£å‰‡æ–¹æ³•å…·æœ‰ç¨€ç–è§£çš„ç‰¹æ€§ï¼Œå› æ­¤å¤©ç„¶å…·å‚™ç‰¹å¾µé¸æ“‡çš„ç‰¹æ€§ï¼Œä½†æ˜¯è¦æ³¨æ„ï¼ŒL1æ²’æœ‰é¸åˆ°çš„ç‰¹å¾µä¸ä»£è¡¨ä¸é‡è¦ï¼ŒåŸå› æ˜¯å…©å€‹å…·æœ‰é«˜ç›¸é—œæ€§çš„ç‰¹å¾µå¯èƒ½åªä¿ç•™äº†ä¸€å€‹ï¼Œå¦‚æœè¦ç¢ºå®šå“ªå€‹ç‰¹å¾µé‡è¦æ‡‰å†é€šéL2æ­£å‰‡æ–¹æ³•äº¤å‰æª¢é©—ã€‚

  - å°æ–¼SVMå’Œlogisticå›æ­¸ä¾†èªªï¼Œåƒæ•¸Cæ§åˆ¶è‘—ç¨€ç–æ€§ï¼šCè¶Šå°ï¼Œé¸æ“‡åˆ°çš„featureså°±è¶Šå°‘ã€‚è€Œå°æ–¼Lassoï¼Œalphaçš„å€¼è¶Šå¤§ï¼Œå‰‡é¸æ“‡åˆ°çš„featuresè¶Šå°‘ã€‚

  - L1æ‡²ç½°é …é™ç¶­çš„åŸç†åœ¨æ–¼ä¿ç•™å¤šå€‹å°ç›®æ¨™å€¼å…·æœ‰åŒç­‰ç›¸é—œæ€§çš„ç‰¹å¾µä¸­çš„ä¸€å€‹ï¼Œæ‰€ä»¥æ²’é¸åˆ°çš„ç‰¹å¾µä¸ä»£è¡¨ä¸é‡è¦ã€‚æ•…å¯çµåˆL2æ‡²ç½°é …ä¾†å„ªåŒ–ã€‚
    - L1æ­£å‰‡åŒ–æ˜¯æŒ‡æ¬Šå€¼å‘é‡wä¸­å„å€‹å…ƒç´ çš„çµ•å°å€¼ä¹‹å’Œ,L1æ­£å‰‡åŒ–å¯ä»¥ç”¢ç”Ÿç¨€ç–æ¬Šå€¼çŸ©é™£ï¼Œå³ç”¢ç”Ÿä¸€å€‹ç¨€ç–æ¨¡å‹ï¼Œå¯ä»¥ç”¨æ–¼ç‰¹å¾µé¸æ“‡
    - L2æ­£å‰‡åŒ–æ˜¯æŒ‡æ¬Šå€¼å‘é‡wä¸­å„å€‹å…ƒç´ çš„å¹³æ–¹å’Œç„¶å¾Œå†æ±‚å¹³æ–¹æ ¹L2æ­£å‰‡åŒ–å¯ä»¥é˜²æ­¢æ¨¡å‹éæ“¬åˆï¼ˆoverfittingï¼‰ã€‚ç•¶ç„¶ï¼Œä¸€å®šç¨‹åº¦ä¸Šï¼ŒL1ä¹Ÿå¯ä»¥é˜²æ­¢éæ“¬åˆ

  ```python
  from sklearn.linear_model import LassoCV
  ```

  

- åŸºæ–¼æ¨¡å‹çš„ç‰¹å¾µé¸æ“‡æ³•(Model based ranking)
  - ç›´æ¥ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•ï¼Œé‡å°æ¯å€‹å–®ç¨çš„ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸å»ºç«‹é æ¸¬æ¨¡å‹ã€‚å‡å¦‚æŸå€‹ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸ä¹‹é–“çš„é—œä¿‚æ˜¯éç·šæ€§çš„ï¼Œå¯ä»¥ç”¨åŸºæ–¼æ¨¹çš„æ–¹æ³•ï¼ˆæ±ºç­–æ¨¹ã€éš¨æ©Ÿæ£®æ—ï¼‰ã€æˆ–è€…æ“´å±•çš„ç·šæ€§æ¨¡å‹ç­‰ã€‚åŸºæ–¼æ¨¹çš„æ–¹æ³•æ¯”è¼ƒæ˜“æ–¼ä½¿ç”¨ï¼Œå› ç‚ºä»–å€‘å°éç·šæ€§é—œä¿‚çš„å»ºæ¨¡æ¯”è¼ƒå¥½ï¼Œä¸¦ä¸”ä¸éœ€è¦å¤ªå¤šçš„èª¿è©¦ã€‚ä½†è¦æ³¨æ„éæ“¬åˆå•é¡Œï¼Œå› æ­¤æ¨¹çš„æ·±åº¦æœ€å¥½ä¸è¦å¤ªå¤§ï¼Œå†å°±æ˜¯é‹ç”¨äº¤å‰é©—è­‰ã€‚é€šéé€™ç¨®è¨“ç·´å°ç‰¹å¾µé€²è¡Œæ‰“åˆ†ç²å¾—ç›¸é—œæ€§å¾Œå†è¨“ç·´æœ€çµ‚æ¨¡å‹ã€‚

  - ä½¿â½¤æ¢¯åº¦æå‡æ¨¹æ“¬åˆå¾Œï¼Œä»¥ç‰¹å¾µåœ¨ç¯€é»å‡ºç¾çš„é »ç‡ç•¶ä½œç‰¹å¾µé‡è¦æ€§ï¼Œä»¥æ­¤åˆªé™¤é‡è¦æ€§ä½æ–¼â¾¨æª»çš„ç‰¹å¾µ

  - ç‰¹å¾µé¸æ“‡ä¸­ï¼Œè¨ˆç®—æ™‚é–“è¼ƒé•·ï¼Œä½†æ˜¯èƒ½æ’é™¤å…±ç·šæ€§ä¸”æ¯”è¼ƒç©©å®šçš„â½…å¼æ˜¯æ¢¯åº¦æå‡æ¨¹åµŒå…¥æ³•

  ```python
  from sklearn.feature_selection import SelectFromModel
  ```



##### Forward Feature Selection

Forward Selection is performed by starting with 1 or a few features initially and creating a model. Another feature is repeatedly added to improve the model till the required level of accuracy is achieved. This is a rather slow approach and impractical when there are a large number of features available.

- [Python Example](https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html)

##### Backward Feature Elemination

Backward Elimination is performed by starting with all or most of the features to be used for the model and eliminating the features one at a time to improve the model. The removed features are indiscriminant and add confusion to the model. Statistical techniques such as R squared metric and statistical tests can be used to decide which features to remove.

- [Python Example](https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b)

##### Subset Selection

In this technique a subset of features is selected by manual trial. Variables are added and removed such that the Error term is reduced. An exhaustive approach would take 2^n models, where n is the number of features â€“ therefore a heuristic technique is used because a thorough approach is too expensive.There are three methodologies â€“ forward selection, backward selection and floating search. Forward selection is performed by incrementally adding a variable to the model to reduce the error. Backward selection is performed by starting with all the variables and reducing them stepwise to improve the model. Floating Search uses a back and forth approach to add and reduce variables to form different combinations.

- [R Example](http://www.science.smith.edu/~jcrouser/SDS293/labs/lab8-r.html)

- æ’åˆ—é‡è¦æ€§ (permutation Importance)
  - ç‰¹å¾µé‡è¦æ€§è¨ˆç®—æ–¹æ³•
    - åœ¨æ¨¹æ¨¡å‹ä¸­ç‰¹å¾µçš„åˆ†æ”¯æ¬¡æ•¸ï¼šweight
    - ç‰¹å¾µè¦†è“‹åº¦ï¼šcover
    - æå¤±å‡½æ•¸é™ä½é‡ï¼šgain
  - é›–ç„¶ç‰¹å¾µé‡è¦æ€§ç›¸ç•¶å¯¦â½¤ï¼Œç„¶â½½è¨ˆç®—åŸç†å¿…é ˆåŸºæ–¼æ¨¹ç‹€æ¨¡å‹ï¼Œæ–¼æ˜¯æœ‰äº†å¯å»¶ä¼¸â¾„éæ¨¹ç‹€æ¨¡å‹çš„æ’åºé‡è¦æ€§
  - æ’åºé‡è¦æ€§è¨ˆç®—ï¼Œæ˜¯æ‰“æ•£å–®â¼€ç‰¹å¾µçš„è³‡æ–™æ’åºé †åºï¼Œå†â½¤åŸæœ¬æ¨¡å‹é‡æ–°é æ¸¬ï¼Œè§€å¯Ÿæ‰“æ•£å‰å¾Œèª¤å·®æœƒè®ŠåŒ–å¤šå°‘

- Ref
  - [è°ˆè°ˆ L1 ä¸ L2-æ­£åˆ™é¡¹](https://liam.page/2017/03/30/L1-and-L2-regularizer/)

  - [Permutation Importances](https://www.kaggle.com/dansbecker/permutation-importance?utm_medium=email&utm_source=mailchimp&utm_campaign=ml4insights)
  - [å¹²è´§ï¼šç»“åˆScikit-learnä»‹ç»å‡ ç§å¸¸ç”¨çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•](https://www.zhihu.com/question/28641663)
  - [ç‰¹å¾å·¥ç¨‹åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ](https://www.zhihu.com/question/29316149)
  - [Kaggleç«¶è³½-éµé”å°¼è™Ÿç”Ÿå­˜é æ¸¬(å‰16%æ’å)]([https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC4-1%E8%AC%9B-kaggle%E7%AB%B6%E8%B3%BD-%E9%90%B5%E9%81%94%E5%B0%BC%E8%99%9F%E7%94%9F%E5%AD%98%E9%A0%90%E6%B8%AC-%E5%89%8D16-%E6%8E%92%E5%90%8D-a8842fea7077](https://medium.com/jameslearningnote/è³‡æ–™åˆ†æ-æ©Ÿå™¨å­¸ç¿’-ç¬¬4-1è¬›-kaggleç«¶è³½-éµé”å°¼è™Ÿç”Ÿå­˜é æ¸¬-å‰16-æ’å-a8842fea7077))

  - [Stopping stepwise: Why stepwise selection is bad and what you should use instead](https://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df)
  - [The 5 Feature Selection Algorithms every Data Scientist should know](https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2)

  - [The Curse of Dimensionality in classification](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)





#### Underfit

- å¦‚æœç¶“éèª¿æ•´æ¨¡å‹åƒæ•¸é‚„æ˜¯ç„¡æ³•æ“¬åˆæ¨¡å‹ï¼Œé‚„å¯ä»¥å˜—è©¦ éŒ¯èª¤åˆ†æä¾†æå‡æ¨¡å‹æ•ˆåº¦
- äººç„¡å®Œäººï¼Œæ¯å€‹æ¨¡å‹ä¸å¯èƒ½éƒ½æ˜¯å®Œç¾çš„ï¼Œå®ƒç¸½æœƒçŠ¯ä¸€äº›éŒ¯èª¤ã€‚ç‚ºç­è§£æŸå€‹æ¨¡å‹åœ¨çŠ¯ä»€éº¼éŒ¯èª¤ï¼Œæˆ‘å€‘å¯ä»¥è§€å¯Ÿè¢«æ¨¡å‹èª¤åˆ¤çš„æ¨£æœ¬ï¼Œç¸½çµå®ƒå€‘çš„å…±åŒç‰¹å¾µï¼Œæˆ‘å€‘å°±å¯ä»¥å†è¨“ç·´ä¸€å€‹æ•ˆæœæ›´å¥½çš„æ¨¡å‹ã€‚é€™ç¨®åšæ³•æœ‰é»åƒå¾Œé¢Ensembleæ™‚æåˆ°çš„Boostingï¼Œä½†æ˜¯æˆ‘å€‘æ˜¯äººç‚ºåœ°è§€å¯ŸéŒ¯èª¤æ¨£æœ¬ï¼Œè€ŒBoostingæ˜¯äº¤çµ¦äº†æ©Ÿå™¨ã€‚é€šééŒ¯èª¤åˆ†æ->ç™¼ç¾æ–°ç‰¹å¾µ->è¨“ç·´æ–°æ¨¡å‹->éŒ¯èª¤åˆ†æï¼Œå¯ä»¥ä¸æ–·åœ°åè¦†é‹ç®—å‡ºæ›´å¥½çš„æ•ˆæœï¼Œä¸¦ä¸”é€™ç¨®æ–¹å¼é‚„å¯ä»¥åŸ¹é¤Šæˆ‘å€‘å°è³‡æ–™çš„å—…è¦ºã€‚
- èˆ‰å€‹ä¾‹å­ï¼Œé€™æ¬¡æ¯”è³½ä¸­ï¼Œæˆ‘å€‘åœ¨éŒ¯èª¤åˆ†ææ™‚ç™¼ç¾ï¼ŒæŸäº›æ¨£æœ¬çš„å…©å€‹å•å¥è¡¨é¢ä¸Šå¾ˆç›¸ä¼¼ï¼Œä½†æ˜¯å¥å­æœ€å¾Œæåˆ°çš„åœ°é»ä¸ä¸€æ¨£ï¼Œæ‰€ä»¥å…¶å¯¦å®ƒå€‘æ˜¯èªç¾©ä¸ç›¸ä¼¼çš„ï¼Œä½†æˆ‘å€‘çš„æ¨¡å‹å»æŠŠå®ƒèª¤åˆ¤ç‚ºç›¸ä¼¼çš„ã€‚æ¯”å¦‚é€™å€‹æ¨£æœ¬ï¼š
  - Question1: Which is the best digital marketing institution in banglore?
  - Question2: Which is the best digital marketing institute in Pune?
- ç‚ºäº†è®“æ¨¡å‹å¯ä»¥è™•ç†é€™ç¨®æ¨£æœ¬ï¼Œæˆ‘å€‘å°‡å…©å€‹å•å¥çš„æœ€é•·å…¬å…±å­ä¸²(Longest Common Sequence)å»æ‰ï¼Œç”¨å‰©é¤˜éƒ¨åˆ†è¨“ç·´ä¸€å€‹æ–°çš„æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œç›¸ç•¶æ–¼å‘Šè¨´æ¨¡å‹çœ‹åˆ°é€™ç¨®æƒ…æ³çš„æ™‚å€™å°±ä¸è¦åˆ¤æ–·ç‚ºç›¸ä¼¼çš„äº†ã€‚å› æ­¤ï¼Œåœ¨åŠ å…¥é€™å€‹ç‰¹å¾µå¾Œï¼Œæˆ‘å€‘çš„æ•ˆæœå¾—åˆ°äº†ä¸€äº›æå‡ã€‚
- æ¨¡å‹çµ„åˆ
  - æŠŠä¸åŒæ¨¡å‹çš„é æ¸¬çµæœç•¶æˆè§£é‡‹è®Šæ•¸ï¼Œè—‰æ­¤é æ¸¬ Y çš„çµæœï¼Œ



##### Feature Construction

> - ã€Œæ•¸æ“šå’Œç‰¹å¾µæ±ºå®šäº†æ©Ÿå™¨å­¸ç¿’çš„ä¸Šé™ï¼Œè€Œæ¨¡å‹å’Œç®—æ³•åªæ˜¯é€¼è¿‘é€™å€‹ä¸Šé™è€Œå·²ã€
> - ç‰¹å¾µå·¥ç¨‹æ˜¯é‡å°æ•¸æ“šé€²è¡ŒåŠ å·¥è™•ç†ï¼Œè®“æ¨¡å‹èƒ½æœ€å¤§é™åº¦çš„å¾åŸå§‹æ•¸æ“šä¸­æ‰¾å‡ºè®Šæ•¸ä¹‹é–“çš„é—œè¯æ€§ï¼Œé€²è€Œæå‡æ¨¡å‹çš„æ•ˆåº¦ã€‚

- ä½¿ç”¨çµ±è¨ˆæˆ–é ˜åŸŸçŸ¥è­˜ï¼Œä»¥å„ç¨®çµ„åˆèª¿æ•´æ–¹å¼ï¼Œç”Ÿæˆæ–°ç‰¹å¾µä»¥æå‡æ¨¡å‹é æ¸¬åŠ›

- æˆ‘å€‘æ‡‰è©²ç›¡å¯èƒ½å¤šåœ°æŠ½å–ç‰¹å¾µï¼Œåªè¦ä½ èªç‚ºæŸå€‹ç‰¹å¾µå°è§£æ±ºå•é¡Œæœ‰èª¬æ˜ï¼Œå®ƒå°±å¯ä»¥æˆç‚ºä¸€å€‹ç‰¹å¾µã€‚ç‰¹å¾µæŠ½å–éœ€è¦ä¸æ–·åè¦†é‹ç®—ï¼Œæ˜¯æœ€ç‚ºç‡’è…¦çš„ç’°ç¯€ï¼Œå®ƒæœƒåœ¨æ•´å€‹æ¯”è³½é€±æœŸæŠ˜ç£¨ä½ ï¼Œä½†é€™æ˜¯æ¯”è³½å–å‹çš„é—œéµï¼Œå®ƒå€¼å¾—ä½ è€—è²»å¤§é‡çš„æ™‚é–“ã€‚

- é‚£å•é¡Œä¾†äº†ï¼Œæ€éº¼å»ç™¼ç¾ç‰¹å¾µå‘¢ï¼Ÿå…‰ç›¯è‘—è³‡æ–™é›†è‚¯å®šæ˜¯ä¸è¡Œçš„ã€‚å¦‚æœä½ æ˜¯æ–°æ‰‹ï¼Œå¯ä»¥å…ˆè€—è²»ä¸€äº›æ™‚é–“åœ¨Forumä¸Šï¼Œçœ‹çœ‹åˆ¥äººæ˜¯æ€éº¼åšFeature Extractionçš„ï¼Œä¸¦ä¸”å¤šæ€è€ƒã€‚é›–ç„¶Feature Extractionç‰¹åˆ¥è¬›ç©¶ç¶“é©—ï¼Œä½†å…¶å¯¦é‚„æ˜¯æœ‰ç« å¯å¾ªçš„ï¼š
  1. å°æ–¼Numerical Variableï¼Œå¯ä»¥é€šé**ç·šæ€§çµ„åˆã€å¤šé …å¼çµ„åˆ**ä¾†ç™¼ç¾æ–°çš„Featureã€‚
  2. å°æ–¼æ–‡æœ¬è³‡æ–™ï¼Œæœ‰ä¸€äº›å¸¸è¦çš„Featureã€‚æ¯”å¦‚ï¼Œæ–‡æœ¬é•·åº¦ï¼ŒEmbeddingsï¼ŒTF-IDFï¼ŒLDAï¼ŒLSIç­‰ï¼Œä½ ç”šè‡³å¯ä»¥ç”¨æ·±åº¦å­¸ç¿’æå–æ–‡æœ¬ç‰¹å¾µï¼ˆéš±è—å±¤ï¼‰ã€‚
  3. å¦‚æœä½ æƒ³å°è³‡æ–™æœ‰æ›´æ·±å…¥çš„ç­è§£ï¼Œå¯ä»¥é€šéæ€è€ƒè³‡æ–™é›†çš„æ§‹é€ éç¨‹ä¾†ç™¼ç¾ä¸€äº›magic featureï¼Œé€™äº›ç‰¹å¾µæœ‰å¯èƒ½æœƒå¤§å¤§æå‡æ•ˆæœã€‚
  4. é€šé**éŒ¯èª¤åˆ†æ**ä¹Ÿå¯ä»¥ç™¼ç¾æ–°çš„ç‰¹å¾µã€‚



##### Features Interaction

- å‡è¨­ä½ æœ‰ `A` å’Œ `B` å…©å€‹ continuous ç‰¹å¾µï¼Œä½ å¯ä»¥ç”¨ `A + B`ã€`A - B`ã€`A * B` æˆ– `A / B` ä¹‹é¡çš„æ–¹å¼å»ºç«‹æ–°çš„ç‰¹å¾µã€‚
- æœ‰äº›ç‰¹å¾µéœ€è¦ä¸€èµ·è€ƒæ…®æ‰æœ‰æ„ç¾©ï¼Œå¦‚åœ¨åˆ†æè¨ˆç¨‹è»Šçš„é‹è¼¸è³‡æ–™æ™‚ï¼Œæœƒæœ‰èµ·é»çš„ç¶“ç·¯åº¦èˆ‡çµ‚é»çš„ç¶“ç·¯åº¦ç­‰4å€‹è®Šé …ã€‚
- å–®ç¨å„è‡ªä½¿ç”¨ã€Œèµ·é»ç¶“åº¦ã€ã€ã€Œèµ·é»ç·¯åº¦ã€ã€ã€Œçµ‚é»ç¶“åº¦ã€æˆ–ã€Œçµ‚é»ç·¯åº¦ã€éƒ½æ˜¯æ²’æœ‰æ„ç¾©çš„ã€‚å¿…é ˆè¦å°‡é€™å››å€‹è®Šæ•¸é€²è¡Œçµ„åˆï¼Œä¸¦è¨ˆç®—å¯¦éš›è·é›¢ã€‚æˆ–æ›´ç´°ç·»çš„è™•ç†æ¯å€‹ç·¯åº¦é•·åº¦ä¸ä¸€è‡´çš„å•é¡Œå¾Œè¨ˆç®—å¯¦éš›è·é›¢ï¼Œèƒ½å¤ å†é€²ä¸€æ­¥æé«˜é æ¸¬çš„ç²¾æº–åº¦ã€‚

##### Feature Combination 

- ç‰¹å¾µçµ„åˆä¸»è¦æ˜¯é‡å° categorical ç‰¹å¾µï¼Œç‰¹å¾µäº¤äº’å‰‡æ˜¯é©ç”¨æ–¼ continuous ç‰¹å¾µã€‚ä½†æ˜¯å…©è€…çš„æ¦‚å¿µæ˜¯å·®ä¸å¤šçš„ï¼Œå°±æ˜¯æŠŠå…©å€‹ä»¥ä¸Šçš„ç‰¹å¾µé€éæŸç¨®æ–¹å¼çµåˆåœ¨ä¸€èµ·ï¼Œè®Šæˆæ–°çš„ç‰¹å¾µã€‚é€šå¸¸ç”¨ä¾†è§£æ±ºä¸€èˆ¬çš„ç·šæ€§æ¨¡å‹æ²’è¾¦æ³•å­¸åˆ°éç·šæ€§ç‰¹å¾µçš„å•é¡Œã€‚

- ç¾¤èšç·¨ç¢¼(Group by Encoding)

  å‡å€¼ç·¨ç¢¼æ˜¯è¨ˆç®—å„å€‹é¡åˆ¥åœ¨ç›®æ¨™è®Šæ•¸çš„å¹³å‡å€¼ï¼Œè€Œç¾¤èšç·¨ç¢¼å‰‡æ˜¯é‡å°å…¶ä»–æ•¸å€¼è®Šæ•¸è¨ˆç®—é¡åˆ¥å¹³å‡å€¼ (Mean)ã€ä¸­ä½æ•¸ (Median)ï¼Œçœ¾æ•¸(Mode)ï¼Œæœ€â¼¤å€¼(Max)ï¼Œæœ€â¼©å€¼(Min)ï¼Œæ¬¡æ•¸(Count)...ç­‰ã€‚

- ç¾¤èšç·¨ç¢¼çš„ä½¿ç”¨æ™‚æ©Ÿæ˜¯ï¼Œå…ˆä»¥ é ˜åŸŸçŸ¥è­˜ æˆ– ç‰¹å¾µé‡è¦æ€§ æŒ‘é¸å¼·â¼’ç‰¹å¾µå¾Œ, å†å°‡ç‰¹å¾µçµ„æˆæ›´å¼·çš„ç‰¹å¾µ

- å¯ä»¥ä¾ç…§é ˜åŸŸçŸ¥è­˜æŒ‘é¸,æˆ–äº‚æ§æ‰“â¿ƒå¾Œå†ä»¥ç‰¹å¾µé‡è¦æ€§æŒ‘é¸

- ä»¥å‰æ˜¯ä»¥éæ¨¹ç‹€æ¨¡å‹ç‚ºä¸», ç‚ºäº†é¿å…å…±ç·šæ€§, æœƒå¾ˆæ³¨æ„é¡ä¼¼çš„ç‰¹å¾µä¸è¦å¢åŠ å¤ªå¤šï¼Œä½†ç¾åœ¨å¼·â¼’çš„æ¨¡å‹éƒ½æ˜¯æ¨¹ç‹€æ¨¡å‹, æ‰€ä»¥åªè¦æœ‰å¯èƒ½å°±é€šé€šå¯ä»¥åšæˆç‰¹å¾µå˜—è©¦!

- å‡è¨­ C æ˜¯ categorical ç‰¹å¾µï¼ŒN æ˜¯ continuous ç‰¹å¾µï¼Œä»¥ä¸‹æœ‰å¹¾ç¨®æœ‰æ„ç¾©çš„çµ„åˆï¼š

  - `median(N) GROUP BY C` ä¸­ä½æ•¸
  - `mean(N) GROUP BY C` ç®—è¡“å¹³å‡æ•¸
  - `mode(N) GROUP BY C` çœ¾æ•¸
  - `min(N) GROUP BY C` æœ€å°å€¼
  - `max(N) GROUP BY C` æœ€å¤§å€¼
  - `std(N) GROUP BY C` æ¨™æº–å·®
  - `var(N) GROUP BY C` æ–¹å·®
  - `N - median(N) GROUP BY C`



##### Feature Extraction

é€šå¸¸å°±æ˜¯æŒ‡ dimensionality reductionã€‚

- Principal Component Analysis (PCA)
- Latent Dirichlet Allocation (LDA)
- Latent Semantic Analysis (LSA)

##### Feature Learning

- è‘‰ç·¨ç¢¼ (leaf encoding) é¡§åæ€ç¾©ï¼Œæ˜¯æ¡â½¤æ±ºç­–æ¨¹çš„è‘‰é»ä½œç‚ºç·¨ç¢¼ä¾æ“šé‡æ–°ç·¨ç¢¼

- æ¦‚å¿µæ˜¯å°‡æ¯æ£µæ¨¹éƒ½è¦–ç‚ºâ¼€å€‹æ–°ç‰¹å¾µï¼Œæ¨¹ä¸‹çš„ n å€‹ç¯€é»å‰‡ä½œç‚ºæ–°ç‰¹å¾µçš„ n å€‹é¡åˆ¥å€¼ï¼Œç”±æ–¼æ¯å€‹è‘‰ç¯€é»çš„æ€§è³ªæ¥è¿‘ï¼Œå› æ­¤å¯è¦–ç‚ºè³‡æ–™çš„â¼€ç¨®åˆ†çµ„â½…å¼ã€‚

- é›–ç„¶ä¸é©åˆç›´æ¥æ²¿â½¤æ¨¹ç‹€æ¨¡å‹æ©Ÿç‡ï¼Œä½†åˆ†çµ„â½…å¼æœ‰ä»£è¡¨æ€§ï¼Œå› æ­¤æŒ‰ç…§è‘‰é»å°‡è³‡æ–™é›¢æ•£åŒ– ï¼Œæœƒæ¯”ä¹‹å‰æéçš„é›¢æ•£åŒ–â½…å¼è·Ÿæœ‰åŠ©æ–¼æå‡ç²¾ç¢ºåº¦

- è‘‰ç·¨ç¢¼çš„çµæœï¼Œæ˜¯â¼€çµ„æ¨¡å‹ç”¢â½£çš„æ–°ç‰¹å¾µï¼Œæˆ‘å€‘å¯ä»¥ä½¿â½¤é‚è¼¯æ–¯å›æ­¸ï¼Œé‡æ–°è³¦äºˆæ©Ÿç‡ (å¦‚ä¸‹è‘‰åœ–)ï¼Œä¹Ÿå¯ä»¥èˆ‡å…¶ä»–ç®—æ³•çµåˆ (ä¾‹å¦‚ : åˆ†è§£æ©Ÿ Factorization Machine )ä½¿è³‡æ–™ç²å¾—æ–°â½£ï¼Œæœ€å¾Œå†ä»¥é‚è¼¯æ–¯è¿´æ­¸åˆä½µé æ¸¬

  ![](https://lh3.googleusercontent.com/Fu1ppabaRpOcfZ1EsWvGRBxVtLz113i_INBrujwkufjo9-xUvXbVrTruCUgSx04xMaJxOxlNb5jaXOganmyGA32mlcSAUIlNb4Po5qD-GRCWl9-khVuWx-5xAkF_jtmbUkc53PNsRZZCBr2PvzxCYlICqEzY_iaVVSjifprLrsosFhZjmkPhYlkO8u_wT1P80E4T65-XsKx9x-Wvk4M7ht9lD6NyV7iTGRjYtD1fsGBd8ILmIbVmMTswrjL6xiTt-EEGr6ZrW3hVqELLzoVFZ9jHk7uRA6BofNiEkZ2MCRiqpcDu8zlY_55pEmVQmB2GhRVl_fA7SH4TdL9U2UqHZSpbkPxXMAj4VIf75FdXadqudS6sJLTHPixaeQGOIkYBko_tuz-lWRj4uUNJNjYTrUTgbPrcPQRn_RLVN6UXWrrnnNMycPaifC2-9WRrR1Yip0pxlGW6GdhhekdMvEmQyrZYjG0mzWyaJjNGjSze6YFZeRRefmWakyK_mOqIBxUIub9zV_-VlNn43-MAte2RvuTGHWQ06Y8_TtixmQuHAnssN9DuQVU6B_x7nnMM5wec_6Bk2W6IBAnqHmZ_c2yt6cE7VBj5EeIGYHqMHg4AwTM3MJNXgl0cHE-mR5lHYWyzdrLsHC8knlwNiBUHvGowl5M6ZgFeXDDoNXMTdiFXKgTX3kAJmbzgAZwEUtyxFyprkn2VjxFkdL0j9N57OWDkO3SK=w459-h365-no)

- è‘‰ç·¨ç¢¼éœ€è¦å…ˆå°æ¨¹ç‹€æ¨¡å‹æ“¬åˆå¾Œæ‰èƒ½â½£æˆï¼Œå¦‚æœé€™æ­¥é©ŸæŒ‘é¸äº†è¼ƒä½³çš„åƒæ•¸ï¼Œå¾ŒçºŒè™•ç†æ•ˆæœä¹Ÿæœƒè¼ƒå¥½ï¼Œé€™é»èˆ‡ç‰¹å¾µé‡è¦æ€§é¡ä¼¼

  - å¯¦éš›çµæœä¹Ÿè­‰æ˜ï¼Œåœ¨åˆ†é¡é æ¸¬ä¸­ä½¿â½¤æ¨¹ç‹€æ¨¡å‹ï¼Œå†å°é€™äº›æ“¬åˆå®Œçš„æ¨¹ç‹€æ¨¡å‹é€²â¾
    è‘‰ç·¨ç¢¼+é‚è¼¯æ–¯è¿´æ­¸ï¼Œé€šå¸¸æœƒå°‡é æ¸¬æ•ˆæœå†é€²â¼€æ­¥æå‡

  ![](https://lh3.googleusercontent.com/UJ3qH8VF0i7DwMoG-5z24kopMAUon2gJzhNZ7uSKRGjEBBiJ_ATsXVrPl91IY8_uOlDq3QYrwtyu6klfXDz-3f5FhhS4kaxZl_gHGnMsfPD6kReUWYmJfOCs6Z5YkIXaxypD1YB8nxrN3DtnqW4TQ9TePasZ-59MNuZ7TeRc1N1wHl-WoE5eNr3IiyAUceVppDykQBw4rj7iSWlD1DD88R3QffNXuvnZV8DHI410XJJQ33YNuzuqY4XBpigkgM1XKeG2_Cg1nb0WohxoU9-sAnT8IA-fqKrIoUDYPq0Xbz4lZC2Kp12Tt0QxbLndap32oPIsaxQHoOMpkd91SAdHGaAypSPEzfyplfTJyPjdB4ccJdWcyaUYpw20UlfaYcM1BOMhYkNAFzZoy03VSVjDMMmwrAhTK0URhul8KvbxKXdG_df31w8hi40Syk-8Uk0YlMux2C5kOrp3vg4laCNAMOgJTf49d-T4GuOu__JQkK6DiMa5uph4NKrEbbBgnrh7bRSGQe0_oSRfTQr6t642bQzZH4TotOFmWW-BOJpKb0QhOwavihWO5P-VSeQ5b9D7nJaMau7ulBd8DVhARxzcTblALuR6aIpmIZ0EuWUCxu5GLtUlNxjSv0ICEWS5p9kISoUUhP3o779fwyKdBvLET9jwWunrc38ud8YYROabd1cefarrwQFfxGkE0p42k7a8WGbC7IjJP0zMCf2d5Qk0jZLq=w660-h242-no)

  - è‘‰ç·¨ç¢¼ç·¨å®Œå¾Œï¼Œå› ç‚ºç‰¹å¾µæ•¸é‡è¼ƒå¤šï¼Œé€šå¸¸æ­é…é‚è¼¯æ–¯å›æ­¸æˆ–è€…åˆ†è§£æ©Ÿåšé æ¸¬ï¼Œå…¶ä»–æ¨¡å‹è¼ƒä¸é©åˆ

- Ref
  - [Practical Lessons from Predicting Clicks on Ads at Facebook](http://quinonero.net/Publications/predicting-clicks-facebook.pdf)
  - [Feature transformations with ensembles of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html#example-ensemble-plot-feature-transformation-py)
  - [CTRé¢„ä¼°: Algorithm-GBDT Encoder](https://zhuanlan.zhihu.com/p/31734283)
  - [ä¸‰åˆ†é˜äº†è§£æ¨è–¦ç³»çµ±ä¸­çš„åˆ†è§£æ©Ÿæ–¹æ³•](https://kknews.cc/code/62k4rml.html)



### Imbalance Data

- Machine Learnings have been well developed and successfully applied to many application domains. However, the imbalanced class distribution of a data set has a problem because the majority of supervised learning techniques developed are for balanced class distribution.
- The imbalanced class distribution usually happens when we are studying a rare phenomenon such as medical diagnosis, risk management, hoax detection, and many more.
- Symptom: High Accuracy but low recall!

#### Resample method

- Under sampling
- Over sampling
- smote
- 

#### Optimal Threshold

- ROC curve

- G-Mean
  $$
  G-Mean = \sqrt {Recall * Specifivity}\\
  = \sqrt{TPR*\frac{TN}{FP+TN}} \\
  = \sqrt{TPR*(1-FPR)}
  $$
  
- Youdenâ€™s J statistic
  $$
  youdenj = tpr-fpr
  $$

- precision-Recall curve

- F-Score
  $$
  fscore = \frac{(2*precision*recall)}{ (precision + recall)}
  $$
  
- Threshold tuning



Ref

- [Imbalanced data & why you should NOT use ROC curve](https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve)

- [Optimal Threshold for Imbalanced Classification | by Audhi Aprilliant | Towards Data Science](https://towardsdatascience.com/optimal-threshold-for-imbalanced-classification-5884e870c293)

### DataLeak

- æœ¬ä¾†ä¸æ‡‰è©²å‡ºç¾åœ¨Xè£¡çš„ã€å’Œç›®æ¨™yæœ‰é—œçš„è³‡æ–™ï¼Œå‡ºç¾åœ¨äº†Xä¸­ã€‚å¦‚æ­¤ä¸€ä¾†ï¼Œæ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•å°±æœƒæœ‰å¥½åˆ°ä¸çœŸå¯¦çš„è¡¨ç¾ã€‚



- è³‡æ–™æ´©éœ²çš„ç¨®é¡ä»¥åŠå½±éŸ¿åˆ†æ
  - æ¸¬è©¦é›†è³‡æ–™è¢«æ´©éœ²åˆ°è¨“ç·´é›†ï¼šéæ“¬åˆï¼Œæ¨¡å‹åœ¨ç¾å¯¦ä¸­çš„è¡¨ç¾é ä¸å¦‚test accuracyï¼›æ¸¬è©¦é›†å¤±å»æ„ç¾©ã€‚

  - æ­£ç¢ºçš„é æ¸¬ï¼ˆyï¼‰è¢«æ´©éœ²åˆ°æ¸¬è©¦é›†ï¼šåš´é‡éæ“¬åˆï¼Œè¨“ç·´å‡ºçš„æ¨¡å‹æ¯«ç„¡ç”¨è™•ï¼Œæ¯”è³½çµ„ç¹”è€…çš„æ¥µå¤§å¤±æ•—

  - æœªä¾†çš„è³‡è¨Šè¢«æ´©éœ²åˆ°éå»ï¼šæ™‚é–“åºåˆ—ç›¸é—œï¼Œç¾å¯¦ä¸­æ¨¡å‹å°‡ç„¡æ³•æœ‰æ•ˆæ ¹æ“šéå»æƒ…æ³é æ¸¬æœªä¾†ã€‚

  - æ¨¡å‹å¯ä»¥ç²å¾—ä¸€äº›ä¸è©²ç²å¾—çš„è³‡è¨Šï¼Œæ¯”å¦‚å’Œç›®æ¨™è®Šæ•¸æœ‰è¼ƒå¤§é—œä¿‚çš„è®Šæ•¸ã€ç¾å¯¦è£¡æ¥è§¸ä¸åˆ°çš„è®Šæ•¸ã€‚ä¾‹å­ï¼šyæ˜¯â€œç—…äººæ˜¯å¦æ‚£æœ‰ç™Œç—‡â€ï¼Œä½†æ˜¯XåŒ…æ‹¬äº†â€œç—…äººæ˜¯å¦æ¥å—è…«ç˜¤åˆ‡é™¤æ‰‹è¡“â€ã€‚

  - åå‘å·¥ç¨‹ï¼Œå»åŒ¿ååŒ–ï¼Œå»é™¤è³‡æ–™é›†ä¸­çš„éš¨æ©Ÿæ‰“äº‚æ“ä½œï¼Œç¤¾æœƒå·¥ç¨‹å­¸ã€‚é€™ç¨®è¡Œç‚ºæ˜¯è³‡æ–™æ¯”è³½æ˜ä»¤ç¦æ­¢çš„ï¼Œè€Œä¸”åœ¨ç¾å¯¦ä¸­ä¹Ÿæ¶‰å«Œä¾µçŠ¯éš±ç§ã€‚ä¾‹å­ï¼šåå‘å·¥ç¨‹â€œéš¨æ©Ÿçš„â€ä½¿ç”¨è€…ç·¨ç¢¼ï¼Œå¾—å‡ºä½¿ç”¨è€…çš„çœŸåã€‚

  - ç¬¬ä¸‰æ–¹ä¿¡æ¯ã€‚ä¾‹å­ï¼šå·²çŸ¥åº§æ¨™ï¼Œåˆ©ç”¨geocoderé¡å‹çš„æœå‹™æ¨å‡ºæ‰€åœ¨åŸå¸‚ï¼›åœ¨é æ¸¬é‡‘èå¸‚å ´æ™‚åŠ å…¥å”åŠ›å» å•†çš„æ”¿ç­–æ–°èçš„ç‰¹å¾µã€‚

 

- æœ‰æ•ˆç™¼ç¾å’Œåˆ©ç”¨è³‡æ–™æ´©éœ²

  è³‡æ–™æ´©éœ²å¯ä»¥åˆ†ç‚ºå…©å¤§é¡ï¼š

  - ç”±æ–¼è‡ªå·±çš„ç–å¿½ï¼Œåœ¨äº¤å‰é©—è­‰ã€è¨“ç·´éç¨‹ä¸­ï¼Œç”¢ç”Ÿçš„è³‡æ–™æ´©éœ²ã€‚é€™ç¨®æƒ…æ³å±¬æ–¼å¤±èª¤ï¼Œæ‡‰ç•¶å„˜é‡é¿å…ã€‚

  - åœ¨è³‡æ–™ç«¶è³½ä¸­ï¼Œæ‰¾åˆ°äº†ç†è«–ä¸Šä¸èƒ½ä½¿ç”¨ï¼ˆä½†æ˜¯ä¹Ÿæ²’æœ‰æ˜ä»¤ç¦æ­¢ï¼‰çš„é¡å¤–è³‡æ–™ï¼Œå¾è€Œæå‡åˆ†æ•¸ã€‚

  - é¿å…ç¬¬ä¸€ç¨®è³‡æ–™æ´©éœ²çš„æ–¹æ³•ï¼Œå¯ä»¥åƒè€ƒkaggleçš„å„é¡æ¯”è³½ã€‚å‡è¨­æœ‰å¤§é‡è³‡æ–™ï¼Œæˆ‘å€‘å¯ä»¥æŠŠæœªè™•ç†çš„è³‡æ–™åˆ†ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†ï¼Œå…¶ä¸­ï¼Œæ¸¬è©¦é›†åŒ…æ‹¬Public LBå’ŒPrivate LBå…©éƒ¨åˆ†ã€‚
    - åœ¨æ¨¡å‹çš„è¨“ç·´ã€é¸æ“‡å’Œäº¤å‰é©—è­‰æ™‚ï¼Œæˆ‘å€‘åªèƒ½æ¥è§¸è¨“ç·´é›†ã€‚
    - åœ¨å°è‡ªå·±çš„æ¨¡å‹éå¸¸è‡ªä¿¡æ™‚ï¼Œå¯ä»¥å¶çˆ¾åœ¨Public LBä¸Šé©—è­‰ã€‚
    - åªæœ‰æ¨¡å‹å³å°‡è¢«ç”¨æ–¼æ­£å¼å•†æ¥­ç”¨é€”æ™‚ï¼Œæ‰èƒ½çœ‹æ¨¡å‹åœ¨Private LBä¸Šçš„è¡¨ç¾ã€‚

  - äº¤å‰é©—è­‰èª¤å·®ã€public LBèª¤å·®ã€private LBèª¤å·®ï¼šå¦‚æœå¾Œè€…çš„èª¤å·®å€¼é¡¯è‘—é«˜æ–¼å‰è€…ï¼Œé‚£éº¼éœ€è¦è€ƒæ…®éæ“¬åˆæˆ–ç¬¬ä¸€é¡è³‡æ–™æ´©éœ²ã€‚

  - ç¬¬äºŒé¡çš„è³‡æ–™æ´©éœ²ï¼Œå±¬æ–¼æ—é–€å·¦é“ã€‚æœ¬è³ªä¸Šï¼Œé€™ç›¸ç•¶æ–¼åœ¨æ¨¡å‹è¨“ç·´éšæ®µï¼Œå¹¹äº†è³‡æ–™æ”¶é›†éšæ®µçš„å·¥ä½œã€‚æœé›†åŸå§‹è³‡æ–™ï¼Œæˆ–æ˜¯è‡ªå·±æä¾›è³‡æ–™èˆ‰è¾¦ç«¶è³½ï¼ˆè©¦åœ–é¿å…ä»–äººåˆ©ç”¨è³‡æ–™æ´©éœ²ï¼‰æ™‚ï¼Œå¯ä»¥åƒè€ƒé€™ç¨®æ€è·¯ã€‚
    - è³‡æ–™å¤¾çš„å‰µé€ æ™‚é–“ã€‚
    - çœ‹ä¼¼äº‚ç¢¼çš„å­—ä¸²ï¼ˆå¦‚å„é¡idï¼‰å¯èƒ½æœ‰çµ±è¨ˆåˆ†ä½ˆçš„è¦å¾‹ã€‚
    - åœ°ç†ä½ç½®è³‡è¨Šï¼šå¦‚æœæä¾›äº†åº§æ¨™ï¼Œå‰‡å¯åå‘åœ°ç†ç·¨ç¢¼ï¼Œå¾—å‡ºç›¸é—œåœ°ç†è³‡è¨Šã€‚

  é€™é¡è³‡æ–™å¯èƒ½æœƒå°è‡´éæ“¬åˆã€‚

  - [ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦é‡è§†æœºå™¨å­¦ä¹ æ¨¡å‹çš„è°ƒè¯•ï¼Ÿ](https://zhuanlan.zhihu.com/p/110754325)








## æ¨è–¦æ›¸å–®èˆ‡å…¬é–‹èª²

**ç½‘ç»œå…¬å¼€è¯¾ï¼š**

- [éº»çœç†å·¥å…¬å¼€è¯¾ çº¿æ€§ä»£æ•°](http://link.zhihu.com/?target=http%3A//open.163.com/special/opencourse/daishu.html)â€”â€”å­¦ä¹ çŸ©é˜µç†è®ºåŠçº¿æ€§ä»£æ•°çš„åŸºæœ¬çŸ¥è¯†ï¼Œæ¨èç¬”è®°[MITçº¿æ€§ä»£æ•°è¯¾ç¨‹ç²¾ç»†ç¬”è®°byå¿†ç‘§](https://zhuanlan.zhihu.com/p/28277072)ã€‚
- [å°å¤§æœºå™¨å­¦ä¹ å…¬å¼€è¯¾](http://link.zhihu.com/?target=https%3A//www.csie.ntu.edu.tw/%7Ehtlin/mooc/)â€”â€”æˆè¯¾äººæ—è½©ç”°ï¼Œè¯¾ç¨‹åˆ†ä¸ºæœºå™¨å­¦ä¹ åŸºçŸ³å’Œæœºå™¨å­¦ä¹ æŠ€æ³•ä¸¤éƒ¨åˆ†ã€‚
- [åç››é¡¿å¤§å­¦æœºå™¨å­¦ä¹ å…¬å¼€è¯¾](http://link.zhihu.com/?target=https%3A//www.coursera.org/specializations/machine-learning)â€”â€”åç››é¡¿å¤§å­¦åœ¨Courseraå¼€çš„æœºå™¨å­¦ä¹ ä¸“é¡¹è¯¾ï¼Œå…±æœ‰å››ä¸ªéƒ¨åˆ†ï¼Œè¿™ä¸ªè¯¾ç›´æ¥ä»åº”ç”¨æ¡ˆä¾‹å¼€å§‹è®²èµ·ï¼Œå¯¹äºå›å½’ï¼Œåˆ†ç±»ï¼ŒååŒè¿‡æ»¤å’Œæƒ…æ„Ÿåˆ†æç­‰éƒ½ä¼šå…·ä½“å»è®²æ€ä¹ˆå®ç°åº”ç”¨ï¼Œå¹¶ä¸”ä¼šå‘Šè¯‰ä½ å¦‚ä½•åœ¨Pythonä¸­åˆ©ç”¨ç½‘ä¸Šä¸€äº›ç°æœ‰çš„åº“æ¥å®ç°ç‰¹å®šçš„åŠŸèƒ½ï¼Œä¹Ÿå°±æ˜¯è¯´åŸºæœ¬ä¸Šåœ¨è¯¾ç¨‹çš„ç¬¬ä¸€éƒ¨åˆ†ä½ å°±å¯ä»¥å…¨é¢çš„çŸ¥é“æœºå™¨å­¦ä¹ èƒ½å¤Ÿåœ¨ç°å®ç”Ÿæ´»ä¸­çš„åº”ç”¨ï¼Œä»¥åŠç®€å•æ–¹å¼å»å®ç°ä¸€äº›åŠŸèƒ½ã€‚
- [æ–¯å¦ç¦å¤§å­¦å…¬å¼€è¯¾ æœºå™¨å­¦ä¹ ](http://link.zhihu.com/?target=http%3A//open.163.com/special/opencourse/machinelearning.html)â€”â€”Andrew Ngï¼ˆå´æ©è¾¾ï¼‰åœ¨æ–¯å¦ç¦å¼€è®¾çš„CS229ï¼Œéš¾åº¦è¿œé«˜äºCourseraä¸Šé¢çš„è¯¾ç¨‹ã€‚
- [Googleç·šä¸Šèª²ç¨‹](https://developers.google.cn/machine-learning/crash-course/)

**ä¹¦å•ï¼š**

- [ã€Šæœºå™¨å­¦ä¹ ã€‹](http://link.zhihu.com/?target=https%3A//book.douban.com/subject/26708119/)by å‘¨å¿—åï¼Œè¿™æ˜¯ä¸€æœ¬ä¸­å›½æ— æ•°Machine Learningçƒ­çˆ±è€…çš„å¯è’™æ•™æï¼Œå®ƒéå¸¸åˆé€‚æ²¡æœ‰ä»»ä½•èƒŒæ™¯çš„åˆå­¦è€…çœ‹ï¼Œæ¯ä¸€ä¸ªæ¦‚å¿µçš„æ¥é¾™å»è„‰è®²çš„éƒ½å¾ˆç»†è‡´ï¼Œæ˜¯ä¸€æœ¬å¤§è€Œå…¨çš„æ•™æã€‚
- [ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹](http://link.zhihu.com/?target=https%3A//book.douban.com/subject/10590856/)by æèˆªï¼Œè¿™æœ¬ä¹¦ä¸»è¦åä¼˜åŒ–å’Œæ¨å€’ï¼Œæ¨å€’ç›¸åº”ç®—æ³•çš„æ—¶å€™å¯ä»¥å‚è€ƒè¿™æœ¬ä¹¦ã€‚è™½ç„¶åªæ˜¯è–„è–„çš„ä¸€æœ¬ï¼Œä½†å…¨æ˜¯ç²¾åå†…å®¹ã€‚
- [ã€Šæœºå™¨å­¦ä¹ å®æˆ˜](http://link.zhihu.com/?target=https%3A//book.douban.com/subject/24703171/)ã€‹by Peter Harringtonï¼Œå¯ä»¥å¯¹åº”ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹è¿›è¡Œå®ç°ä»£ç ã€‚
- [ã€ŠPattern Recognition And Machine Learningã€‹](http://link.zhihu.com/?target=http%3A//www.rmki.kfki.hu/%7Ebanmi/elte/Bishop%2520-%2520Pattern%2520Recognition%2520and%2520Machine%2520Learning.pdf) by Christopher Bishopï¼Œå±äºæœºå™¨å­¦ä¹ è¿›é˜¶ä¹¦ç±ï¼Œå†…å®¹å…¨ï¼Œå»ºè®®é¦–å…ˆå®Œæˆä»¥ä¸Šä¸‰æœ¬ä¹¦ç±ï¼Œå†çœ‹è¿™æœ¬ã€‚
- [ã€Šåˆ©ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æã€‹](http://link.zhihu.com/?target=https%3A//book.douban.com/subject/25779298/)â€”â€”Pythonå¸¸ç”¨çš„åº“å­¦ä¹ ï¼ˆnumpyï¼Œpandasï¼‰
- [ã€Šå‰‘æŒ‡offerã€‹](http://link.zhihu.com/?target=https%3A//book.douban.com/subject/25910559/)â€”â€”å¸¸è§é¢è¯•é¢˜ï¼Œé¢è¯•å¿…å¤‡ã€‚

æœ€åæ¨èä¸€ä¸ªç½‘ç«™ï¼Œæ”¶é›†äº†è¿›é˜¶çš„æœºå™¨å­¦ä¹ å„ç§èµ„æº[Githubæœºå™¨å­¦ä¹ Machine-Learning](http://link.zhihu.com/?target=https%3A//github.com/JustFollowUs/Machine-Learning%23learning_route)



### åƒè€ƒè³‡æ–™

- Kmeans
  - [StatsLearning Lect12c 111113](https://www.youtube.com/watch?v=aIybuNt9ps4)
  - [KMeans Algorithm](https://www.youtube.com/watch?v=hDmNF9JG3lo)
  - [Unsupervised Machine Learning: Flat Clustering](https://pythonprogramming.net/flat-clustering-machine-learning-python-scikit-learn/)
- Hierarchical Clustering
  - [StatsLearning Lect12d](https://www.youtube.com/watch?v=Tuuc9Y06tAc)
  - [StatsLearning Lect12e](https://www.youtube.com/watch?v=yUJcTpWNY_o)

### å¸¸è¦‹å•é¡Œèˆ‡è™•ç†

- Overfiting
  - æ¨¡å‹æ•æ‰åˆ°å¤ªç´°ç¯€çš„ç‰¹å¾µï¼Œå°è‡´åœ¨è¨“ç·´è³‡æ–™çš„é æ¸¬æ•ˆæœå¾ˆå¥½ï¼Œä½†åœ¨æ¸¬è©¦è³‡æ–™å°±å®Œå…¨å£æ‰
  - è™•ç†æ–¹å¼
    - å°‡è§£é‡‹è®Šæ•¸èˆ‡ç›®æ¨™è®Šæ•¸çš„åˆ†ä½ˆç•«å‡ºä¾†ï¼Œé€ä¸€æª¢è¦–è§£é‡‹è®Šæ•¸å°æ–¼ç›®æ¨™è®Šæ•¸çš„å€è¾¨æ•ˆåº¦
    - ç¢ºèªç›®æ¨™è®Šæ•¸çš„è³‡æ–™æ˜¯å¦å­˜åœ¨é›¢ç¾¤å€¼
    - å°‡è¤‡é›œæ¨¡å‹æ›¿æ›ç‚ºç°¡å–®çš„æ¨¡å‹ï¼Œé™ä½æ¨¡å‹çš„è¤‡é›œåº¦



## FAQ

### Linear Regression

- **What is the p-value?** 
  - To understand the P-value, we need to start by understanding the null hypothesis: the null hypothesis is the assumption that the parameters associated to your independent variables are equal to zero. 
  - Therefore under this hypothesis, your observations are totally random, and donâ€™t follow a certain pattern. The P-value is the probability that the parameters associated to your independent variables have certain nonzero values, given that the null hypothesis is True. The most important thing to keep in mind about the P-Value is that it is a statistical metric: the lower is the P-Value, the more statistically significant is an independent variable, that is the better predictor it will be.

- **What are the Multiple Linear Regression assumptions in more details?** 
  - Linearity: There must be a linear relationship between the dependent variable and the independent variables. Scatterplots can show whether there is a linear or curvilinear relationship. 
  - Homoscedasticity: This assumption states that the variance of error terms is similar across the values of the independent variables. A plot of standardized residuals versus predicted values can show whether points are equally distributed across all values of the independent variables. 
  - Multivariate Normality: Multiple Linear Regression assumes that the residuals (the differences between the observed value of the dependent variable y and the predicted value yË† are normally distributed. 
  - Independence of errors: Multiple Linear Regression assumes that the residuals (the differences between the observed value of the dependent variable y and the predicted value yË† are independent. 
  - Lack of multicollinearity: Multiple Linear Regression assumes that the independent variables are not highly correlated with each other. This assumption is tested using Variance Inflation Factor (VIF) values.

- **How is the coefficient b0 related to the dummy variable trap?**

  - Since $D2 = 1 âˆ’ D1$ then if you include both $D1$ and $D2$ you get:
    $$
    \begin{equation}\begin{split} 
    y& = b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4D_1 + b_5D_2\\
    &= b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4D_1 + b_5(1 âˆ’ D_1)\\
    &= b_0 + b_5 + b_1x_1 + b_2x_2 + b_3x_3 + (b_4 âˆ’ b_5)D_1\\
    &= b^âˆ—_0 + b_1x_1 + b_2x_2 + b_3x_3 + b^âˆ—_4D_1
    \end{split}\end{equation}
    $$

  - with $b^âˆ—_0 = b_0 + b_5$ and  $b^âˆ—_4 = b_4 âˆ’ b_5$ 

  - Therefore the information of the redundant dummy variable $D2$ is going into the constant $b_0$.

### Decision Tree Regression

- **How does the algorithm split the data points?** 
  - It uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased right after a split. Hence, building a decision tree is all about finding the attribute that returns the highest standard deviation reduction (i.e., the most homogeneous branches).

- **What is the Information Gain and how does it work in Decision Trees?**
  - The Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are looking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the more the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.

- **What is the Entropy and how does it work in Decision Trees?**
  - The Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous is your data in a part, the lower will be the entropy. The more you have splits, the more you have chance to find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in these parts. However you might still find some nodes where the data is not homogeneous, and therefore the entropy would not be that small.

### Random Forest Regression

- **What is the advantage and drawback of Random Forests compared to Decision Trees?** 
  - Advantage: Random Forests can give you a better predictive power than Decision Trees. 
  - Drawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the graph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture. Thatâ€™s something you canâ€™t do with Random Forests.

- **When to use Random Forest and when to use the other models?**
  - First, you need to figure out whether your problem is linear or non linear. 
  - Then: If your problem is linear, you should go for Simple Linear Regression if you only have one feature, and Multiple Linear Regression if you have several features. If your problem is non linear, you should go for Polynomial Regression, SVR, Decision Tree or Random Forest. 

### Evaluating Regression Models Performance

- **What are Low/High Bias/Variance in Machine Learning?** 
  - Low Bias is when your model predictions are very close to the real values. 
  - High Bias is when your model predictions are far from the real values. 
  - Low Variance: when you run your model several times, the different predictions of your observation points wonâ€™t vary much. 
  - High Variance: when you run your model several times, the different predictions of your observation points will vary a lot.
  - What you want to get when you build a model is: Low Bias and Low Variance.

###  Logistic Regression

- **Is Logistic Regression a linear or non linear model?**
  - It is a linear model. You will visualize this at the end of the section when seeing that the classifierâ€™s separator is a straight line

- **What are the Logistic Regression assumptions?**
  - First, binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal. 
  - Second, logistic regression requires the observations to be independent of each other. In other words, the observations should not come from repeated measurements or matched data. 
  - Third, logistic regression requires there to be little or no multicollinearity among the independent variables. This means that the independent variables should not be too highly correlated with each other. 
  - Fourth, logistic regression assumes linearity of independent variables and log odds. although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.

### K-Nearest Neighbors (K-NN)

- **Is K-NN a linear model?**
  - No, K-NN is a non linear model, as you will see in the practical sections of this course.
- **What number of neighbors should we choose?**
  - The more you have neighbors, the more this team of neighbors has chance to find correct predictions, and therefore the more your model accuracy has chance to increase. 
  - However be careful, if you have too many neighbors, that will cause overfitting on the training set and the predictions will be poor on new observations in the test set.

### Support Vector Machine (SVM)

- **Is SVM a linear model?** 
  - Yes, SVM is a linear model. You will see that easily in the practical sections of this course, when visualizing the results on the graph (you will notice that the prediction boundary is a straight line). However we can make the SVM a non linear model, by adding a kernel, which you will see in the next section.

- **Why does we see the support vectors as vectors not as points?**
  - The vectors are points in 2-D space (as in this example), but in real-world problems we have data-sets of higher dimensions. In an n-dimensional space, vectors make more sense and it is easier to do vector arithmetic and matrix manipulations rather than considering them as points. This is why we generalize the data-points to vectors. This also enables us to think of them in an N-dimensional space.

### Naive Bayes

- **Is Naive Bayes a linear model or a non linear model?** 
  - Naive Bayes is a non linear model. You will see that very clearly in Python or R when plotting the prediction boundary which will be a very nice curve well separating the non linearly distributed observations.

- **How does the algorithm decide the circle?**
  - In the Intuition lecture we see that a circle is drawn to create a collection of data points similar to the new datapoint. The new datapoint was roughly at the center of the circle and hence we saw that number of green points were lesser than the number of red points and hence the new point went to the red category. 
  - But if we had drawn the circle a little differently around the new datapoint, then the number of green points could have been more than red. So how is that circle chosen? There is a parameter in the model that decides the radius of the circle, just like there is a parameter that chooses the number of neighbors in K-NN.

### Decision Tree Classification

### Random Forest Classification

### Evaluating Classification Models Performance

### K-Means Clustering

- **Where can we apply clustering algorithm in real life?** 
  - You can apply them for different purposes: 
    - Market Segmentation
    - Medicine with for example tumor detection
    - Fraud detection
    - to simply identify some clusters of your customers in your company or business.

### Hierarchical Clustering

- **What is the point of Hierarchical Clustering if it always leads to one cluster per observation point?** 
  - The main point of Hierarchical Clustering is to make the dendrogram, because you need to start with one single cluster, then work your way down to see the different combinations of clusters until having a number of clusters equal to the number of observations. And itâ€™s the dendrogram itself that allows to find the best clustering configuration.
- **When you are comparing the distance between two clusters or a cluster and a point, how exactly is it measured?** 
  - Are you taking the centroid in the cluster and measuring the distance? Exactly, the metric is the euclidean distance between the centroid of the first cluster and the point, (or the centroid of the other cluster for the distance between two clusters).

- **Do we also need to perform feature scaling for Hierarchical Clustering**
  - Yes because the equations of the clustering problems involve the Euclidean Distance. Anytime the model equations involve the Euclidean Distance, you should apply feature scaling.

- **Should we use the dendrogram or the elbow method to find that optimal number of clusters?** 
  - You should use both (itâ€™s faster to try both than you think thanks to the templates), just to double check that optimal number. However if you really only have time for one, I would recommend the elbow method. The dendrogram is not always the easiest way to find the optimal number of clusters. But with the elbow method itâ€™s very easy, since the elbow is most of the time very obvious to spot.

### Association Rule Learning

- **What are the three essential relations between the support, confidence and lift?**

  - Given two movies M1 and M2, here are the three essential relations to remember: Relation between the support and the confidence
    $$
    confidence(M_1 \to M_2) = \frac{support(M_1,M_2)}{support(M_1)}
    $$
    
  - Relation between the lift and the support:
    $$
    lift(M_1\to M_2) = \frac{support(M_1, M_2)}{support(M_1)Ã—support(M_2)}
    $$
    
  - Relation between the lift and the confidence (consequence of the two previous equations):
    $$
    lift(M_1 \to M_2) = \frac{confidence(M_1, M_2)}{support(M_2)}
    $$
  
- **Are the confidence and lift symmetrical functions?**

  - Given the three equations of the previous question, we can easily see that: Confidence is non symmetrical:
    $$
    confidence(M_1 \to M_2) \ne confidence(M_2 \to M_1)
    $$

  - Lift is symmetrical:
    $$
    lift(M_1 \to M_2) = lift(M_2 \to M_1) 
    $$

- **In real time scenario what is the ideal time period we should consider to make a good Market Basket Analysis model?And should it be done on each store separately or region wise? **

  - One month is a good time period. However you could also consider 3-6 months to normalize the seasonality effect, or you can run the same model every month, which I would rather do to catch the specificities of each month (season, tourism rate, etc.). Then Market Basket Analysis should be done on each store separately, since it depends on the customer behaviors within a specific neighborhood. Basically customers might behave differently across different neighborhoods.

### Eclat

- **When should we use Eclat rather than Apriori?** 
  - The only advantage of Eclat compared to Apriori is that it is simpler and faster to use. However if you need to run a deep analysis of your market basket, then you should definitely go for Apriori.

### Upper Confidence Bound (UCB)

### Thompson Sampling

- **How is Thompson Sampling better than UCB?** 
  - Thompson Sampling is better than UCB in terms of convergence of the regret. The regret is the difference between the optimal reward and the reward you accumulate with your algorithm. Thompson Sampling shows a better regret curve than UCB in my experience. Also, the fact that UCB is deterministic as opposed to Thompson Sampling being stochastic, helps making Thompson Sampling outperform UCB. Besides you will see in the practical sections that Thompson Sampling finds the best ad faster and with more certainty than UCB.

- **How Thompson Sampling can accept delayed feedback.**
  - When doing Thompson Sampling, we can still perform updates in our algorithm (like making new guesses for the distributions with existing data, sampling from the guessed distribution, etc) while we are waiting for the results of an experiment in the real world. This would not hinder our algorithm from working. This is why it can accept delayed feedback.

- **What are further examples of Thompson Sampling applications?**
  - Another potential application of Multi-armed bandits (MAB) can be the online testing of algorithms. For example, letâ€™s suppose you are running an e-commerce website and you have at your disposal several Machine Learning algorithms to provide recommendations to users (of whatever the website is selling), but you donâ€™t know which algorithm leads to the best recommendations. 
  - You could consider your problem as a MAB problem and define each Machine Learning algorithm as an "arm": at each round when one user requests a recommendation, one arm (i.e. one of the algorithms) will be selected to make the recommendations, and you will receive a reward. In this case, you could define your reward in various ways, a simple example is "1" if the user clicks/buys an item and "0" otherwise. Eventually your bandit algorithm will converge and end up always choosing the algorithm which is the most efficient at providing recommendations. This is a good way to find the most suitable model in an online problem. Another example coming to my mind is finding the best clinical treatment for patients: each possible treatment could be considered as an "arm", and a simple way to define the reward would be a number between 0 (the treatment has no effect at all) and 1 (the patient is cured perfectly). 
  - In this case, the goal is to find as quickly as possible the best treatment while minimizing the cumulative regret (which is equivalent to say you want to avoid as much as possible selecting "bad" or even sub-optimal treatments during the process).

###  Principal Component Analysis (PCA)

- **What is the true purpose of PCA?** 
  - The true purpose is mainly to decrease the complexity of the model. It is to simplify the model while keeping relevance and performance. Sometimes you can have datasets with hundreds of features so in that case you just want to extract much fewer independent variables that explain the most the variance.
- **What is the difference between PCA and Factor Analysis?**
  - Principal component analysis involves extracting linear composites of observed variables. 
  - Factor analysis is based on a formal model predicting observed variables from theoretical latent factors. PCA is meant to maximize the total variance to look for distinguishable patterns, and Factor analysis looks to maximize the shared variance for latent constructs or variables.
- **Should I apply PCA if my dataset has categorical variables?**
  - You could try PCA, but I would be really careful, because categorical values can have high variances by default and will usually be unstable to matrix inversion. 
  - Apply PCA and do cross validation to see if it can generalize better than the actual data. If it does, then PCA is good for your model. (Your training matrix is numerically stable). However, I am certain that in most cases, PCA does not work well in datasets that only contain categorical data. Vanilla PCA is designed based on capturing the covariance in continuous variables. There are other data reduction methods you can try to compress the data like multiple correspondence analysis and categorical PCA etc.

- **Is it better to use Feature Extraction or Feature Selection, or both? If both, in which order?**
  - Feature Extraction and Feature Selection are two great dimensionality reduction techniques, and therefore you should always consider both. 
  - What I recommend is doing first Feature Selection to only keep the relevant features, and then apply Feature Extraction on these selected relevant features to reduce even more the dimensionality of your dataset while keeping enough variance.

- **How much total variance ratio do we need to use? Is there any threshold for good total variance ratio?** 
  - Generally a good threshold is 50%. But 60% is more recommended.

- **Is it more common to use exactly 2 independent variables to build a classifier, or do people typically use more than that?** 
  - In general people just extract a number of independent variables that explain a sufficient proportion of the variance (typically 60%). So itâ€™s not always two. It can be more. And if itâ€™s two that is great because then you can visualize better.

### Linear Discriminant Analysis (LDA)

- **Could you please explain in a more simpler way the difference between PCA and LDA?** 
  - A simple way of viewing the difference between PCA and LDA is that PCA treats the entire data set as a whole while LDA attempts to model the differences between classes within the data. Also, PCA extracts some components that explain the most the variance, while LDA extracts some components that maximize class separability.
- **Feature Selection or Feature Extraction?**
  - You would rather choose feature selection if you want to keep all the interpretation of your problem, your dataset and your model results. But if you donâ€™t care about the interpretation and only car about getting accurate predictions, then you can try both, separately or together, and compare the performance results. So yes feature selection and feature extraction can be applied simultaneously in a given problem.

- **Can we use LDA for Regression?**
  - LDA is Linear Discriminant Analysis. It is a generalization of Fisherâ€™s linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification. However, for regression, we have to use ANOVA, a variation of LDA. LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made. LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.

### Kernel PCA

- **Should Kernel PCA be used to convert non-linearly separable data into linearly separable data?** 
  - Thatâ€™s right, but you donâ€™t need to use Kernel PCA with a non linear classifier since the data will be linearly separable after applying Kernel PCA, and therefore a linear classifier will be sufficient.
- **When should we use PCA vs Kernel PCA?** 
  - You should start with PCA. Then if you get poor results, try Kernel PCA.

- **How do I know if my data is linearly separable or not?** 
  - A good trick is to train a Logistic Regression model on it first. If you get a really good accuracy, it should be (almost) linearly separable.

- **Is there a huge difference and what is better to use between Kernel PCA + SVM vs PCA + Kernel SVM?** 
  - Yes there is a difference. 
    - Use Kernel PCA + SVM when you can transform your data into a non-linear low dimensional manifold where the points are separable. 
    - Use PCA + Kernel SVM when you need to transform your data through a linear transformation into a low dimensional manifold, using these points to be transformed into a non-linear space where they are separable.

- **How do we decide which kernel is best for Kernel PCA?**
  - The RBF Kernel is a great kernel, and is the best option in general. But the best way to figure out what kernel you need to apply is to do some Parameter Tuning with Grid Search and k-Fold Cross Validation. 

### k-Fold Cross Validation

- **What is low/high bias/variance?** 
  - These concepts are important to understand k-Fold Cross Validation: 
    - Low Bias is when your model predictions are very close to the real values. 
    - High Bias is when your model predictions are far from the real values. 
    - Low Variance: when you run your model several times, the different predictions of your observation points wonâ€™t vary much. 
    - High Variance: when you run your model several times, the different predictions of your observation points will vary a lot.

- **Does k-Fold Cross Validation improve the model or is it just a method of validation?**
  - k-Fold Cross Validation is used to evaluate your model. It doesnâ€™t necessarily improve your model, but improves your understanding of the model. However you can use it to improve your model by combining it with some Parameter Tuning techniques like Grid Search.

- **What is the difference between a parameter and a hyperparameter?** 
  - Hyper parameters and parameters are very similar but not the exact same thing. 
  - A parameter is a configurable variable that is internal to a model whose value can be estimated from the data. 
  - A hyperparameter is a configurable value external to a model whose value cannot be determined by the data, and that we are trying to optimize (find the optimal value) through Parameter Tuning techniques like Random Search or Grid Search.

- **What is a good/best value of k to choose when performing k-Fold Cross Validation?** 
  - We strongly recommend 10.

- **What does this Standard Deviation tell us exactly?** 
  - The Standard Deviation of the modelâ€™s accuracy simply shows the variance of the model accuracy is 6%. This means the model can vary about 6%, which means that if I run my model on new data and get an accuracy of 86%, I know that this is like within 80-92% accuracy. Bias and accuracy sometimes donâ€™t have an obvious relationship, but most of the time you can spot some bias in the validation or testing of your model when it does not preform properly on new data.
- **How to calculate the F1 score, Recall or Precision from k-Fold Cross Validation?** 
  - You can use sklearnâ€™s metrics library for this. Here is a link.

### Grid Search

- **How do we know which values we should test in the Grid Search?** 
  - A good start is to take default values and experiment with values around them. For example, the default value of the penalty parameter C is 10, so some relevant values to try would be 1, 10 and 100.
- 
